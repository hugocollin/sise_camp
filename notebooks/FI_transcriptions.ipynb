{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données depuis la BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "#Connection à la base de données\n",
    "conn = sqlite3.connect('../src/videos_youtube.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Charger l'url\n",
    "cursor.execute(f\"SELECT id, transcription FROM videos WHERE transcription IS NOT NULL;\")\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Fermer la connexion à la base de données\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks des transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> list[str]:\n",
    "    \"\"\"Permet de splitter le texte en plusieurs chunks\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_ids(input_list: list[tuple[int, str | None]], chunk_size: int = 500, chunk_overlap: int = 50) -> list[tuple[str, str]]:\n",
    "    \"\"\"Génère une liste d'ID de chunks au format '010001'\"\"\"\n",
    "    chunk_list = []\n",
    "\n",
    "    for original_id, text in input_list:\n",
    "        if text is None:\n",
    "            continue\n",
    "\n",
    "        chunks = split_text(text, chunk_size, chunk_overlap)\n",
    "        for chunk_id, chunk in enumerate(chunks, start=1):\n",
    "            formatted_id = f\"{original_id:02d}{chunk_id:04d}\"\n",
    "            chunk_list.append((formatted_id, chunk))\n",
    "\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('010001', \"Bien, c'est parti. Dans cette vidéo, je vais parler de la paralysation des traitements sous Python en utilisant la bibliothèque Joblib. Vous avez vu, la page web est là. Très bien. C'est une série d'outils, comme ils le disent là, qui font référence au Python. Et moi, ce qui m'intéresse en particulier dans cette vidéo-ci, c'est cette paralysation. On peut paralyser les traitements de manière relativement simple. Et ça, c'est quelque chose qui me paraît intéressant à creuser. Je dis ça, pourquoi?\"), ('010002', \"paraît intéressant à creuser. Je dis ça, pourquoi? Parce que je fais travailler mes étudiants sur des projets, je ne suis pas le seul, quasiment toutes nos évaluations se font sous forme de projet, donc les étudiants doivent créer des applications d'IA, d'intelligence artificielle, et en tous les cas, pour les miens, j'essaie de les faire travailler sur toute la chaîne de traitement qui parle de l'extraction des données jusqu'au déploiement, création de conteneurs Docker, d'images Docker qu'on p\"), ('010003', \"tion de conteneurs Docker, d'images Docker qu'on peut ensuite exécuter. Très bien. Voilà. Bon, le déploiement, ça peut être plus large que ça, on est d'accord. Voilà. Bien. Alors, donc, l'application, je mets une série de fonctionnalités que j'attends. Voilà. C'est le cahier des charges. Alors, généralement, les étudiants s'y conforment bien, parce que de toute manière, c'est obligatoire, donc ils s'y conforment. Mais il y a un aspect sur lequel ils s'investissent un peu moins, parce qu'ils sont\"), ('010004', \"ils s'investissent un peu moins, parce qu'ils sont plus dans l'idée de « il faut le faire, il faut terminer le projet ». Ils prennent moins de temps, ils ne prennent pas assez de temps à mon goût sur le fignolage. C'est-à-dire qu'il faut optimiser notre programme, déjà le sécuriser au maximum, et ensuite l'optimiser pour qu'il soit performant. Et pourtant, ça fait partie du job libre, justement. C'est très marrant, ça. Alors, tout le monde l'a noté, du coup. comme voilà ça a été bien ordonné, co\"), ('010005', \"té, du coup. comme voilà ça a été bien ordonné, commence par soi-même voilà, j'ai fait une série de vidéos pour qu'il puisse s'en inspirer, on est d'accord là-dessus bon, en espérant que ces vidéos servent à tout le monde bien sûr, c'est pour ça que je mets toujours en ligne, ça je le dis sans arrêt ça, c'est-à-dire que tous mes tous les éléments là, ils sont là, voilà, très bien alors là j'ai un peu, ces derniers temps j'ai fait série de vidéos donc je n'ai pas le temps de tout terminer mais je\"), ('010006', \"donc je n'ai pas le temps de tout terminer mais je vais le faire je vais le faire avant la fin de l'année là très bien donc je mets ici systématiquement la description de la vidéo avec la vidéo et là je mets tout le temps le notebook ou les données ou les programmes enfin je mets tous les différents éléments comme ça vous avez le cas quand je crée des environnements spécifiques je mets également le fichier iaml moi j'utilise plutôt conda très bien du coup vous pouvez le récupérer ici et reprodui\"), ('010007', \"n du coup vous pouvez le récupérer ici et reproduire à l'identique tout ce que je dis ça c'est très important je vois plein de tutos sur internet et des fois on dirait que ça marche parce que le gars il dit que ça marche mais moi je peux pas je peux pas reproduire si je parle pour produire ça n'a aucun intérêt d'accord alors ces derniers temps donc j'ai créé une série de vidéos sur ces sur ces sur ces aspects là d'optimisation des traitements voilà donc sur air et pour python et et R d'ailleurs.\"), ('010008', \"là donc sur air et pour python et et R d'ailleurs. Là, c'est comment intégrer du code C++ dans R. Là, c'est plutôt pour Python, avec différents aspects. Et j'en suis arrivé. Donc, ça peut arriver sur la... Alors, l'optimisation des traitements, ça peut être sur l'algorithme, ça peut être sur l'organisation des programmes. Voilà. Et ça peut jouer, en réalité. Ça peut jouer l'organisation d'autres programmes. Très bien. Et il y a aussi le choix des technologies. Donc, on avait vu ici, j'ai beaucou\"), ('010009', \" technologies. Donc, on avait vu ici, j'ai beaucoup parlé de Numba, j'ai parlé de Tai Chi, il y a différents aspects, de X-Array également, ou de Sidon. Et maintenant, j'en suis au stade où je parle justement de la paralysation. Alors, j'avais commencé à en parler ici avec Numba, et en fait, j'avais déjà fait aussi une vidéo par le passé. Donc là, c'est la vidéo avec Numba que j'ai fait récemment, là, elle date de hier, 29 décembre, on est le 30 décembre, vous voyez bien, 30 décembre, très bien,\"), ('010010', \"décembre, vous voyez bien, 30 décembre, très bien, j'avais fait hier, j'avais fait ça, et il y a quelques temps déjà, j'en avais fait un avec Dask, parce que c'est une question récurrente en réalité. Bon, là, c'était un peu particulier parce que je demandais aux étudiants de travailler sur des très grandes bases de données et avec Dask, en fait, on a des possibilités de paralyser les algos de machine learning, voilà, qu'ils ont mis en place. Mais on va plus loin que ça avec Dask. On peut aussi p\"), ('010011', \" on va plus loin que ça avec Dask. On peut aussi programmer nos propres traitements, nous-mêmes, en les paralysant. Donc, il y a tout un cadre, tout un framework qui permet de faire ça. Alors, on a vu avec Dask, on a vu avec Numba, aujourd'hui on va traiter avec Jobly, c'est ça l'idée. Alors, pour bien illustrer le propos, je vais reprendre le tutoriel que j'ai fait précédemment, qui est celui-là, là, voilà, où je vais essayer de reprogrammer le OneVS REST classifié de CKITLEAR. Très bien. Alors\"), ('010012', \"OneVS REST classifié de CKITLEAR. Très bien. Alors, je m'étais comparé ici avec OneVS REST classifié, mais cette fois-ci je vais vraiment faire à part. Pourquoi? Parce que Joblib propose des options qui me paraissent intéressantes d'explorer. D'ailleurs, le OneVS REST classifier de CITLEARN, si on lit la documentation, ce que j'ai fait, bien sûr, il s'appuie également sur Joblib. Vous avez vu ça? C'est intéressant, le OneVS REST classifier de CITLEARN, quand il paralélise les calculs, il utilise\"), ('010013', \"LEARN, quand il paralélise les calculs, il utilise Joblib. C'est intéressant, on va essayer de faire pareil. C'est ça l'idée de l'histoire. Alors, pour ça, on va utiliser justement la classe parallèle qui est là. Voilà, la classe parallèle. Je vais sur la version stable. Très bien, la classe parallèle qui est là. Avec ses différents paramétrages. Et notamment, il y en a un, c'est le nombre de ressources qu'on va solliciter. Mais on va revenir là-dessus. Allons-y. J'ai préparé le notebook, comme \"), ('010014', \"dessus. Allons-y. J'ai préparé le notebook, comme d'habitude, qui est là. Et cette fois-ci, on va donc travailler sur Joblib. Là, je suis dans l'environnement base d'Anaconda Python, de la distribution d'Anaconda, la dernière de cet automne. Donc, j'ai récupéré, j'ai installé. Et moi, dans l'environnement base, je n'installe jamais rien. Donc, dans l'environnement base, par défaut d'Anaconda, de la distribution d'Anaconda, il y a Python, il y a NumPy, Et il y a aussi Joblib, comme il y a NumBuy,\"), ('010015', \"umPy, Et il y a aussi Joblib, comme il y a NumBuy, ainsi de suite d'ailleurs. Et c'est Hitler, ainsi de suite. Donc on est ici, on est le 30 décembre 2024, on voit bien, on est sur la version de Python 3.12.7. Et on est bien dans l'environnement base d'Anaconda Python. Alors j'affiche la version de Python, j'affiche la version de NumPy, j'affiche la version de Joblib. Voilà, 3.12.7, 1.26.4 pour NumPy, parce qu'on va l'utiliser intensivement, et 1.4.2, 1 pour Joblib. Alors ensuite, je vais utilis\"), ('010016', \".4.2, 1 pour Joblib. Alors ensuite, je vais utiliser des données. J'aurais pu charger un fichier de données, effectivement, mais là en l'occurrence, j'ai préféré utiliser des données déjà disponibles. Mais le jeu de données correspondant, c'est la base CoverType qui est là. Alors dans la base CoverType, l'idée c'est de prédire les types de forêts. Il y en a sept types possibles. donc on est dans le classement multiclasse, la variable cible a plus de deux modalités, cette en l'occurrence ici, on \"), ('010017', \" de deux modalités, cette en l'occurrence ici, on dispose de 54 descripteurs, variables explicatives, toutes quantitatives, en tout cas numériques, et le nombre d'instances est 581 012. Pour traiter ça, je vais essayer de reproduire le One-Way-Stress Classifier, où l'idée, justement, c'est de décomposer le problème multiclasse en une série de problèmes binaires. Voilà, c'est ça l'idée. Alors, j'en parle en détail ici. J'en parle en détail de cet aspect-là ici. Je ne vais pas le reproduire, je ne\"), ('010018', \"aspect-là ici. Je ne vais pas le reproduire, je ne vais pas refaire toute l'histoire. Donc, regardez le début de cette vidéo. Je vais mettre des timestamps là. Il faut que je m'en occupe, soit aujourd'hui, soit demain. Je vais mettre des timestamps ici. Voilà. Et comme ça, vous aurez la possibilité de regarder tous les chapitres. Je vais mettre un chapitrage. OK. Donc, voilà. C'est ça, la base que je suis en train de charger et qui est intégrée dans le module dataset de CKITLER. Voilà. Donc, je \"), ('010019', \"ans le module dataset de CKITLER. Voilà. Donc, je récupère la base en question. Et une fois qu'elle est récupérée, je récupère les descripteurs. Et on a bien 54 descripteurs pour 581 012 individus. 581 012 individus. OK. Ensuite, je récupère la variable cible. Le nombre d'observations est le même, sinon il y aurait un problème, on est bien d'accord là-dessus. Et je regarde la fréquence des classes, et on a bien 7 classes, ce qui est dit ici, on est bien d'accord. 7 classes, et donc numérotées de\"), ('010020', \"st bien d'accord. 7 classes, et donc numérotées de 1 à 7. Très bien. Alors, qu'est-ce qui se passe? Je vais lancer une régression logistique. je ne vais pas utiliser une descente de gradient, je vais utiliser un fichier scoring, mais fondamentalement, c'est mieux de ramener les variables sur les mêmes échelles. Du coup, je fais de la standardisation. Bien. Alors, dans CKIT Learn, on utilise ici l'estimateur, j'ai utilisé la régression logistique la dernière fois, et c'était une descente de gradi\"), ('010021', \"la dernière fois, et c'était une descente de gradient. Il y a un autre algorithme possible, c'est d'utiliser le fichier scoring. Je suis allé sur Copilot, et j'ai lui demandé de me proposer du code pour programmer from scratch, la régression logistique avec l'algorithme Fischer Scoring. Il m'a proposé un code.  qui est jolie, mais j'en avais parlé la dernière fois, il y a des éléments qui posent problème, c'est ça, justement, ce calcul. Alors, là, on a une matrice PN, P plus 1, parce qu'on rajou\"), ('010022', \", on a une matrice PN, P plus 1, parce qu'on rajoute une constante, c'est ce que je fais ici, là, je rajoute une constante, là, ici, pour avoir l'intercept, et donc là, j'ai une matrice P plus 1 N, et là, même si c'est une matrice diagonale, elle est de taille NN. et le n pour nous c'est le nombre d'instances il est égal à 581 012 donc il va créer une matrice 580 012 fois 580 012, ben non sachant que chaque valeur vaut 8 octets même pas en rêve donc ça a planté sur des petites bases sur iris ça \"), ('010023', \"onc ça a planté sur des petites bases sur iris ça va marcher, on est d'accord là dessus mais il n'y a pas qu'iris dans la vie, on est d'accord les bases sont plutôt importantes de nos jours du coup ça pose un vrai souci du coup l'algo il est là j'en parle ici du l'algo si vous voulez le regarder dans la taille il est là donc qu'est-ce qu'on a pour mettre à jour le vecteur des coefficients on calcule le gradient qui est là la matrice dérivée partielle première enfin le vecteur pour le coup ici et\"), ('010024', \"elle première enfin le vecteur pour le coup ici et là c'est la matrice S qui vous donne la courbure de la fonction à optimiser, la fonction de perte optimisée très bien cette matrice là est de dimension P plus 1 P plus 1, et il nous permet d'avoir la variance-covariance des coefficients, d'ailleurs, si on veut faire de l'inférence statistique. Ce qu'on n'a pas naturellement avec une descente de gradient, justement, parce qu'on n'a pas cette matrice-là. En revanche, ça induit un calcul supplément\"), ('010025', \"ce-là. En revanche, ça induit un calcul supplémentaire. Et ça, c'est du côté négatif, mais du côté positif, c'est qu'on a une convergence nettement plus rapide. Par rapport à une descente de gradient, la convergence est nettement plus rapide. Ce qui est tout à fait normal, parce que en réalité, dans la descente de gradient, qui est juste en dessous là, on n'utilise que le vecteur gradient. Et donc, on a des informations parcellaires, en fait. Alors qu'avec le fichier scoring, Newton-Raphson, ici\"), ('010026', \"rs qu'avec le fichier scoring, Newton-Raphson, ici, dans ce cadre-ci, c'est la même chose. Voilà, on a bien la pente, voilà, et on a aussi la courbure. Du coup, l'algorithme d'optimisation est mieux guidé. On est d'accord là-dessus. Donc, j'ai vu ça. Bon, ce n'était pas gérable de le faire, du coup, je l'ai un peu modifié ici. Voilà, je l'ai un peu modifié, je l'ai un peu optimisé, et notamment parce que j'ai une inversion de matrice à un moment donné, on le voit l'inversion de matrice là. Voilà\"), ('010027', \"donné, on le voit l'inversion de matrice là. Voilà. Et je stabilise la matrice, la inversée, en rajoutant une petite valeur, une petite constante sur la diagonale principale. Ça n'a aucune justification mathématique, je l'avais dit, mais numériquement, ça rend le calcul plus sûr. C'est juste ça. Très bien. Donc, ça, c'est la réaction logistique binaire en utilisant de mes études. Voilà. Très bien. Donc, j'ai passé le maxité régal à 10. par rapport à la précédente vidéo. Je ne l'avais pas dit, ça\"), ('010028', \"t à la précédente vidéo. Je ne l'avais pas dit, ça. Voilà. Tout simplement, pour laisser la possibilité d'aller plus loin. Et mon idée aussi, c'est de faire durer plus les calculs pour qu'on voie mieux l'intérêt de la paralysation. C'est juste ça. OK. Très bien. Du coup, une fois qu'on a la régression binaire, là, on a une régression binaire. Le Y, là, c'est une colonne de 0,1. Une colonne de true-false, en fait. Un vecteur de true-false. Du coup, pour mettre en place la régression multiclasse, \"), ('010029', \", pour mettre en place la régression multiclasse, j'applique la stratégie one vs rest, c'est exactement ce qui est expliqué ici une classe contre les autres et je passe tour à tour les classes donc il y a cette classe ici, il y a cette régression à faire vous êtes d'accord là dessus voilà, donc on voit bien tour à tour, je prends la valeur comme comme variable cible je prends comme modalité cible une des modalités de la classe, de la variable classe et donc je fais les régressions logistiques bi\"), ('010030', \"sse et donc je fais les régressions logistiques binaires correspondantes voilà la fonction et il ne me reste plus qu'à le lancer alors là pour le coup ce que j'obtiens c'est une liste de coefficients là je suis en séquentiel on voit bien ici donc il est en train de s'énerver il devrait être en haut il a terminé déjà? on n'a pas Python, c'est marrant on devrait avoir Python quelque part et non il est encapsulé dans VS Code je pense ok bien Alors, une fois que c'est fait ça, du coup, je les mets s\"), ('010031', \"une fois que c'est fait ça, du coup, je les mets sous forme de coefficients cette fois-ci. Donc là, j'ai une matrice de coefficients. Alors, on voit que chaque classe est en ligne, et on a les descripteurs en colonne, plus l'intercept qui est en première position. Une fois qu'on a les coefficients estimés ici, vous avez vu les temps de calcul, 13,6, mais bon, il y a beaucoup d'interférences, donc je vais faire un time-it tout à l'heure pour comparer les temps de traitement. Très bien, je crée un\"), ('010032', \"rer les temps de traitement. Très bien, je crée une fonction de prédiction. Alors, la fonction de prédiction est très simple, c'est que je prends tout simplement la matrice, je rajoute la colonne de constante, c'est-à-dire qu'on donne l'intercept, et je fais un produit matriciel tout simplement avec les coefficients. Et ensuite, je prends comme prédiction celle qui correspond au score maximal. Je rajoute 1 parce que vous avez vu que les classes commencent à 1 et non pas à 0 dans notre fichier. C\"), ('010033', \"ommencent à 1 et non pas à 0 dans notre fichier. C'est juste ça. voilà, et du coup je fais la prédiction sur nos mêmes échantillons de données, là l'idée c'est comparer les temps de traitement on n'est pas sur les performances prédictives, on est sur les temps de traitement très bien, et je mesure la accuracy, voilà, 0 71 53, c'est à peu près les résultats qu'on avait obtenus avec C. Hitler dans la précédente vidéo, donc on est bien dans les clous donc tout l'enjeu ici maintenant c'est comment c\"), ('010034', \"s donc tout l'enjeu ici maintenant c'est comment cette procédure séquentielle-là, qui est une boucle sur chaque, je fais une boucle où j'itère, je fais une itération sur chaque modalité de la variable cible, comment je vais paralyser ça. Très bien. Alors, Joblib, justement, permet de le faire facilement. Donc, j'importe de Joblib deux éléments, un parallèle qui permet de mettre en place la paralysation des calculs et délayer dans la réalité qui permet d'attendre qu'on ait défini tous les calculs\"), ('010035', \"ermet d'attendre qu'on ait défini tous les calculs avant de le lancer. J'en reparle tout à l'heure. Très bien. Donc, je reprends la même fonction que dessus, la fonction séquentielle. Je reprends exactement la fonction séquentielle que dessus. Voilà. Je récupère le nombre de classes, comme ici. La fonction, elle est là, on est d'accord. Je récupère le nombre de classes. Et ensuite, je vais itérer sur chaque modalité de la variable cible. Voilà. C'est exactement ce qui se passe ici. Donc, je récu\"), ('010036', \"'est exactement ce qui se passe ici. Donc, je récupère le nombre de classes maximales. C'est cette ici. Voilà. Donc, je crée une instance de parallèle, où je dis que le nombre de jobs, ça va être le nombre de classes, 7 en l'occurrence, et je veux utiliser le mécanisme des processus. Voilà. Alors, petit retour. Sur ma machine, j'ai 8 coeurs. Donc, si je demande 7 jobs, ça le fait. Il reste un job pour le reste. Il reste un cœur pour le reste. Et le cœur, ça va me servir à quoi? Enregistrer la vi\"), ('010037', \"le cœur, ça va me servir à quoi? Enregistrer la vidéo. Vous avez vu, j'utilise Zoom, la version gratuite de Zoom. Très bien. Et du coup, il vaut bien qu'il continue à enregistrer. Si je commence à prendre tous les cœurs, ça va créer des saccades ou je ne sais pas quoi. Je ne vais pas essayer. Parce que ça ne sert à rien d'essayer ça. On est d'accord là-dessus. Donc, on est bien dans l'écho. Ensuite, ça c'est un premier point. Donc, c'est le end job qui est là et qui est égal à 7. L'autre élément\"), ('010038', \"ob qui est là et qui est égal à 7. L'autre élément important, c'est que j'utilise une technologie spécifique, c'est que je crée des processus. Et tout à l'heure, je vais créer des crènes, je vais utiliser des threads. Alors, c'est quoi la différence entre les deux? Là, je suis allé sur Copilot et je lui dis, mais quelle est la différence entre un processus et un thread dans la programmation parallèle? En réalité, ce qu'il vous dit, c'est que dans les processus, on a une instance de programme ind\"), ('010039', \" les processus, on a une instance de programme indépendant. C'est-à-dire qu'il a son propre espace mémoire et il s'exécute de manière indépendante. Voilà. Donc, on a des séries de programmes, si vous voulez. Et là, on a une série de sept programmes. OK. En revanche, si on utilise des threads, c'est un autre mécanisme qu'il est utilisé, c'est que j'ai un seul processus et dedans, on a une série de threads, de files, si vous voulez. OK. Bon, le fil n'est pas... Je préfère threads, pour le coup. C'\"), ('010040', \" n'est pas... Je préfère threads, pour le coup. C'est plus compréhensible. OK. Là, l'intérêt, c'est qu'on a le même espace mémoire, et du coup, on peut échanger des informations d'un trait à l'autre. Mais bon, ça, généralement, Python, il n'aime pas trop, il fait des verrouillages partout. Mais c'est une possibilité. Alors moi, je ne l'exploite pas ici. On a bien vu que, ici, les régressions binaires sont indépendantes. Les régressions binaires, là, sont indépendantes. Même si j'utilise le même \"), ('010041', \"là, sont indépendantes. Même si j'utilise le même X, ici, le même descripteur, il est là, là. Même si j'utilise le même descripteur, les Y sont différents. Et donc, ils peuvent s'exécuter de manière indépendante. Et le Z, là, on ne le modifie jamais. D'accord, là-dessus. Donc, il n'y a pas de problème. OK. Une fois que j'ai fait ça, on est prêt à lancer. Je vais lancer la régression logistique binaire. Alors, le rôle de délayer, c'est pourquoi? C'est qu'en réalité, si je ne mets pas de délayer d\"), ('010042', \"'est qu'en réalité, si je ne mets pas de délayer d'ici, qu'est-ce qui va se passer? C'est que dès que j'ai lancé la première régression, il va se lancer tout de suite, sans attendre les autres. Du coup, ça va créer des interférences. Donc, l'idée de délayer, tout simplement, c'est que définis d'abord l'ensemble des calculs. Et une fois qu'on a fini de définir l'ensemble des calculs, il peut le lancer. C'est ça l'idée. Il va les processer. On est d'accord là-dessus. Très bien. OK. Alors, qu'est-c\"), ('010043', \"d'accord là-dessus. Très bien. OK. Alors, qu'est-ce qui se passe ici? Qu'est-ce que je délaye, c'est le fitreg, la réaction binaire au-dessus. On est d'accord. C'est le z qui est là. Là, le cible maintenant, k plus 1. Parce que j'étais sous k. de range NB class, voilà, et les paramètres additionnels de la...  régulation logistique binaire. Très bien. Donc, par rapport au séquentiel, qu'est-ce qui change? J'ai créé un process, voilà, avec la technologie des processus, et j'ai défini, et on a bien\"), ('010044', \"ologie des processus, et j'ai défini, et on a bien une liste de coefficients de nouveau. Les résultats se mettent sous l'ordre d'une liste de coefficients. On obtient, avec le process que j'appelle, j'obtiens une liste, en fait. Ça est défini ici. Regardez bien la documentation. Ça, c'est la page de garde. Mais ce qui m'a surtout servi, c'est la documentation technique. Voilà. Donc, la documentation technique, elle est là. Vous avez l'adresse qui est là. Et justement, elle est là. Il va retourne\"), ('010045', \" est là. Et justement, elle est là. Il va retourner les résultats sous forme de liste. C'est exactement ce que je voulais. Donc, tout va bien. Tout va bien. Très bien. Alors, une fois que c'est bon, ça, très bien. Le préfère, il est là. Le préfère, il est là. Il est où le préfère, là? Il est plus bas. Voilà. et justement, moi, je préfère utiliser, pour l'instant, j'utilise le process. Voilà. Ok. Alors, je vais afficher ça ici et on va voir qu'est-ce qu'il nous dit. Je mets ça comme ça et je lanc\"), ('010046', \"-ce qu'il nous dit. Je mets ça comme ça et je lance les calculs. J'ai trié selon le processus, très bien. Allons-y. Tac. et voilà les enfants de Python qui s'énervent dans tous les sens et il y en a 7 là, si vous comptez bien il y en a 7 parce que le n-job que moi j'avais demandé, il y en avait 7 c'est fini, vous avez bien vu alors vérifions si on a bien les mêmes résultats on a bien les mêmes résultats si on regarde les coefficients, on a bien les mêmes coefficients donc le fait de paralyser le\"), ('010047', \"es mêmes coefficients donc le fait de paralyser les calculs on ne fait pas en plus besoin le fait de paralyser les calculs ne joue pas sur la qualité des résultats On est d'accord là-dessus. Le seul objectif, c'est de réduire le temps de calcul. C'est d'optimiser les traitements. Voilà. Mais les résultats ne doivent pas être modifiés. On est d'accord là-dessus. Bien. Alors, ce qui est très intéressant par rapport à Numba, c'est que, ici, les résultats sont récupérés dans l'ordre d'appel. Voilà. \"), ('010048', \"ltats sont récupérés dans l'ordre d'appel. Voilà. Et non pas dans l'ordre de l'achèvement des calculs. Donc, je n'ai pas besoin de synchronisation à faire. ou dans Numba, il fallait créer une liste de vecteurs d'abord et ensuite faire une affectation spécifique. Là, tout simplement, on les envoie à la queue de le et ils vont revenir à la queue de le, dans le même ordre. C'est ça l'intérêt. Ça également, je l'ai trouvé dans la documentation. Ça, c'est très important. Parce que si les résultats n'\"), ('010049', \"'est très important. Parce que si les résultats n'arrivent pas dans le même ordre que l'appel, c'est à vous de retrouver, ils sont où, et ainsi de suite. Et c'est juste impossible. Très bien. C'était plutôt la phrase. Les systèmes sont dans l'ordre des soumissions, on n'a pas besoin de réorganisation. Ça, c'est la première technologie, c'est les process. Et comme j'avais vu en lisant la doc de Joblib qu'on pouvait utiliser les trades, et on voit ici ce que ça veut dire les trades, plus légère, o\"), ('010050', \"ici ce que ça veut dire les trades, plus légère, on peut avoir de la mémoire partagée, je me suis dit, tiens, essayons ici, est-ce que dans notre contexte, ça permet aussi d'avoir des résultats plus performants? Je ne vois pas trop pourquoi, parce que je ne charge pas d'informations entre les différents modules appelés, si vous voulez. Donc, le fait qu'on utilise des trades ici n'est pas très avantageux pour soi, puisque chaque exécution est indépendante. Mais bon, peut-être qu'à l'intérieur, c'\"), ('010051', \"pendante. Mais bon, peut-être qu'à l'intérieur, c'est programmé différemment et que c'est plus rapide. On va essayer de voir ça. Donc, j'ai fait exactement le même programme qu'au-dessus, là, sauf que j'ai changé cette option-là. Voilà ce que j'ai changé. Tout le reste est pareil. Voilà, et encore une fois, les vecteurs obtenus sont dans l'ordre de soumission. Hop là! Une fois que c'est bon, je lance de nouveau les calculs. Voilà, donc si je viens ici, on n'a plus qu'une seule instance de Python\"), ('010052', \"s ici, on n'a plus qu'une seule instance de Python. Vous avez bien vu, on a un seul processus, et dans le processus, on a 7 threads. 7, pourquoi? Parce que le nombre de classes est égal à 7 ici. Vous avez vu, voilà. Et vous avez vu, on n'a vécu qu'un seul Python. Bon, il y en a d'autres ici, c'est les traces d'avant, je pense. Une fois que c'est fait ça, on a bien les mêmes performances prédictives. Donc, la qualité de prédiction n'est pas altérée du tout par le passage à la paralysation. C'est \"), ('010053', \"e du tout par le passage à la paralysation. C'est très important. Une fois que c'est fait ça, du coup, j'ai décidé de mesurer les temps d'exécution. Alors, là, pour le coup, je vais suspendre la vidéo parce que ça va prendre un peu de temps. mais l'idée est la même chose, toujours comme dans la vidéo précédente, comme dans cette vidéo aussi, c'est que je vais voir les différentes versions. Un, le programme séquentiel, sans paralysation, très bête. Ensuite, deux, avec la paralysation JobLib, mais\"), ('010054', \". Ensuite, deux, avec la paralysation JobLib, mais avec la technologie Processus, qui est expliquée ici, on est d'accord. Et la même chose avec JobLib toujours, mais avec la technologie T-Trade. et on va voir qu'est-ce qu'on gagne, qu'est-ce qu'on gagne pas. Voilà. Très bien. Donc, je lance ça et je suspends la vidéo et je reprends la main quand tous les calculs seront terminés. Voilà. À tout de suite. Je reviens. Ça y est, il a terminé les calculs. Donc, j'ai utilisé TimeEat. Vous avez vu, j'ai\"), ('010055', \"ls. Donc, j'ai utilisé TimeEat. Vous avez vu, j'ai utilisé TimeEat et j'ai demandé 10 itérations pour pouvoir mesurer les temps de traitement. Comme ça, on n'est pas embêté par les interférences. 10 fois, bon, voilà. Et qu'est-ce qui se passe? Avec la version séquentielle, on est sur du 11,9 secondes, en moyenne à peu près, 12 secondes à peu près, si vous voulez, très bien. Et quand on passe à la paralysation, on est sur du 8,4 avec les process et 8,63, c'est pareil, avec les threads. Donc déjà,\"), ('010056', \"t 8,63, c'est pareil, avec les threads. Donc déjà, premier élément, c'est qu'en paralisant des calculs, on gagne. Mais on ne divise pas par 7. On est d'accord, même si j'utilisais cette job, voilà, on ne divise pas par 7, tout simplement, pourquoi? Parce qu'il y a des initialisations à mettre en place, voilà, ensuite, dispatching, et ensuite, une fois que les calculs sont terminés, il faut mettre en place la consolidation, et tout ça prend du temps. Donc c'est pour ça que on n'a pas une division\"), ('010057', \"ps. Donc c'est pour ça que on n'a pas une division par 7, même si on a demandé 7 jobs. Ça c'est une première réponse, mais quand même, il y a un gain indéniable, 1,5 à peu près, 1,4, 1,5. On a réduit d'un facteur à 1,5. ça c'est un premier élément donc pas réaliser les calculs, c'est intéressant surtout si les machines sont multi-coeurs tout simplement ou si on a des cartes graphiques moi je ne fais pas ici parce que je n'ai pas de carte graphique si j'en ai une mais bon, de la basique celle qui\"), ('010058', \"e si j'en ai une mais bon, de la basique celle qui est sur la carte mère mais si on a des cartes j'ai vu qu'il y a des cartes graphiques super performantes avec plusieurs gigas de RAM et tout ça là « Ya! » Si on peut exploiter ça, ce serait pas mal. On est d'accord là-dessus. Alors, ça c'est un premier élément. Deuxième élément important, la technologie de paralysation ne joue pas. La technologie de paralysation ne joue pas ici. On a quasiment la même chose. Dans mon contexte, ici, qu'on passe p\"), ('010059', \" même chose. Dans mon contexte, ici, qu'on passe par des processus ou qu'on passe par des threads, ça n'a pas joué sur la qualité des résultats. Peut-être qu'il y a des contextes où les threads, c'est mieux. peut-être qu'il y a des contextes où les processus sont mieux. Généralement, c'est la nécessité de faire des échanges d'informations ou pas. C'est là que ça va jouer en réalité. Mais bon, dans le contexte ici, c'est pareil. Voilà. Donc, si je résume, il y a différents programmes des applicat\"), ('010060', \" résume, il y a différents programmes des applications d'IA, donc d'intelligence artificielle. Ces applications-là, en tout cas, en ce qui concerne, je demande aux étudiants des projets qui sont plutôt transversaux. Alors, il y a différents éléments dans le cahier des charges. Et parmi les éléments qui m'importent pour moi, en tous les cas, c'est la performance. Il faut créer des applications performantes. Alors, du coup, il y a différentes pistes possibles qui tiennent à l'organisation des prog\"), ('010061', \"s possibles qui tiennent à l'organisation des programmes, au choix d'algorithmes. Il y a aussi les choix de technologie. Et enfin, il y a un aspect que j'ai voulu montrer ici, que je montre ces derniers temps. Voilà, c'est la paralysation des traits. Voilà, excellent travail à tous....\"), ('020001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de la stratégie One vs. REST classifieur dans le cadre du classement multiclasse, c'est-à-dire l'apprentissage supervisé où la variable cible est catégorielle et à plus de deux modalités. C'est ça l'idée. Dans ce cadre-là, il y a plusieurs approches possibles. Voici, il y a l'approche multinomiale. Très bien, on traite directement les classes, sachant qu'elles sont mutuellement exclusives, et il y a une autre approche possible, c'est de \"), ('020002', \", et il y a une autre approche possible, c'est de décomposer les traitements en une série de problèmes binaires. Et justement, dans ce cadre-là, il y a la stratégie One vs. REST classifié. Alors, ce n'est pas totalement innocent en réalité, le vrai propos ici, c'est de voir comment on peut paralyser le calcul dans ce cadre-là. Alors, j'essaie de restituer ça rapidement pour que tout le monde puisse bien profiter de cette vidéo. En ce moment, je travaille beaucoup sur l'optimisation des calculs, \"), ('020003', \"ravaille beaucoup sur l'optimisation des calculs, l'optimisation des traitements quand on programme en Python, ou R. Donc, j'en ai fait pour R, j'en ai fait pour Python, et parmi les technologies cette fois-ci, j'ai vu qu'il y avait Numba, qui est ici, qui annonce des choses très intéressantes, qui approche les performances de ce qu'on peut avoir en CO4. Et dans ce cadre-là, justement, j'avais fait une vidéo. Et dans cette vidéo-ci, j'avais pu voir, j'avais pu montrer, mais de manière très succi\"), ('020004', \"ir, j'avais pu montrer, mais de manière très succincte, qu'on pouvait paralyser les calculs. Voilà. C'est cette idée-là que je vais explorer, plus en profondeur cette fois-ci. Voilà. Et pour montrer l'intérêt de la paralysation des calculs, on va s'appuyer sur le 1 vs rest classifier. Donc, le réel propos de cette vidéo, c'est comment paralyser les calculs quand on traite des problèmes multiclasse avec la stratégie One vs. REST classifieur. Alors là, je suis sur la doc de SikitLearn. Pourquoi? P\"), ('020005', \" là, je suis sur la doc de SikitLearn. Pourquoi? Parce que lui aussi le fait. J'ai vu la doc ici, il explique comment il fait ça. Vous avez un problème multiclasse, vous prenez chaque classe individuellement contre les autres, et là-dessus, vous prenez un classifieur binaire. Le classifieur binaire, ça peut être n'importe quelle technique. moi ici, je me suis appuyé sur la régression logistique. Très bien, je reviens là-dessus. Et dans ce cadre-là, du coup, on peut simplement, très simplement, p\"), ('020006', \"à, du coup, on peut simplement, très simplement, paralyser des calculs. Parce que paralyser des calculs, c'est assez compliqué en réalité. Dans certains cas, c'est évident, ce sera le cas ici. Dans d'autres cas, il faut bien connaître les algorithmes pour savoir les décomposer en sous-problèmes et les soumettre aux différents processeurs, aux différents cœurs de votre processeur. Alors justement, ici, il nous montre qu'il y a number of jobs. Il dit que si on met 1, ça veut dire qu'on n'utilise q\"), ('020007', \"it que si on met 1, ça veut dire qu'on n'utilise qu'un seul cœur, il n'y a pas de paralysation. Et si on met moins 1, on utilise tous les processeurs disponibles. Très bien. On va voir la différence. Et ensuite, dans un deuxième temps, on va essayer de reproduire cette idée-là en nous appuyant sur les possibilités de Numba, qui sont assez phénoménales. Voilà, c'est ça l'idée. Voilà. Alors, allons-y. Voilà. Et on va ouvrir notre notebook. Alors, dans un premier temps, qu'est-ce que je fais? Dans \"), ('020008', \"ans un premier temps, qu'est-ce que je fais? Dans un premier temps, je vais utiliser le OneVSRest classifier de C-Ketler. Et voir déjà comment il implémente la paralysation de calcul en jouant sur ce paramètre-ci. C'est ça l'idée. Dans un deuxième temps, on va programmer nous-mêmes l'arrégation logistique et on va voir si on peut paralyser les calculs en utilisant Numba. C'est ça l'idée. Voilà le plan de présentation. Allons-y alors. Comme d'habitude, là j'utilise tous les paquets disponibles av\"), ('020009', \"tude, là j'utilise tous les paquets disponibles avec l'environnement base d'Anaconda Python, de la distribution d'Anaconda Python. Donc on est bien sur l'environnement base de la distribution d'Anaconda Python et actuellement la version disponible c'est la 3.12.7. ok, très bien je vérifie la version de Python quand même voilà, je vérifie aussi la version de NumPy parce que je vais l'utiliser intensivement je vérifie la version de CKIT Learn parce que je vais l'utiliser ici et enfin je vais utili\"), ('020010', \" que je vais l'utiliser ici et enfin je vais utiliser NumBank très bien, voilà les différentes versions que j'ai dans ce tutoriel-ci c'est très important que vous ayez cette visibilité-là parce que je vais mettre en ligne le notebook vous pourrez le charger et peut-être que vous aurez des résultats différents vérifiez déjà si on a bien les mêmes versions. C'est ça qui est important. Une fois que ça s'est fait, je vais charger un jeu de données. Pour une fois, je vais utiliser des données qui son\"), ('020011', \"our une fois, je vais utiliser des données qui sont incorporées dans des packages. Pourquoi? Parce que ça simplifie la chose simplement. Mais bon, quand même, les données qui sont utilisées ici, c'est la base CoverTip. Alors CoverTip, vous le trouverez en ligne. Les données sont là, en fait. Donc, c'est ces données-là qui sont impactées dans le module dataset de CKETL1. Alors, il s'agit de prédire cette type de fourrée à partir des données cartographiques, des variables cartographiques. Ce qui m\"), ('020012', \"raphiques, des variables cartographiques. Ce qui m'intéresse ici, c'est qu'il y a 54 variables, et on a un problème à 7 classes. On est bien dans le cadre multiclasse. On est bien d'accord là-dessus. Pour corser l'affaire, parce que sinon c'est trop simple, on a 581 012 instances. Exemple. Donc le n, le nombre d'observations, il est assez important. Donc le temps de calcul va être un élément sur lequel on va jouer. Très bien. Et on va voir l'intérêt justement de la paralysation dans ce cadre-là.\"), ('020013', \"rêt justement de la paralysation dans ce cadre-là. Et l'autre aspect important également, c'est qu'on a bien 7 classes, on est bien dans le cadre de multi-classes. Bon, le nombre de variables ici reste modéré, 54, plus l'intercept qu'on va rajouter, la constante de 1, passera à 55. Voilà. Mais bon, ça reste gérable, sans problème. Allons-y alors. Donc, dans le package dataset de SightLearn, je récupère les données. Je prends les X d'un côté, on a bien 54 descripteurs, je suis tout à fait d'accor\"), ('020014', \" bien 54 descripteurs, je suis tout à fait d'accord là-dessus, et le nombre d'observations, c'est 581 012. C'est bien ça, c'est bien ça. Très bien. Ensuite, je récupère la classe, voilà. Je vérifie les valeurs possibles avec les effectifs, et il y a bien cette classe. Très bien, donc c'est bien ça. Alors, les classes, ici, sont numérotées 1, 2, 3, 4, 5, 6, 7. OK. Pourquoi pas, hein? Voilà. Mais il faut le savoir. Il faut le savoir. Ça, c'est très important. OK. Alors, bon, comme je vais lancer u\"), ('020015', \" important. OK. Alors, bon, comme je vais lancer une régression logistique avec une descente de gradient, c'est celle-ci que je vais utiliser. Alors, la régression logistique de CedicLarn peut traiter directement le cadre multiclasse avec un modèle multinomial. Il peut le faire. Voilà. Mais bon, moi, ici, je vais utiliser que les possibilités binaires, tout simplement. Et il utilise une descente de gradient. Alors ça, pour avoir une idée là-dessus, vous allez sur ma page de cours, là, de régress\"), ('020016', \"s, vous allez sur ma page de cours, là, de régression logistique, voilà, qui est là. Et sur ma page de cours, il y a des supports, c'est celui-là, voilà. Et je montre à peu près l'idée de la descente de gradient. Alors là, c'est la version la plus simplifiée qui soit. Il y a des variantes plus performantes, appelées à Adam, ainsi de suite, ainsi de suite. Voilà, là vraiment, sur une descente de gradient, la plus simple possible. Voilà, dans une expression la plus simple. C'est ce que j'ai mis en\"), ('020017', \"xpression la plus simple. C'est ce que j'ai mis en œuvre ici. OK. Alors, très bien, ça va jouer en gros sur le calcul du gradient. Et là, on a le taux d'apprentissage qui permet de corriger au fur et à mesure les coefficients. Ça, c'est l'algorithme de base de C.Kitler. Avec des variantes possibles, mais toujours, c'est la même idée. Basé sur le gradient. Alors, pour qu'on puisse utiliser le gradient de manière efficace, c'est mieux de standardiser les variables. Ce n'est pas toujours le cas, ma\"), ('020018', \"er les variables. Ce n'est pas toujours le cas, mais là, en l'occurrence, pour éviter les problèmes, je le fais d'emblée, et je le fais sur toute la base. Parce que ce qui m'intéresse ici, c'est le temps de calcul, on est bien d'accord. Sinon, non, non, il faut le faire sur la partie apprentissage et l'appliquer sur l'échantillon de test. D'accord, bien. Alors, voyons ce qui se passe avec C-Kit Learn. Alors, avec C-Kit Learn, du coup, j'ai vu qu'on peut décomposer le problème de manière tout à f\"), ('020019', \"on peut décomposer le problème de manière tout à fait lisible. Ça, c'est génial, ça. c'est que j'ai le classifiaire de base binaire qui est là, c'est logistique régression, et cette régression logistique binaire est encapsulée dans un framework ou un VS13 classifier. Donc l'approche est très généralisée cette fois-ci. Dire qu'on a une sorte de métaclassifier qui va utiliser ce classifiaire-là. Nickel, très bien. Alors, mon classifieur interne, j'ai mis random stat 0 pour que vous puissiez reprod\"), ('020020', \"ai mis random stat 0 pour que vous puissiez reproduire à l'identique les calculs, avec les paramètres par défaut, avec la version de CKITLEARN, parce que des fois les paramètres par défaut peuvent changer, nous on est sur la version 1.5.1. Très bien. Voilà. Et surtout, je dis, je ne veux pas de paralysation des calculs. Voilà. Donc, nJob c'est égal à 1. Voilà. Donc, il n'y a pas de paralysation ici. Alors, très bien. Donc, je vais instancier ici. Il y a une double instanciation, du classifier in\"), ('020021', \" Il y a une double instanciation, du classifier interne et du métaclassifier, qui est le one best classifier, et je lance l'apprentissage. J'affiche la liste des classes pour contrôle, mais normalement, on a bien 1, 2, 3, jusqu'à 7. Allons-y. Eh bien, il lance ses calculs, et là, je regarde. Effectivement, dans le gestionnaire de tâches de Windows, on voit bien qu'il s'énerve sur Python. en fait à la base j'ai 8 coeurs et il s'énerve sur un coeur et comme il y a le mode turbo je dépasse ici la f\"), ('020022', \" et comme il y a le mode turbo je dépasse ici la fréquence nominale la vitesse nominale il est allé plus loin là vous avez bien vu  Ça y est, il a terminé. Alors, voilà, on a ici, ça a duré, bon, on mesurera les temps de calcul tout à l'heure. Très bien. Alors, je mesure l'accuratie, donc je fais une prédiction en resubstitution, pour simplifier les choses tout simplement. Voilà. Et je compare la prédiction avec le target pour avoir une accuratie de base. Voilà. Voilà. Donc, l'accuratie de base \"), ('020023', \" de base. Voilà. Voilà. Donc, l'accuratie de base est de 0, 51, 51, 53. OK. Bien. Très bien. Alors, maintenant, on va voir. Il le dit, c'est dit dans la documentation, il faut lire la documentation. Voilà. Très bien, il dit tout simplement que, voilà, ça. On peut lancer la paralysation des calculs. Voilà. En jouant sur le NJobs. Si on met moins 1, il utilise toutes les ressources disponibles. Et là, ça tombe bien. Parce que, regardez. sur ma machine, j'ai 8 cœurs. Voilà. Et j'ai 7 classes. Donc,\"), ('020024', \"ine, j'ai 8 cœurs. Voilà. Et j'ai 7 classes. Donc, je ne vais pas déborder. Sinon, il est obligé de créer soit des files d'attente, soit je ne sais pas ce qu'il fait. Mais là, en l'occurrence, il n'y en a pas de problème de ça parce qu'on a moins de classes que 2 cœurs. Si vous n'avez que 2 cœurs, et que vous avez 7 classes, il met en place une file d'attente, j'imagine. Enfin, il faut voir. Il faut lire la doc pour savoir ce qu'il fait exactement. Mais là, en l'occurrence, on n'a pas de problèm\"), ('020025', \"t. Mais là, en l'occurrence, on n'a pas de problème là-dessus. Donc, je refais exactement la même manip, sauf que là, maintenant, le nJobs, je l'ai mis à moins 1, pour lui dire, utilise toutes les ressources disponibles. Et comme on a 7 classes, il va lancer 7 processus. Voilà, on est d'accord là-dessus. Très bien. Donc, de nouveau, je lance et je calcule l'accuratie. Si on doit avoir la même valeur que là, ce n'est pas parce qu'on est passé en calcul parallèle que le résultat doit être différen\"), ('020026', \"alcul parallèle que le résultat doit être différent. On est d'accord là-dessus. sinon ça n'a pas de sens. Je lance ici, et je vais dans le gestionnaire de tâches, et là, très intéressant, regardez, on a 7 processus Python. Donc il fait bien du calcul parallèle. Et on a bien différents processus qu'il a lancés. Bon, ça y est, c'est fini déjà. Qu'est-ce qui se passe? On a ici un accuracy de 0,7153 qui est le même qu'ici. Si ce n'est pas le même, on arrête tout de suite. Ça veut dire qu'il y a eu u\"), ('020027', \" arrête tout de suite. Ça veut dire qu'il y a eu un problème. Voilà. Ce n'est pas parce qu'on passe sur du calcul parallèle que les résultats doivent être différents. C'est exactement la même chose, mais on exploite mieux les ressources de la machine. Donc, on doit avoir exactement les mêmes résultats. Très intéressant, maintenant, regardons si le passage par le parallélisme de C-Ket-Learn nous permet de gagner du temps. Et ce qui permet de diviser par 7 le temps. C'est pas vrai, ça n'arrive jam\"), ('020028', \"er par 7 le temps. C'est pas vrai, ça n'arrive jamais. Sauf dans des cas très particuliers, très scolaires, on ne divise jamais le temps de calcul par le nombre de processus. Non. Parce qu'il y a des calculs de synchronisation qui sont mis en place, rien que le dispatching des opérations, ça va prendre du temps. Ensuite, il faut consolider les résultats, ça va prendre du temps, ainsi de suite. Voilà. Mais bon, on va voir combien on a gagné. Donc là, je fais un time-it, tout simplement, et pour q\"), ('020029', \"là, je fais un time-it, tout simplement, et pour que ça ne prenne pas trop de temps, mais je vais quand même faire une suspension de la vidéo. Voilà, je réitère cette fois. Et voilà, avec une loupe, une boucle. Très bien. Et là, je vais faire sans paralysation. Allons-y. Donc, on voit bien. Il s'énerve bien sur un processus. Et avec le turbo mode, il accélère beaucoup. On le voit ici. On dépasse la vitesse réelle et au-dessus de la vitesse nominale, parce qu'il s'est mis en turbo mode. On s'éner\"), ('020030', \"le, parce qu'il s'est mis en turbo mode. On s'énerve sur un seul cœur. ici. Voilà. Et on le voit bien. On le voit bien ici. Voilà. Je vais suspendre la vidéo parce que ça va prendre quelques minutes. Et du coup, je reprends juste après. Je reprends l'enregistrement. Voilà. Et là, il a lancé 7 fois. Avec une boucle chaque fois. Et le temps de traitement est de 14,3 secondes. Pour chaque lancement. On est d'accord là-dessus. Ça, c'est sans paralysation. Maintenant, on va voir ce qui se passe avec \"), ('020031', \"tion. Maintenant, on va voir ce qui se passe avec paralysation, s'il a lancé 7 processus, est-ce qu'il divise par 7 le temps de calcul? On verra, ce n'est pas vrai en fait, parce que j'avais dit tout à l'heure, il y a beaucoup de synchronisation à mettre en place, ainsi de suite. Et même l'opération de dispatching, lui-même, prend du temps. Je le lance, très bien. Et on voit ici, dans le gestionnaire de tâches, de nouveau, on a les processus Python qui arrivent, les 7, parce qu'il y a 7 classes.\"), ('020032', \"on qui arrivent, les 7, parce qu'il y a 7 classes. Alors, de nouveau, je suspends la vidéo et je reprends la main juste après. Ça y est, il a terminé. Voyons ce que ça a donné, le fait qu'on passe par une paralysation avec C-Kid Learn. Il a gagné, vous avez vu, mais pas tant que ça. Très peu. En réalité, pourquoi? Parce qu'en même temps, ici, j'enregistre, j'utilise Zoom, vous avez bien vu que j'utilise Zoom. Et ça prend des ressources aussi. Du coup, il ne peut pas prendre autant ses ailes qu'i\"), ('020033', \"coup, il ne peut pas prendre autant ses ailes qu'il veut en réalité. Quand je n'ai pas Zoom, Là, le gain est un peu plus important, mais pas en tout cas divisé par 7. Et voilà, on a gagné, mais pas tant que ça. Mais en tous les cas, paralyser les calculs permet de gagner du temps. Et dans le cadre du One vs REST, tout simplement, ce qui se passe, c'est qu'il lance l'apprentissage de chaque classe versus les autres, voilà, en tant que processus. Donc, comme il y a 7 classes, il lance 7 processus.\"), ('020034', \"onc, comme il y a 7 classes, il lance 7 processus. On a bien vu dans le gestionnaire de tâches tout à l'heure. C'est ça l'idée. Alors, une fois que j'ai vu ça, voilà, vous avez vu, C'est décevant quand même un petit peu. On est d'accord là-dessus, mais on a gagné. C'est indéniable. Il y a presque 3 secondes, 2,5 secondes d'écart. Une fois que j'ai vu ça, je me suis dit, nous-mêmes, quand on programme des algorithmes de machine learning, on a des outils qui permettent de paralyser. Et notamment, \"), ('020035', \"outils qui permettent de paralyser. Et notamment, il y a Numba. Donc, je me suis dit, je vais essayer de programmer le oneVestRest classifier en utilisant Numba. Sauf que, comme je suis... J'aime bien la difficulté, voilà, au lieu d'utiliser la régression logistique de ce kit learn, je vais programmer moi-même la régression logistique en utilisant non pas la descente de gradient, mais en utilisant le fichier scoring. Newton-Raphson, parce qu'ici, la matrice SN, elle est calculée directement, ave\"), ('020036', \" la matrice SN, elle est calculée directement, avec la bonne formule. Alors, j'ai mis l'idée ici. Vous avez vu la différence, c'est que là, on garde que le gradient est là, on a eu le taux d'apprentissage, et là, cette fois-ci, on utilise la matrice Sien. Alors, l'intérêt d'utiliser cette approche-là, le V-Share scoring, c'est que on tient compte de la pente, de la fonction de perte, quand on optimise, et on tient compte également avec cette partie-là de la courbure de la fonction de perte. de l\"), ('020037', \"ie-là de la courbure de la fonction de perte. de la fonction de perte. Et du coup, il optimise mieux. Et le nombre d'itérations est nettement réduit. Ça, c'est l'avantage. Le gros inconvénient, c'est quoi? C'est qu'il faut calculer cette matrice-là, qui est de taille P plus 1, P plus 1. P, c'est le nombre de descripteurs, plus 1 parce qu'il y a la constante, l'intercept, et c'est une matrice carré-symétrique. P plus 1, P plus 1. Alors, 54 plus 1, 55, on a une matrice 55-55, qu'il faut savoir man\"), ('020038', \" 55, on a une matrice 55-55, qu'il faut savoir manipuler. Très bien, mais bon, en tout cas, on a bien cette idée-là. Donc, qu'est-ce que j'ai fait pour faire ça? Je suis allé sur Copilot et je lui ai dit, proposez-moi, propose-moi un code Python qui calcule la réaction logistique binaire avec l'algorithme Fischer-Scoring. Voilà ce qu'il dit. Et il me propose ce code-là. Alors, ce code est très bien, très bien, mais en réalité, je ne peux pas l'utiliser directement pour différentes raisons. Il y \"), ('020039', \"iliser directement pour différentes raisons. Il y en a une déjà, qu'il faut bien voir, c'est que ce ne sont pas toutes les fonctions NumPy qui sont portées sous Numba. Avec Numba, je vais avoir une sorte de compilation. On est d'accord ici, c'est bien ce qui est idée ici, et j'en parle ici. On a une sorte de compilation. Et dans ce cadre-là, ce ne sont pas toutes les fonctionnalités de NumPy qui sont transposées sous Numba. Moi, au départ, j'ai mis ça, j'ai rajouté la directive de compilation de\"), ('020040', \"is ça, j'ai rajouté la directive de compilation de Numba, et boum, ça n'a pas marché. Donc, j'ai regardé en détail et je suis tombé sur cette doc-là. Voilà. On est bien sur la doc de Numba, là. On est d'accord là-dessus. Voilà. On est bien sur la doc de Numba. Et en réalité, il n'y a qu'une partie des fonctionnalités de NumPy qui sont portées sous Numba, qui sont supportées par Numba. Voilà. Il faut se palucher la doc. Il faut se palucher la doc. C'est ce que j'ai fait. Ça m'a pris du temps monu\"), ('020041', \" C'est ce que j'ai fait. Ça m'a pris du temps monumental. Mais maintenant, je connais bien. Là, pour le coup, pour les projets, quand vous allez travailler dessus, je peux vous dire qu'on ne va pas pouvoir me raconter des salades. Ça ne va pas être possible. Parce que là, je connais vraiment très bien maintenant l'outil. OK. Du coup, il y a plein de fonctions qui ne marchaient pas, comme le hashtag, là, on a dû remplacer par concatenate, ainsi de suite. Très bien. Et il y a un deuxième aspect ég\"), ('020042', \" suite. Très bien. Et il y a un deuxième aspect également, c'est cette partie-là. Pour calculer la matrice Sienne, il prend la matrice X et dans laquelle on a rajouté la colonne de constante et la transpose. Donc on a P plus 1 croix N en termes de dimension de matrice. Ensuite, on a une matrice diagonale où on a les probas. Simplement, P fois 1 moins P. P, c'est la probate appartenant sous classe. Très bien. Alors c'est une matrice diagonale, mais de manière tout à fait prosaïque, sa taille, c'e\"), ('020043', \"s de manière tout à fait prosaïque, sa taille, c'est N croix N. N croix N.  n et n pour nous ici c'est 580 et 1000 donc quand j'ai voulu lancer ce code là sans réfléchir parce que je suis comme tout le monde je lance d'abord et quand ça plante je regarde pourquoi on est d'accord là dessus la machine m'a dit mais non tu peux pas louer une vecteur de enfin une matrice de 2 terras je dis quoi ça de terras et pas de terras là dedans mais en regardant un un peu, ben non, non, non, clairement, effecti\"), ('020044', \" un un peu, ben non, non, non, clairement, effectivement, parce que, ben non, non, non, là, là, là, ça ne va pas le faire. Donc, du coup, il a fallu modifier le code également pour qu'on n'ait pas à créer cette matrice de taille N croix N, 5801012 croix 5801012. Vous êtes d'accord là-dessus. Voilà, du coup, j'ai adapté le code. Donc, j'ai importé Numba avec le NJIT, tout ça, j'ai expliqué ici, je ne vais pas revenir en détail là-dessus. Vous êtes d'accord là-dessus. Ça permet de compiler votre c\"), ('020045', \" d'accord là-dessus. Ça permet de compiler votre code. Ensuite, j'ai modifié le code pour que soient uniquement pris en compte les fonctions de NumPy reconnues par Numba. Ça, c'est un premier point. Ensuite, le calcul de la mort qui crée la matrice de taille n croix n, je l'ai enlevé et je l'ai transformé comme ceci. Alors là, regardez, je mettrai ça en ligne, on est d'accord là-dessus, comme d'habitude. Regardez bien comment j'ai fait pour reprendre ce code de base-là. Et comment j'ai palié cet\"), ('020046', \"ndre ce code de base-là. Et comment j'ai palié cet énorme problème. En la transformant en un calcul plus simple qui ne prend pas beaucoup en mémoire. C'est ça l'idée. Avec exactement les mêmes résultats. J'ai vérifié, figurez-vous. On est bien d'accord là-dessus. Alors, j'ai rajouté un autre élément. C'est qu'à un moment donné dans le calcul, on a une inversion de matrice. C'est jamais bon ça. quand vous faites des calculs itératifs et qu'il y a des inversions de matrice mais elle est nécessaire\"), ('020047', \"des inversions de matrice mais elle est nécessaire l'inversion de matrice et là l'inversion de matrice, on est d'accord là dessus généralement le problème de matrice singulière arriverait très rapidement donc pour stabiliser les calculs je rajoute une toute petite constante sur la diagonale principale du coup on n'a pas de problème avec l'inversion là pour le coup c'est vraiment une astuce de calcul très bête en théorie mathématique, il n'y a rien qui justifie ça. Mais en théorie numérique, dans\"), ('020048', \"n qui justifie ça. Mais en théorie numérique, dans le calcul numérique, ça permet de stabiliser la matrice pour que l'inversion ne pose pas problème. Voilà ce que j'ai mis en place. Très bien. Donc, une fois que j'ai fait ça, là, c'est la régression logistique binaire, seulement que j'ai fait en utilisant l'algorithme de Fischer-Scoring, ou Newton-Raphson ici. Dans la régression logistique, c'est la même chose. Voilà. avec ce mode de mise à jour des paramètres, des coefficients calculés. Voilà, \"), ('020049', \"des paramètres, des coefficients calculés. Voilà, je lance ici, ça y est, il est chargé en mémoire, et ensuite, j'implémente le OneVsRest de manière séquentielle. Qu'est-ce qui se passe tout simplement? Je récupère le nombre de classes maximales, sachant que les classes sont numérisées, sont identifiées par un code numérique, 1, 2, 3 jusqu'à nombre de classes maximales. Donc je récupère le nombre de classes maximales ici, et je vais créer une liste des coefficients pour chaque classifieur binair\"), ('020050', \"te des coefficients pour chaque classifieur binaire. Et donc je fais une boucle, je crée le Y binaire, 1, 0, et ensuite je lance le phi-grade-numba que j'ai mis là. pour le classifieur binaire. Très bien. Et je rajoute la liste des coefficients. C'est ce que je fais ici. Et bien sûr, toujours en utilisant l'énergie. Sans paralysation, pour l'instant. Donc là, j'ai une liste de coefficients. Il y en aura sept. Sept vecteurs de coefficients. Et je les rajoute au fur et à mesure. Comme j'ai un calc\"), ('020051', \"les rajoute au fur et à mesure. Comme j'ai un calcul séquentiel, il n'y a pas d'ambiguïté possible. Je vais avoir les coefficients, classifieurs binaires dans l'ordre. C'est l'idée. Je lance ça. Très bien. Et je lance l'apprentissage. Voilà. Je lance l'apprentissage. Comme tout à l'heure, il va s'énerver sur un seul processeur. Il prend beaucoup de temps là-dessus. Il utilise toutes les ressources disponibles en passant en mode turbo toujours. Bien vu. En passant en mode turbo toujours. J'ai vu \"), ('020052', \"en vu. En passant en mode turbo toujours. J'ai vu qu'il y a certains nouveaux processeurs qui peuvent monter jusqu'à 6 GHz. Ça laisse rêveur. Ça laisse vraiment rêveur. On ne peut pas trop rêver avec ça à l'université pour l'instant. Ça y est, là, il avance. C'est fini. Il a fait des calculs. J'ai des coefficients. Il l'a transposé ici, mais on a bien 55 coefficients, 54 variables plus l'intercept. Et on a bien 7 jeux de coefficients parce qu'il y a 7 classes. ensuite j'ai créé la fonction de pr\"), ('020053', \"y a 7 classes. ensuite j'ai créé la fonction de prédiction qui prend en entrée tout simplement ici la description des chantiers en test et la matrice des coefficients parce que là vous avez vu j'avais une liste et j'ai transformé en une matrice numpy c'est cette matrice numpy que je récupère là et ensuite je fais tout simplement le produit et Et chaque individu à classer, je regarde la classe qui a obtenu le score maximal. C'est juste ce qui est fait ici. Comme le numéro de colonne commence à 0 \"), ('020054', \"fait ici. Comme le numéro de colonne commence à 0 et que mes classes commencent à 1, il faut que je rajoute la valeur de 1. C'est juste ça. Très bien. Allons-y alors. Je fais la prédiction en resubstitution sur les mêmes jeux de données et je calcule l'accuracy. Alors, ce n'est pas exactement la même valeur qu'au-dessus. au coup dessus là, vous avez vu tout à l'heure sur ce kit-là, avec les paramètres par défaut, j'avais 0.71.53 voilà et ici, avec ma méthode FischerScoring mais avec nettement mo\"), ('020055', \"c ma méthode FischerScoring mais avec nettement moins d'itérations j'ai 0.71.43 voilà la troisième décimale, si vous voulez c'est pas ça l'enjeu, c'est vraiment pas l'enjeu ici l'enjeu ici, c'est comment on va pariser les calculs très bien, mais ça me donne une référence ça veut dire que quand je vais passer au cas de plus de parallèle, il faut que j'obtienne exactement le même d'ici. Je n'ai pas la même accuracy que c'est Hitler, parce que les algos sous-jacents, la réagulation logistique binai\"), ('020056', \"lgos sous-jacents, la réagulation logistique binaire ne sont pas les mêmes. Là, c'est une descente de gradient, moi c'est un fichier scoring, mais en revanche, moi, avec moi-même, sans ou avec la paralysation, je dois obtenir le même résultat. Alors, la paralysation, justement, elle est là. Et regardons La différence qu'il peut y avoir entre l'algorithme non parallélisé, séquentiel, qui est là, ici, et celui que j'ai parallélisé. Regardez. Alors, toujours, on a la directive de Numba. Très bien. \"), ('020057', \" toujours, on a la directive de Numba. Très bien. Et je rajoute un élément supplémentaire. Je dis que maintenant, je veux que les calculs soient parallélisés. Je récupère le nombre de classes. Très bien. Et c'est là que commence un nouveau problème. Qu'est-ce qui se passe? Je ne peux pas créer une liste vide et les rajouter au fur et à mesure. Parce que je ne maîtrise pas la durée de calcul pour chaque classe contre les autres. Donc, si je reste sur ce schéma-là et que je fais un p-range, parce \"), ('020058', \"sur ce schéma-là et que je fais un p-range, parce que c'est le p-range qui permet de paralyser les calculs, voilà, vous avez vu, j'ai rajouté p-range ici. Voilà. Qu'est-ce qui va se passer? Il se peut que les vecteurs de coefficient n'arrivent pas dans le même ordre. Parce que certains ont voyagé plus vite et d'autres non. Certains ont des classes rares, d'autres non. Voilà. Et alors là, si les classes, si les vecteurs, les coefficients associés aux classes n'arrivent pas dans le même ordre, la \"), ('020059', \"aux classes n'arrivent pas dans le même ordre, la prédiction, vous oubliez. On est d'accord là-dessus. Donc, il m'a fallu utiliser un autre artifice. Quel est l'artifice que j'ai mis en place? J'ai créé une liste de coefficients en la remplissant avec des vecteurs vides pour l'instant. Avec autant de vecteurs qu'il y a de nombre de classes. Très bien. Ensuite, je fais une boucle à la place du range standard qui est là. Vous avez vu ici, là c'est un range standard. J'utilise le pRange pour dire q\"), ('020060', \"un range standard. J'utilise le pRange pour dire que je vais paralyser les calculs. Ensuite, je crée la variable cible, ça ne change pas. Je lance le calcul et cette fois-ci, je fais un accès indexé. Du coup, il n'y a pas d'ambiguïté à la place numéro 0. Vous avez bien le premier classifieur de la première classe. À la place numéro, à la place suivante, vous avez le deuxième classifiant, parce que j'ai fait un accès indexé et non pas un append. C'était ça, le truc. Sinon, c'est foiré. Tout le re\"), ('020061', \"'était ça, le truc. Sinon, c'est foiré. Tout le reste, c'est pareil. Je lance. Et je vais lancer les calculs. Et là, regardez. J'ai qu'un seul Python. je n'ai pas comme sur NumPy comme CKitLearn où il y avait plusieurs processus là je n'en ai qu'un seul pourquoi? parce que NumBAR utilise la technique des trades pour paralléliser donc c'est une autre technologie ça je l'ai su comment? je l'ai lu la doc tout simplement je suis allé la doc j'ai regardé comment il fait les calculs parallèles ça m'a \"), ('020062', \"rdé comment il fait les calculs parallèles ça m'a pris un peu de temps voilà et je me suis rendu compte qu'il utilise en réalité les trades, tout simplement. Ok, très bien. Donc c'est pour ça que dans notre gestionnaire de tâches, il n'y en avait qu'une seule version de Python. Est-ce que pour autant, ça accélère les calculs? On va essayer de voir après, le temps de la rapidité. Là, il a terminé en tous les cas.  Je vérifie. Voilà, et on a bien la même valeur. 0, 71, 43. 0, 71, 43. Je suis cohér\"), ('020063', \"a même valeur. 0, 71, 43. 0, 71, 43. Je suis cohérent avec moi-même. Heureusement que je suis cohérent avec moi-même. Sans et avec la paralysation. Alors, une fois que c'est fait ça, maintenant, mesurons les temps de calcul. Voilà, sans et avec paralysation, en utilisant mon système à moi basé sur Numba. L'objectif de la vidéo, c'est bien ça. c'est comment on peut paralyser des calculs en utilisant Numba. C'est ça l'idée. Et pour montrer ça, on est allé voir comment on peut mettre ça en place su\"), ('020064', \"st allé voir comment on peut mettre ça en place sur le OneVSRest classifiant, qui est un cadre relativement simple. On est d'accord là-dessus. Alors, mesurons le temps de calcul sans paralysation. Tac, en utilisant TimeEat. C'est ce que je fais ici. Donc, il lance les calculs là, et je vais suspendre. voilà donc il a fini et il m'a dit que chaque chaque chaque calcul chaque lancement de l'algorithme a pris séquentiel sans paralisation ici après 17,4 secondes alors ce temps là n'est pas vraiment \"), ('020065', \"7,4 secondes alors ce temps là n'est pas vraiment comparable avec ce kit l'air pourquoi parce que c'est pas les deux mêmes algorithmes sous jacents rappelez vous un c'est kit l'air lui il utilise une descente de gradient comme ceci là ou un avatar une variante, si vous voulez, et moi, j'utilisais un fichier scoring. Donc, comparer les deux, ça n'a pas trop de sens. Moi, ce qui est intéressant, en revanche, c'est comparer ce temps sans paralysation avec le temps de calcul qui sera mesuré ici, où \"), ('020066', \"n avec le temps de calcul qui sera mesuré ici, où j'ai introduit la paralysation. C'est ça qui est intéressant. Je lance. Très bien. On regarde un peu. On n'a toujours qu'un seul Python qui est utilisé. Pourquoi? Parce que, rappelez-vous, il s'appuie sur les mécanismes des threads, qui est un autre mécanisme. C'est pour ça qu'on n'a pas plein de versions de Python qui apparaissent là, à la différence de Cititlern, qui manifestement s'appuie sur une autre technologie. On est d'accord là-dessus. O\"), ('020067', \"ne autre technologie. On est d'accord là-dessus. On va voir si ça gagne. Je vais suspendre la vidéo et je la reprends juste après. Ça y est, il a terminé. Et regardons, voilà. Ça a quasiment divisé par deux. Donc, Numba s'appuie sur les trades pour paralléliser les calculs, et on voit que dans ce cadre-ci, on a quand même quasiment divisé par deux le temps de traitement. Alors, ça ne divise pas par sept. Encore une fois, pourquoi? Parce qu'il y a des problèmes d'utilisation des ressources, de sy\"), ('020068', \" des problèmes d'utilisation des ressources, de synchronisation, de dispatchage des calculs, de dispatching des calculs, ainsi de suite. Voilà, donc c'est très rare qu'on puisse diviser par le nombre de trades. Mais quand même, ça permet de gagner. voilà, alors si je résume je m'intéresse à l'optimisation des calculs sous Python et sous R je fais les deux, j'essaie de trouver les deux alors dans ce cadre là je m'intéresse aux technologies et il y a une technologie qui est pas mal c'est Numba qui\"), ('020069', \" a une technologie qui est pas mal c'est Numba qui permet de compiler le code c'est ce qu'il dit ici, moi j'invente rien et notamment il propose des solutions pour la paralysation des algorithmes notamment par l'utilisation des trades alors pour montrer si c'est vraiment probant j'ai essayé de reproduire le OneVSRest classifier en me basant sur une régression logistique maison où j'utilise le Fischer Scoring voilà Newton Raphson ici en l'occurrence c'est la même chose ici dans ce cadre ici et on\"), ('020070', \"ce c'est la même chose ici dans ce cadre ici et on voit qu'on gagne vraiment pas mal et on a intérêt à passer à la paralysation mais ce n'est pas aussi trivial que ça là pour le coup ce cadre ici est très simple. J'avais donné un projet il y a quelques années à mes étudiants où je leur demandais de voir le classement avec la régulation PLS, basé sur la régulation PLS. Une grosse partie des étudiants sont partis sur la PLS2 parce que c'était un problème multiclasse que je leur demandais. Ils ont \"), ('020071', \"oblème multiclasse que je leur demandais. Ils ont eu toutes les peines du monde à essayer de paralyser. En fait, non. Jamais pu trouver, vu le temps imparti, vu le temps qu'ils pouvaient consacrer au projet, ils n'ont pas trouvé. Et un seul groupe a eu l'idée de partir sur la PLS1 cette fois-ci. et tu utilises la stratégie One vs. stress classifier, et ça a été très payant. Donc c'est quelque chose qu'il faut regarder en mixant avec les possibilités des outils en termes de paralysation des calcu\"), ('020072', 'tés des outils en termes de paralysation des calculs. Et Numba le fait très bien. Voilà, excellent travail à tous.'), ('030001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de Taichi. Non, je vais parler d'art martial, encore moins de gymnastique, et encore moins d'aspect spirituel. Enfin, voilà, c'est pas trop non plus, voilà, plutôt cartésien ici. Non, je vais parler du travail que je fais avec mes étudiants, c'est ce qui m'intéresse le plus.\\n\\nAlors, qu'est-ce qui se passe? Je demande beaucoup à mes étudiants de travailler en termes de développement. Voilà, ça je le dis tout le temps, un bon développeur n\"), ('030002', \", ça je le dis tout le temps, un bon développeur ne fait pas un bon data scientist, mais un bon data scientist ne peut l'être que s'il est bon développeur. Voilà, c'est ça l'idée. Et je fais beaucoup de travail, mes étudiants, sur des projets, et je me suis rendu compte que, ok, déjà le programme marche, c'est déjà pas mal. Ensuite, il travaille beaucoup sur les algorithmes, c'est ce que je veux également. Il travaille beaucoup sur la structuration et l'organisation du code, c'est également ce q\"), ('030003', \"on et l'organisation du code, c'est également ce que je demande. Et il y a un aspect qui laisse un peu de côté et que j'aimerais qu'ils explorent un peu plus, c'est pour ça que je fais beaucoup de vidéos là-dessus, c'est les technologies. C'est pour ça que ces derniers temps, je n'ai pas arrêté là-dessus. Vous avez vu, là je n'ai vraiment pas arrêté ces derniers temps, c'est que j'ai essayé de montrer un peu les différentes technologies qu'on peut utiliser pour améliorer le temps de traitement q\"), ('030004', \"t utiliser pour améliorer le temps de traitement quand on manipule les grandes bases de données dans le cadre de la data science, data analyse, data engineer, data science, très bien, donc là j'avais montré des solutions pour R, voilà, et pour Python, j'avais mis quand même, parce qu'on travaille énormément sur Python, donc il n'y a pas de surprise là-dessus, un c'est avec Numba, comment accélérer son code avec Numba, un avec Citon, et dernièrement, il n'y a pas trop longtemps là-dessus, j'avais\"), ('030005', \"nt, il n'y a pas trop longtemps là-dessus, j'avais travaillé, j'avais montré comment travailler sur les, comment on pouvait vectoriser les calculs quand on travaille sur des vecteurs et des matrices NUMP, pour rendre les traitements plus rapide, tout simplement.\\n\\nEt toujours dans ce cadre-là, du coup, je vais parler de celui-là, je vais parler de Taichi. Alors Taichi, c'est un package, c'est un package pour Python qui permet d'améliorer ses temps de calcul. C'est écrit ici, la page, elle est là.\"), ('030006', \" de calcul. C'est écrit ici, la page, elle est là. Très bien. Et pour l'installer, bien sûr, il pourrait aller là, tout simplement. Donc j'ai regardé la dernière version, elle est du 23 décembre 2024. C'est un peu récent, on est d'accord. On est le 27 décembre, là. 2024. Oui, d'accord. Très bien. Donc, j'ai lu ça en détail. J'ai lu ici, j'ai vérifié les versions de Python, j'ai vu que ça marche avec Python 3.11, donc j'ai créé un environnement Python 3.11 et j'ai installé ce package de Taichi. A\"), ('030007', \"thon 3.11 et j'ai installé ce package de Taichi. Alors, sur la page de Taichi, il y a des choses intéressantes. Moi, ce qui m'a surtout... Bon, il y a les possibilités, ainsi de suite. Speed up Python, c'est ça qui m'a intéressé. Mais c'est surtout cette partie-là qui m'a vraiment torpolé. Alors déjà un il est facile à utiliser, c'est ce qu'on va voir. Il y a deux aspects qu'on va essayer de voir ici. Un, il améliore les performances même sur du CPU. Voilà, de toute manière moi les machines que \"), ('030008', \"CPU. Voilà, de toute manière moi les machines que j'ai là, s'il y a bien une carte graphique sinon on ne peut rien afficher, on est d'accord, mais c'est de base, qui est intégrée directement, enfin bref, ce n'est pas du tout les super cartes graphiques qu'on peut voir par ailleurs, très bien, donc moi je ne travaille que sur du CPU, ok, mais ils disent ça c'est quand même intéressant ça, on peut avoir du code avec peu de modifications, easy to learn, on peut avoir du code qui surpasse ces écrits\"), ('030009', \"arn, on peut avoir du code qui surpasse ces écrits là, c'est pour moi qui invente. Alors bien sûr, si travailler qu'en interne c'est un problème, moi je veux que c'est travailler sur mes objets quand je programme notamment et c'est ce qui m'intéresse ici, c'est sur les structures NAMPI matrice et vecteur, ce sont les tensors de PyTorch, puis également il peut interagir avec des images, voilà, les étudiants travaillent sur le traitement d'images, je le sais ça très bien, ou bien également pour fa\"), ('030010', \"je le sais ça très bien, ou bien également pour faire des graphiques avec PyTorch, bon là c'est moi mais bon, là Numpy ça m'intéresse PyTorch, les tensors de PyTorch ça m'intéresse et les images, les structures d'images chargées avec Pillow, ça également ça m'intéresse, donc il y a beaucoup de choses à gratter ici. Alors moi dans ma vidéo ici je vais m'en tenir qu'à Numpy mais clairement, là je parle à mes étudiants très rapidement, regardez en détail parce qu'il y a des choses très intéressante\"), ('030011', \"étail parce qu'il y a des choses très intéressantes. Alors c'est pas le propos aujourd'hui mais quand même j'ai aussi noté cet aspect-là, c'est à dire qu'on peut développer des applications avec Taichi et les déployer sans avoir à déployer avec Python. Donc quand vous allez faire vos images Docker, la moralité, pas besoin de mettre Python là-dedans même si Python l'irait puisqu'on l'installe. Voilà, on peut déployer apparemment sans avoir à faire accompagner votre programme par piston. Voilà, ça\"), ('030012', \" accompagner votre programme par piston. Voilà, ça j'attends de voir, j'attends de voir. Là, pour le coup, peut-être que je vais faire une, je vais regarder, je vais explorer ça, parce que ça, ça m'intéresse énormément. Mais bon, pour l'instant, je m'en tiens uniquement à cet aspect-là, à les performances. Est-ce que ça améliore les performances? Voilà, et l'autre aspect, c'est je vais interagir avec des structures numpy. J'aurais pu le faire avec PyTorch aussi bien sûr, on est d'accord là-dessu\"), ('030013', \"c PyTorch aussi bien sûr, on est d'accord là-dessus ou bien avec Dimash, chargez avec Pilo, ainsi de suite. Du coup ma référence en réalité c'est cette vidéo-ci Python rapide avec Numba, d'ailleurs les titres je vais mettre Python rapide avec Taichi tout simplement pour bien voir la différence entre les deux très bien, et très rapidement je manipule les vecteurs NumPy qui est ici, et ma référence ça va être Numba, qui affirme la même chose aussi. Alors, lui, ce n'est pas surpassé. C'est qui appr\"), ('030014', \" Alors, lui, ce n'est pas surpassé. C'est qui approche, là. On va voir cela. Très bien. Alors, avant de commencer, bien sûr, parlons des environnements. Là, pour le coup, il n'est pas installé de base, Taichi. Donc, du coup, j'ai créé l'environnement Taichi, que je mettrai en ligne, comme vous pouvez vous récharger avec le notebook. Et pour avoir la liste des composants, des packages qui sont avec, voilà, on le voit ici. Taichi, il est là. Il est là, ici, voilà. Et par la même occasion, j'ai ins\"), ('030015', \" là, ici, voilà. Et par la même occasion, j'ai installé aussi NumPy et NumBan. Voilà, j'ai fait vraiment minimum, et j'ai la Python 3.11 ici. J'ai travaillé là-dessus. Et l'idée, c'est qu'on va utiliser un algorithme en O2N2, quadratique, pour chercher le minimum. C'est vraiment chercher la petite bête, on est d'accord là-dessus. Moi, ce qui m'intéresse, c'est les comparaisons de temps de calcul. Voilà, c'est juste ça. Donc, c'est pour ça que j'ai créé un algorithme super, enfin, de complexité q\"), ('030016', \"i créé un algorithme super, enfin, de complexité quadratique, pour chercher un minimum. Ce n'est pas le truc à faire, on est absolument d'accord là-dessus. Alors, il est là. Mon notebook est là, j'ai chargé le notebook, je viens de l'exécuter, j'ai effacé toutes les sorties, et on voit bien, c'est l'environnement qui est ici, avec un Python 3.11, qui est là. Donc j'affiche la version de Python toujours, j'affiche la version de NumPy qui a été installée, c'est la 2.0.2, et ensuite, je vais génére\"), ('030017', \"tallée, c'est la 2.0.2, et ensuite, je vais générer 5000 valeurs au hasard. Donc là également j'ai mis le seed, le random stat si vous voulez, pour que ce soit totally reproductible, très bien et je calcule le minimum donc voilà ici la valeur minimum de toute manière là on a un générateur aléatoire selon une loi uniforme entre 0 et 1 donc le minimum il est proche de 0 tout simplement alors j'ai écrit un programme de recherche de minimum là je me suis tiré une balle dans le pied mais bon c'est po\"), ('030018', \"suis tiré une balle dans le pied mais bon c'est pour la beauté du geste, on est bien d'accord là dessus qu'est-ce qu'il fait? C'est un algorithme qui est un brique de boucle où on regarde le premier élément, on va tester tous les autres si les plus petits, ensuite on regarde le deuxième élément, on va tester les autres si les plus petits, on regarde le troisième élément, on va tester les autres si les plus petits, ainsi de suite jusqu'au dernier. Aussi bête que ça. Donc forcément, c'est en haut \"), ('030019', \" Aussi bête que ça. Donc forcément, c'est en haut de N2 dans les meilleurs ou dans les pires cas. Le meilleur cas et le pire cas ici, puisque je passe tout en revue. Bien, allons-y. Alors, je l'exécute sur mon vecteur, on doit avoir la même valeur qu'ici. On est d'accord là-dessus. Là, j'ai utilisé la fonctionnative de NumPy pour chercher le minimum. Donc, normalement, je dois trouver la même chose. Allons-y. Et comme mon algorithme n'est absolument pas optimisé, ça prend un peu plus de temps. O\"), ('030020', \"ent pas optimisé, ça prend un peu plus de temps. On est d'accord là-dessus. Mais on a bien le même résultat. Ça, c'était super important. Vérifiez toujours vos implémentations. La première chose quand on programme, c'est que le programme doit calculer juste. Sinon, on arrête. Ça ne sert à rien de programmer. On est d'accord là-dessus. Après, on finance. On essaie d'améliorer, on raffine, et notamment, parmi les éléments qui m'intéressent moins, c'est l'optimisation des temps de traitement. Mais \"), ('030021', \"'est l'optimisation des temps de traitement. Mais ça, ça vient après. D'abord, s'assurer que le programme est safe, c'est ça qui est important. Alors, je mesure le temps de calcul, comme ça prend un peu de temps, et que je ne veux pas rester des heures à regarder quelque chose calculé, surtout qu'il y a la vidéo qui est enregistrée en même temps, donc ça pompe des ressources, je ne fais que trois répétitions. J'appelle mon min, et il va prendre 4 secondes 56 en moyenne. Voilà. Donc le temps de r\"), ('030022', \" secondes 56 en moyenne. Voilà. Donc le temps de référence pour nous, si on utilise la compilation à la volée de standard de Python où il crée du bytecode en fait, et après il y a un runtime qui le lance. Donc il a compilé en bytecode là, c'est pour ça que je l'ai appelé une fois ici, pour être sûr que il l'a bien mis en mémoire en bytecode. Et c'est le bytecode, c'est la fonction compilée si vous voulez, qui va lancer ici. Très bien, 4 secondes 56 secondes. Voyons ce qui se passe avec Taichi. A\"), ('030023', \"56 secondes. Voyons ce qui se passe avec Taichi. Alors, j'importe la version de Taichi que j'ai installée. Donc ici, c'est la 1.7.3. Très bien. Et je initialise Taichi avec un CPU. On aurait pu utiliser le GPU. Quand il n'y a pas de GPU, il utilise le CPU quand même. Bon, moi, mon GPU, il n'est pas... Donc, j'utilise quand même le CPU. Voilà. Alors, la fonction min, maintenant, je la rééclate. C'est écrit à la mode Taichi. Donc, je lui dis, il y a deux modes. Il y a le func, ti-func, donc arroba\"), ('030024', \"a deux modes. Il y a le func, ti-func, donc arrobas, donc directive de compilation de ti-func, c'est les fonctions internes qu'on va utiliser. C'est une sorte de fonction privée, si vous voulez. Et les ti-kernels, comme celui qui est là, c'est les fonctions publiques, qu'on peut appeler directement via le Python standard. Donc, il y a deux modes d'utilisation des fonctions, soit les func, qui sont les fonctions internes que nous, on appelle via nos autres fonctions, soit kernel et les fonctions \"), ('030025', \"os autres fonctions, soit kernel et les fonctions kernel qui sont publiques si vous voulez, qui sont exposées et qu'on peut utiliser dans notre programme Python. C'est ce que je vais faire ensuite ici. Très bien. Alors qu'est-ce qui se passe? Ce que demande Taichi tout simplement, c'est qu'il veut qu'on type pour qu'il puisse mieux fonctionner. Donc il y a deux typages. Ici, l'élément que j'ai pris en interne c'est un vecteur numpy et le résultat que je veux renvoyer c'est le minimum. Ça il en p\"), ('030026', \" que je veux renvoyer c'est le minimum. Ça il en parle dans la documentation. C'est ce que je montre ici. Donc, il dit, voilà, est-ce qu'on peut interagir avec les tableaux externes? La réponse est oui. Voilà. Soit on crée un objet Taichi qu'on lui passe en paramètre. Donc, vous créez un amont avant d'appeler la fonction. Vous créez l'objet Taichi et vous le passez en paramètre. Soit vous le passez directement ici avec ce type-là. C'est ce que j'ai utilisé, vous avez vu. Voilà. Pour dire que c'e\"), ('030027', \"ai utilisé, vous avez vu. Voilà. Pour dire que c'est un vecteur numpique que j'utilise. C'est exactement ce qu'il dit ici. Alors, il a dit, mais attention, en fait, il fait un passage par référence. Il en parle ici. C'est un passage par référence. Du coup, si l'objet pointé est modifié, c'est répercuté à l'extérieur. Et là, je tombe sur une limitation de Tai Chi que je n'avais pas dans Numba. Rappelez-vous Numba, j'ai mis ici, il est là. J'ai repris le... Dans Numa, j'allais chercher le minimum \"), ('030028', \"ris le... Dans Numa, j'allais chercher le minimum ici, en triant les données cette fois-ci. Et comme je ne voulais pas que le vecteur initial soit trié, je fais une copie locale. Et pour créer la copie locale, j'utilise un MPI pour faire une copie locale. Ça, Taichy, il n'en veut pas. Taichy, il n'en veut pas, en réalité, parce qu'il le dit dans la documentation, déjà, on ne peut pas créer nous-mêmes on ne peut pas créer un... Voilà. Dans l'espace Taichi, on ne peut pas créer des nouveaux tablea\"), ('030029', \"e Taichi, on ne peut pas créer des nouveaux tableaux. Ça, c'est une sacrée limitation. Donc, le code que je fais là, lui, il n'en veut pas. D'autant plus que NumPy n'est pas non plus une librairie Taichi. Donc là également, l'appel de NumPy n'en veut pas. En fait, j'ai essayé de passer par des listes. Et quand j'ai fait des appends sur la liste, il a planté. Alors, j'ai passé un bon bout de temps, je peux vous dire. Et c'est le vrai problème de Taichi, c'est qu'il est très susceptible sur les ob\"), ('030030', \"aichi, c'est qu'il est très susceptible sur les objets qu'il peut manipuler en interne. Donc, il faut être très... Parce qu'il y a plein de démos, là. Ici, regardez, voilà, use case. Il y a plein de use case. Get started ici. Ensuite, use case. Et il y a des super exemples qui font des factales et tout ça. Moi, je veux bien, mais ça ne m'intéresse pas les fractales. Moi, ce qui m'intéresse, c'est manipuler les objets, les données, que j'ai soit importé des fichiers, soit généré à partir d'une im\"), ('030031', \"mporté des fichiers, soit généré à partir d'une image, soit une image elle-même, ainsi de suite. Donc, il faut me dire exactement qu'est-ce que je peux faire et qu'est-ce que je ne peux pas faire. Et je me suis rendu compte, du coup, qu'il ne fait qu'un passage par référence et qu'on n'a pas le droit de créer une copie locale et qu'on ne peut pas utiliser les librairies externes à l'intérieur des fonctions Taichi. Il faut le savoir. Il faut le savoir. Très bien. Ici, là, je m'en tiens vraiment a\"), ('030032', \"voir. Très bien. Ici, là, je m'en tiens vraiment au strict minimum pour le coup. Oui, d'accord. Donc ça, il faut le dire. Et il faut le savoir en lisant la doc. Je ne me suis pas léché la doc. Je peux vous dire que j'ai passé un temps monumental à lire la doc pour bien comprendre ce qui se passe. En plus, il y a très peu d'exemples en ligne. Les seuls exemples qu'il y a, c'est sur le site de Tai Chi. Donc, à chaque fois que je faisais une requête de Google, je suis retombé sur le site de Tai Chi\"), ('030033', \" de Google, je suis retombé sur le site de Tai Chi. Donc, au bout d'un moment, j'ai lu quasiment en entier la doc de Tai Chi. Donc, très bien. Mais bon, il y a des choses intéressantes. Notamment sur la paralysation. Ça, peut-être qu'on verra un peu plus tard. Pas aujourd'hui, mais ça fait des choses qui seraient pas mal qu'on regarde un peu. Très bien. Donc, on fait un passage par référence seulement, et on ne peut pas créer des copies locales. Et les appels, on ne peut pas créer des copies loc\"), ('030034', \"Et les appels, on ne peut pas créer des copies locales, et les appels via des librairies externes dans les fonctions Taichi, on ne peut pas. Même les listes, on ne les voulait pas. Même les listes, ils ne voulaient pas. Très bien. Là, pour le coup, ça marche. Alors, il y a un autre aspect également que je voulais dire, c'est que par défaut, en fait, ils paralysent les boucles. En tout cas les boucles externes et moi je ne veux pas paralyser ici moi ici c'est totally séquentiel mon programme donc\"), ('030035', \"oi ici c'est totally séquentiel mon programme donc je ne veux surtout pas qu'il le paralysse quoi que ce soit là, sinon ça ne marche pas mon programme d'accord, donc je lui dis non, pas de paralysation des boucles c'est ce que fait cette option là mais donc par défaut le for in range, en tout cas quand c'est la boucle externe il le paralysse si ce qu'on veut c'est nickel, si ce n'est pas ce qu'on veut ben non, il ne faut pas le faire donc c'est ce que j'ai fait moi ici J'ai refusé. Très bien. Do\"), ('030036', \"e que j'ai fait moi ici J'ai refusé. Très bien. Donc, je lance. On a bien le même minimum. Sinon, ça ne va pas. Je mesure le temps de calcul. 3,16 millisecondes. Vous avez vu ça? On était à 4,56 secondes. 4,56 secondes. Et on est passé à 3,16 millisecondes. On peut voir. C'était combien? 4560 ms divisé par 3,16. Ya! Ya! Bon, là ça vaut le coup. Ça vaut le coup, on est bien d'accord là-dessus. Tous les embêtements que j'ai vécu à essayer de comprendre comment ils fonctionnent, à essayer de me cad\"), ('030037', \"ndre comment ils fonctionnent, à essayer de me cadrer par rapport à ses spécifications, le résultat, il est là. Ça vaut quand même un peu le coup. On est d'accord là-dessus. Alors, je me suis dit, mais c'est bien. Pour le coup, là, c'est bien. On est d'accord, vous avez vu. Le ratio, là, il est monumental. Mais je me suis dit, qu'est-ce qu'il fait par rapport à Numba? Là, j'avais fait une vidéo là-dessus. Si Numba, réellement, je le trouve formidable. Je le trouve vraiment formidable. Même par r\"), ('030038', \"able. Je le trouve vraiment formidable. Même par rapport à Citon, je le trouve formidable. Pourquoi? Parce que les modifications de code qu'on peut mettre en place sont vraiment très minimes. Extrêmement minimes. Dans Citon aussi, il fallait faire du typage. Du coup, ça... Et dans Citon, il faut installer un compilateur aussi. Ça également, c'est un peu restrictif. Numba, c'est zéro modification. Et on peut utiliser des objets numpy, enfin des objets Python, dans du code Numba. Donc, je me suis \"), ('030039', \"jets Python, dans du code Numba. Donc, je me suis dit, est-ce qu'avec les mêmes choses, toutes choses égales par ailleurs, comment se comporte Numba? Donc, j'avais installé PuneBas, la dernière version c'est le 0.60, bien sûr. J'ai repris le code ici, centipage, vous avez vu, centipage, avec juste la directive de compilation, entre guillemets, compilation. Voilà, centipage, donc c'est vraiment le même code qu'initialement. Voilà, je charge en mémoire, je lance une fois, voilà, on a bien toujours\"), ('030040', \"oire, je lance une fois, voilà, on a bien toujours le minimum, et je mesure le temps de calcul. 2,7 millisecondes. On était à 3,16. Ici, on passe à 2,7. Donc, c'est pas mal aussi. En tout cas, dans ce cadre-ci, Numba. On est d'accord là-dessus. Mais on a différentes solutions. C'est à nous de choisir les meilleurs. Moi, ce que je dis, moi, ce que je vous dis, c'est que je vous rejoins à regarder ces différents aspects-là. Ça me paraît intéressant, notamment sur Computer Vision. Notamment sur Com\"), ('030041', \", notamment sur Computer Vision. Notamment sur Computer Vision. C'est pas comme si je n'ai pas de lit. et regarder également ce qu'il met en avant ici. Essayez de voir ce qui est intéressant là-dessus. Moi, je pense qu'il y a des choses à en tirer à deux aspects. Un, c'est la paralysation des calculs. Ça me paraît important d'explorer cet aspect-là. Et cette histoire de déploiement, sans Python, ça également, j'aimerais qu'on regarde pas plus mal, qu'on regarde d'un peu près pour savoir qu'est-c\"), ('030042', \", qu'on regarde d'un peu près pour savoir qu'est-ce qu'on peut en faire. Voilà, excellent travail à tous....\"), ('040001', \"Bien, c'est parti. Dans cette vidéo, je vais parler de la vectorisation des calculs sur de grandes structures de données, principalement des vecteurs et des matrices avec NumPy. Nous allons voir plusieurs outils et packages sous Python, dont NumPy, Numba et Xarray.\\n\\nNumPy est un élément fondamental de Python lorsqu'on manipule des données organisées en vecteurs ou en tableaux. Numba permet de compiler votre code pour qu'il soit aussi rapide que s'il avait été rédigé en C ou en Fortran. Xarray, q\"), ('040002', \"'il avait été rédigé en C ou en Fortran. Xarray, quant à lui, est une surcouche de NumPy avec des fonctionnalités supplémentaires, notamment l'étiquetage, mais ce qui m'intéresse ici, c'est la vectorisation des calculs, qui permet d'obtenir des gains de performance impressionnants en modifiant à peine le code.\\n\\nJe travaille actuellement sur l'optimisation des calculs, car mes étudiants doivent réaliser des projets. J'ai remarqué que souvent, ils se concentrent sur le fait que le code fonctionne \"), ('040003', \"se concentrent sur le fait que le code fonctionne et calcule correctement, ce qui est déjà une bonne chose. Ensuite, ils s'efforcent de rendre le code maintenable, c'est-à-dire en suivant les principes de bonne conduite et en organisant le code de manière à faciliter la maintenance prédictive, évolutive et corrective. Si le code est ingérable et illisible, il sera difficile de le maintenir.\\n\\nUn autre aspect important est l'optimisation du code pour le rendre plus rapide. Par exemple, un calcul q\"), ('040004', \"ur le rendre plus rapide. Par exemple, un calcul qui prend 10 heures peut être réduit à 10 minutes grâce à des optimisations. Pour cela, il faut des connaissances en programmation, en algorithmie, et une bonne maîtrise des langages et des technologies, en particulier des packages qui conviennent à ce que l'on fait.\\n\\nJ'ai déjà mentionné Citon pour Python, Numba et RCPP pour R. Pour R, j'enseigne également la programmation R et j'ai un cours sur l'utilisation du code C++ dans R via le package RCPP\"), ('040005', \"utilisation du code C++ dans R via le package RCPP. Dans ce cours, je parle des vecteurs, tableaux et matrices sous R. Lorsque vous manipulez ces structures, il est souvent tentant d'utiliser des boucles, mais il est préférable d'utiliser des outils dédiés pour vectoriser les calculs, comme les fonctions apply.\\n\\nCette même idée peut être transposée sous Python avec NumPy. En effet, NumPy offre des outils pour vectoriser les calculs sur des vecteurs et des matrices. Par exemple, la fonction Vecto\"), ('040006', \"rs et des matrices. Par exemple, la fonction Vectorize permet d'appliquer une fonction de transformation sur chaque élément d'un vecteur.\\n\\nPour illustrer cela, je vais générer un vecteur de valeurs aléatoires reproductibles et appliquer une fonction de transformation sur chaque élément du vecteur. La fonction de transformation que j'utiliserai est la suivante : si le scalaire est inférieur à 0,1, le résultat est 1 ; sinon, s'il est inférieur à 0,3, le résultat est le carré du scalaire ; s'il est\"), ('040007', \"3, le résultat est le carré du scalaire ; s'il est compris entre 0,3 et 0,8, le résultat est une autre transformation ; et s'il est supérieur ou égal à 0,8, le résultat est une autre valeur.\\n\\nJe vais d'abord tester cette fonction sur un scalaire pour m'assurer qu'elle fonctionne correctement. Ensuite, je vais l'appliquer sur chaque élément du vecteur en utilisant une boucle. Pour mesurer le temps de traitement, je vais itérer l'opération 10 fois.\\n\\nEnsuite, je vais tester une autre méthode en uti\"), ('040008', \"\\n\\nEnsuite, je vais tester une autre méthode en utilisant une compréhension de liste, qui est censée être plus rapide. Enfin, je vais créer un vecteur NumPy avec la bonne taille et le remplir directement avec des accès indicés pour éviter les allocations mémoire successives.\\n\\nJe vais mesurer le temps de traitement pour chaque méthode et comparer les résultats. Je m'attends à ce que la vectorisation soit plus rapide que les boucles, mais je veux m'assurer que cela est bien le cas.\\n\\nEn conclusion, \"), ('040009', \"assurer que cela est bien le cas.\\n\\nEn conclusion, utiliser des boucles sur des matrices et des vecteurs avec NumPy fonctionne, mais on peut faire mieux en utilisant la vectorisation. En vectorisant les calculs, on peut obtenir des gains de temps de calcul impressionnants. Il est donc important de connaître et d'utiliser ces outils pour optimiser les calculs sur de grandes structures de données.\\n\\nMerci d'avoir regardé cette vidéo. N'hésitez pas à poser des questions en commentaire et à vous abonn\"), ('040010', \"poser des questions en commentaire et à vous abonner pour plus de contenu sur l'optimisation des calculs et la programmation en Python.\"), ('050001', \"Bien, c'est parti. Dans cette vidéo, je vais parler de Cython pour Python. L'idée, globalement, est d'intégrer du code C dans un programme Python pour obtenir des performances plus rapides, en compilant des morceaux de code. C'est l'idée de base. Mais cela va un peu plus loin que cela. Donc, la page web est là, siton.org, vous l'avez là, siton.org, vous avez les différents éléments qui sont là. Très bien. Et on va essayer de voir l'intérêt de ces fonctionnalités-là. Alors, prenons un peu de recu\"), ('050002', \" fonctionnalités-là. Alors, prenons un peu de recul pour bien situer les choses.\\n\\nQuand nous développons des applications, que ce soit en R ou en Python, ce sont les principaux langages utilisés en Data Science. Il y a différents aspects importants. Un, le code doit être sûr, sans erreur. Deux, il doit être maintenable, dans le temps, il ne doit pas s'arrêter, vous n'avez pas fini de développer, c'est fini, c'est mort. Non, il y a une suite, un programme vit au fil du temps, il y a les maintenan\"), ('050003', \"rogramme vit au fil du temps, il y a les maintenances correctives, les maintenances évolutives, enfin tout un tas de choses. Et il doit également être performant, c'est-à-dire avec une certaine rapidité d'exécution. Vous avez un programme pour le même traitement, vous avez un programme qui met 10 heures et un autre programme qui fait 10 minutes, c'est absolument pas la même chose. Surtout que nous manipulons souvent des consulumétries et que le temps de traitement, parfois, a une importance cruc\"), ('050004', \"emps de traitement, parfois, a une importance cruciale.\\n\\nAlors, pourquoi? Je parle de pourquoi? Parce que R et Python sont pseudo-compilés, généralement ça génère du bytecode, du coup c'est déjà plus rapide que l'interprété de base, mais dans certaines situations, on doit pouvoir intégrer du code vraiment compilé cette fois-ci, généralement ça va être du C, qu'on va appeler, avec lequel on va interagir dans notre code natif, qui est soit R ou soit Python. Alors pour R, justement, j'en avais parl\"), ('050005', \"t Python. Alors pour R, justement, j'en avais parlé ici. Donc on utilise un package RCPP qui permet d'intégrer du code C++, C ou C++ dans notre code R. Et aujourd'hui, on va essayer de voir comment on peut mettre en place ça avec Python.\\n\\nAlors, Cython, c'est plus complexe que ça. Plus complexe que ça, c'est parce qu'en réalité il y a deux aspects : c'est une extension de Python déjà avec certaines évolutions, on va parler dans les versions 3 récentes, et c'est aussi un dispositif qui permet de \"), ('050006', \"entes, et c'est aussi un dispositif qui permet de compiler, donc ça veut dire qu'il faut absolument qu'on mette en place notamment des librairies et notamment également un compilateur. Je vais revenir là-dessus. Alors, je suis allé sur TchadGBT, j'ai vu qu'il y a une version TchadGBT gratuite là, on n'a pas besoin de s'inscrire, vous avez vu, je ne me suis pas inscrit là. Oh là là! Bien. Alors, je vais demander, qu'est-ce que c'est-on? Donc, il m'a expliqué à peu près ici, qui est pas mal. Alors\"), ('050007', \"'a expliqué à peu près ici, qui est pas mal. Alors, il montre des exemples. Et dans l'exemple qui est donné ici, on doit créer un fichier à part. Voilà. Avec également un fichier setup qui permet de compiler. Donc, votre code est dans un fichier à part. Il faut créer un fichier setup pour pouvoir le compiler. C'est ce qu'il est expliqué ici. Voilà. Bon, moi, je trouvais ça intéressant. Mais moi, je trouvais plus intéressant pour ma part de ne pas avoir à manipuler tout ça. Et de manipuler des fi\"), ('050008', \" avoir à manipuler tout ça. Et de manipuler des fichiers additionnels et de pouvoir mettre directement dans un notebook les différents éléments pour fonctionner. Et de manière transparente, il y a le dispositif qui va mettre en place la compilation pour qu'on puisse disposer des fonctions compilées. Très rapide. Voilà. Alors, bon, le site web, il est là. Voilà, donc je suis allé voir ce tutoriel, je suis allé voir les éléments de langage. Il y a aussi peut-être un article que j'ai trouvé intéres\"), ('050009', \"aussi peut-être un article que j'ai trouvé intéressant, c'est sur le monde informatique. Voilà, je fais des recherches Internet, comme je le fais habituellement. Et dans cet article du monde informatique, qui date de juillet 2023, ils expliquent les nouveautés de Cython 3.0, justement, 3.x, qui montrent les différents éléments qui sont intéressants ici. Notamment, maintenant, il y a deux manières d'utiliser Cython. Soit on écrit vraiment du code C, avec des interactions avec Python, soit on écri\"), ('050010', \"C, avec des interactions avec Python, soit on écrit du code Python qui reste interprétable par Python lui-même. Voilà. mais qui permet de bénéficier du dispositif. C'est ce qui est décrit ici. C'est qu'il y a deux modes possibles, du coup, pour l'utilisation de Cython. Soit on utilise le mode pur Python, totally compatible avec l'interprèteur Python, soit avec quand même des types statiques, ce qui est décrit ici. Soit on utilise Cython à l'ancienne, si vous voulez, où on écrit vraiment du code \"), ('050011', \"nne, si vous voulez, où on écrit vraiment du code C. Voilà, bon, différentes manières de voir les choses. Moi, pour le coup, dans les tutos qu'ils ont mis en ligne, là, il y a les deux modes possibles. C'est expliqué ici. Pur Python, voilà, et Cython natif, à l'ancienne, si vous voulez. Très bien. Bon, moi, comme je voulais absolument rentrer vers le C, pour qu'on puisse voir réellement la puissance de l'affaire, je suis allé sur le mode Cython. Voilà, très bien. Mais sachez qu'il y a une deuxiè\"), ('050012', \"Voilà, très bien. Mais sachez qu'il y a une deuxième possibilité ici. Donc ça, c'est intéressant. Pour les projets, là, c'est à vous de voir. C'est à vous de voir pour vos projets. Là, pour le coup, je parle deux secondes à mes étudiants. Pour vos projets, c'est à vous de voir. Le plus important, c'est que le programme doit être safe, sans erreur, on est d'accord là-dessus, et que ça marche très bien. C'est ça l'idée. Donc, n'allez pas vous lancer dans un truc où vous ne maîtrisez pas, et après,\"), ('050013', \"r dans un truc où vous ne maîtrisez pas, et après, ça ne marche pas, ou... Alors, avant de commencer, bien sûr, j'ai créé un environnement. Donc, j'ai mis le fichier IAML que je mettrai en ligne. Voilà. Donc, j'ai créé un environnement que j'ai appelé CITON, en CITON. Très bien. J'ai mis Python 3.11, ici, parce que j'ai vu qu'avec 3.12, il y avait des embrouilles que je n'avais pas envie de gérer. Voilà. Donc, je me suis resté à 3.11 ici. Donc, attention au 3.12. Il y a des trucs, je ne suis pas\"), ('050014', \"ttention au 3.12. Il y a des trucs, je ne suis pas sûr que... Enfin, je ne voulais pas perdre du temps là-dessus. Mais c'est à vous de voir. C'est à vous de voir. Très bien. Donc, il y a l'environnement qui est là. Et ensuite, j'ai installé NumPy parce que je vais manipuler des vecteurs, tout simplement. Et j'ai installé Cython. Ça, c'est le premier aspect. Et l'autre aspect également, c'est qu'il va compiler du code, du coup. Voilà. Du coup, voilà. Il fallait installer un compilateur. Donc, je \"), ('050015', \"là. Il fallait installer un compilateur. Donc, je suis allé sur Visual Studio Microsoft, là, et j'ai chargé Build de Toulouse. Donc, je l'ai chargé, je l'ai installé, et j'ai installé surtout le compilateur. J'ai installé surtout le compilateur qui sera appelé. Donc, tous les fichiers intermédiaires, là, comme je vais travailler dans un notebook, tous les fichiers intermédiaires,.pix et tout ça, là, ils le gèrent tout seul de manière transparente. C'est ça l'intérêt de l'affaire, de travailler a\"), ('050016', \". C'est ça l'intérêt de l'affaire, de travailler avec un notebook. Voilà. Bon, j'ai fini mon préambule, là. On va pouvoir commencer. Allons-y, alors. Donc, mon notebook, il est là. Eh bien, j'ai créé un notebook. Et bien sûr, j'ai le kernel, là, c'est l'environnement que j'ai créé. Vous êtes d'accord là-dessus? Vous êtes d'accord là-dessus? Alors, bon, je vérifie la version de Python, 3.11. Je vais vérifier la version de NumPy, c'est 2.2.1. Je vais dire que j'ai 3 000 valeurs. Donc, le notebook \"), ('050017', \"is dire que j'ai 3 000 valeurs. Donc, le notebook sera en ligne. On est d'accord là-dessus. Et pareil pour les fichiers IAML. Donc, vous aurez la possibilité de répéter à l'identique tout ce que je dis là. Et bien sûr je génère un vecteur de valeur aléatoire 3000 voilà alors qu'est-ce qu'il se passe? je vais calculer un extrémum en utilisant un triabule en complexité O2N2 on est bien d'accord là-dessus complexité quadratique alors il y a deux paramètres il y a le vecteur que je manipule qui sera\"), ('050018', \"amètres il y a le vecteur que je manipule qui sera un vecteur numpy très bien et ça aide à y près 0 si je veux renvoyer le minimum et ça aide à y près des différentes 0 je renvoie le maximum j'en avais parlé dans cette vidéo-ci. Ou cette fois-ci, j'utilise une autre solution. Là, pour le coup, c'est Numba que j'utilise. Là, on garde le Python vraiment tel quel, sans excitation C, et avec des directives, tout simplement, de compilation Numba, ça réduit les temps de calcul dans un facteur phénomén\"), ('050019', \"éduit les temps de calcul dans un facteur phénoménal. Donc ça, c'est une autre solution possible. Regardez la vidéo que j'ai mise en ligne ce matin. On voit la date ici, à 23 décembre 2024. Je l'avais fait à 10 heures. ok, donc voilà l'idée c'est tout simplement d'avoir un algorithme qui n'est pas très rapide en réalité, et de voir si on gagne en passant sur des codes compilés, c'est ça l'idée donc très bien, je le charge en mémoire, et je calcule l'extrémum sur mon vecteur ça prend un certain t\"), ('050020', \"e l'extrémum sur mon vecteur ça prend un certain temps ça a généré des valeurs aléatoires entre 0 et 1 selon une loi uniforme, donc on a bien ici la valeur qui est proche de 0, le minimum on est d'accord, si je mets 1 ici il m'enverra le maximum Alors, je vais déterminer le temps de calcul en utilisant TimeIt. Voilà, je lance TimeIt. Très bien, donc il est en train de répéter plusieurs fois l'opération pour voir combien de temps prend cette fonction. Parce qu'il y a toujours des artefacts avec l\"), ('050021', \"ion. Parce qu'il y a toujours des artefacts avec les différents éléments qui tournent en même temps. Voilà, très bien. Donc, le temps de calcul de ma fonction là, sur le vecteur de 3000 observations, de 3000 valeurs et d'une seconde 47. Ok. Alors, je vais passer en Cython. Alors là, je vais faire une première chose. Déjà, il faut que j'importe la librairie. Ok. Très bien. Ensuite, le deuxième élément, c'est que je vais introduire dans mon notebook la possibilité de compiler des cellules, des chu\"), ('050022', \"k la possibilité de compiler des cellules, des chunks, des cellules en Cython. Donc ça, ça permet d'avoir l'extension qui permet de compiler en Cython certaines cellules. OK. Et donc, qu'est-ce que je fais? Je reprends exactement la même fonction sans rien modifier pour l'instant. Et je dis que ce code-là, je vais le compiler. Là, on est sur du Python brut, il n'y a rien de changé, on est d'accord là-dessus. Voilà. Donc il est en train de compiler, ça prend un peu de temps. il me dit, il l'a com\"), ('050023', \"r, ça prend un peu de temps. il me dit, il l'a compilé, il l'a mis dans ce dossier là vous avez vu  Donc, c'est users et Python, c'est users, Rico et Python, voilà, très bien. Il l'a mis dans ce dossier-là, et ça y est, il a généré le code. Donc, il a compilé, la librairie est en fait compilée. Très bien. OK. Donc, ça, c'est important, ça. Alors, si on ne travaillait pas dans un notebook, et qu'on n'utilise pas, ici, l'instruction magique, la directive magique, Cython, c'est à nous de créer les \"), ('050024', \"ective magique, Cython, c'est à nous de créer les fichiers intermédiaires. Et ensuite, on lance dessus le compilateur. Voilà. Bon. C'est quand même plus simple. Vous prenez une première commande ici pour dire que je veux pouvoir utiliser l'instruction magique Cython. Ensuite, vous mettez en place l'instruction magique Cython et vous prenez le code. C'est quand même mieux que d'avoir à se trimpaler tout un tas de fichiers à côté. On est d'accord là-dessus. Dans certains cas, quand vous déployez v\"), ('050025', \"à-dessus. Dans certains cas, quand vous déployez votre application, la question peut se poser aussi. On est d'accord là-dessus. Mais c'est à vous de regarder ça. Donc là, j'ai repris le code tel quel, vraiment tel quel. Je l'ai compilé en utilisant Cython. On va voir si on gagne. Donc là, c'est 6 extrémum V1. Je lance. Voilà. Et ensuite, je mesure les temps de calcul. Voilà. Alors, on va voir. Donc, tout à l'heure, rappelez-vous, c'était 1 seconde 47. Qu'est-ce que ça donne ici? 1 seconde 11. Ça\"), ('050026', \"e 47. Qu'est-ce que ça donne ici? 1 seconde 11. Ça gagne pas mal, quand même. Regardons. 1.47 divisé par 1 seconde 11, on a gagné. On a divisé. Oui, très bien. Le ratio est ici. Voilà. On a gagné. Très bien. Mais ça, ils le disent. Ils le disent justement ici. Voilà. Ils le disent tout simplement que, voilà, on peut gagner un peu en compilance, mais on ne gagne pas suffisamment en réalité, parce qu'on n'a pas défini les types statiques et tout ça, comme on le fait en C. La déclaration de type es\"), ('050027', \", comme on le fait en C. La déclaration de type est vraiment très importante en réalité, parce que ça permet au compilateur de mieux fonctionner, tout simplement. Sinon, c'est lui qui fait tous les contrôles. Alors, encore une fois, il y a deux manières de faire, c'est ce que j'expliquais ici. Soit on l'utilise compatible avec Python, mais avec quand même des déclarations statiques, sans utiliser le Cython avec des déclarations C cette fois-ci. C'est ce que moi j'ai choisi de faire. Donc j'ai re\"), ('050028', \"'est ce que moi j'ai choisi de faire. Donc j'ai repris ma fonction là, très bien, et j'ai réécrit. Donc de nouveau ici, la cellule c'est du code Cython, très bien, et j'ai réécrit ici. Donc cette fois-ci j'ai typé les différents éléments. Donc j'ai typé les paramètres, donc là c'est un vecteur de double, là le site c'est un entier, et les variables locales là, voilà, j'ai tous déclaré avec cdf. encore une fois, il y a deux manières de faire soit en utilisant le pur Python compatible avec l'inter\"), ('050029', \"en utilisant le pur Python compatible avec l'interpréteur Python soit vraiment passer sur du C cette fois-ci, une extension de C enfin, Cython quoi avec des déclarations de variables donc j'ai mis les déclarations de variables ici, très bien avec même les indices qui sont là et je compile donc de nouveau il va compiler en créant les fiches intermédiaires voilà, il a créé d'ici et il a mis les différents éléments. Très bien. Alors, il m'a dit qu'il y a un warning, mais bon, ça a fonctionné, je vé\"), ('050030', \"l y a un warning, mais bon, ça a fonctionné, je vérifiais. Alors, quand vous avez compilé une fois, quand vous recompilez une deuxième fois, vous n'avez plus le message ici, parce que ça y est, c'est prêt. Donc, je suis allé dans le dossier, j'ai tout supprimé, donc on voit bien les différents éléments qu'il a mis en place. Donc, qu'est-ce qui a changé ici avec Cython? Ce qui a changé, tout simplement, c'est que j'ai rajouté un petit page. Voilà. Et comme ça, voilà, le compilateur va mieux compr\"), ('050031', \" Et comme ça, voilà, le compilateur va mieux comprendre ce qu'on est en train de manipuler et il a moins besoin de faire les contrôles. Ça, c'est vraiment important parce que moi qui suis un férus pendant très longtemps, j'ai enseigné le Pascal, le C++, Java, et en ce moment même, j'enseigne le C Sharp. Et le typage est très important parce que ça permet de mieux guider le compilateur en réalité et ça permet de mieux optimiser son code. Et quand on travaille avec des langages à typage dynamique \"), ('050032', \"on travaille avec des langages à typage dynamique de genre R et Python, j'avoue que bon, mais après, voilà, on fait avec, on fait avec. Très bien, mais donc là, pour le coup, on revient plus vers les langages à l'ancienne, peut-être, je ne sais pas comment dire ça. Ok, une fois, alors, il reste ici, je ne l'ai pas typé d'ailleurs, vous avez vu, j'ai défini ici, voilà, que j'ai fait une copie locale, n et tout ça, mais reste, je ne l'ai pas typé d'ailleurs. Donc là, ça lui fait un contrôle en plu\"), ('050033', \"'ailleurs. Donc là, ça lui fait un contrôle en plus. Là, pour le coup, j'aurais dû mettre ici un cdf double, on est d'accord là-dessus, mais bon, ça il s'est compilé, je ne vais pas m'embêter là-dessus et bien, vérifions ce que ça donne ça renvoie le même résultat, on est d'accord là-dessus et vérifions le temps de calcul alors vérifions le temps de calcul, on regarde bien très bien, ça est en train de faire les calculs, là il est en train de mesurer les temps de calcul, donc il fait un peu plus\"), ('050034', \"urer les temps de calcul, donc il fait un peu plus de boucles, comme c'est un peu plus rapide il fait un peu plus de boucles, voilà, et il va nous dire voilà, 17,2 millisecondes, vous voyez ça là, la version initiale de Python est de 1,47 secondes. Je suis passé en Cython et j'ai rajouté quelques typages, tout simplement. Rien de plus. Et on est passé à 17,2 millisecondes. Regardons ça. Voilà. 1470 1470 divisé par 17,2. 85 fois plus rapide. Ça vaut le coup, on est d'accord là-dessus. Là, ce qui \"), ('050035', \"ut le coup, on est d'accord là-dessus. Là, ce qui est très intéressant, c'est qu'on n'a pas besoin de créer des fichiers à côté. C'est ça l'intérêt. Il suffit simplement qu'on mette l'instruction magique ici qui permet de définir une cellule Cython. Alors, ceux qui ont un peu programmés dans les langages compilés, justement, connaissent les directives de compilation. Là, pour le coup, ici, il y a deux éléments sur lesquels on peut jouer. Par exemple, il y a un premier élément sur lequel on peut \"), ('050036', \"ple, il y a un premier élément sur lequel on peut jouer, c'est de dire que je n'ai pas besoin de mettre en place le contrôle et débordement d'indices. Je maîtrise les indices, je ne vais jamais déborder ici parce que je sais programmer, tout simplement. Du coup, j'enlève ces contrôles-là. Ça va simplifier les instructions machine. Autre élément important également, c'est qu'en Python, on peut mettre des indices négatifs pour partir à rebours. On est d'accord là-dessus. Là, comme je programme en \"), ('050037', \"est d'accord là-dessus. Là, comme je programme en C, je n'ai pas besoin de mettre en place ça. Donc là également, j'enlève cette possibilité-là. De nouveau, ça va de nouveau simplifier le code compilé. Donc les instructions machine. Allons-y ici. Voilà, donc j'ai compilé, là, il est en train de compiler, là, très bien. Et je relance. Bon, c'est rapide, relançons, timide. Et on va voir que par rapport à la version sans directives de compilation, on gagne méchamment. Rappelez-vous, sans directives\"), ('050038', \"n gagne méchamment. Rappelez-vous, sans directives de compilation, c'est des 17,2, avec les directives de compilation qui enlèvent des contrôles, en fait, sur les débordements d'indices et sur les indicages négatifs, Voilà, on a divisé par deux. Par rapport à la version initiale, du coup, on a divisé à peu près 200. Voilà, donc ça vaut le coup. Ça vaut le coup, surtout quand on a des portions de code qui prennent beaucoup de temps et qui jouent le rôle du goulot d'étranglement. Ça vaut le coup d\"), ('050039', \"e rôle du goulot d'étranglement. Ça vaut le coup d'essayer de voir ces extensions-là. Ces extensions, ça existe pour R. Voilà, ça existe également. On vient de le voir aujourd'hui avec Python, avec Cython. Sachant que sous Python, il y a une autre possibilité. N'oubliez pas ça. N'oubliez pas ça. L'autre possibilité, c'est de passer par Numba. Ou cette fois-ci, vous ne modifiez en rien votre code. Pas besoin de réécrire du code. Il suffit d'utiliser les directives de compilation Numba. Ça, c'est \"), ('050040', \"er les directives de compilation Numba. Ça, c'est encore une autre solution là. Mais c'est assez formidable. Là, le gain, il est vraiment pas mal non plus. Enfin, également. Le bien est vraiment pas mal également. Voilà. Excellent travail à tous.\"), ('060001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de Numba. Numba est un package pour Python qui permet de compiler du code Python en instructions machine. Compiler, c'est bien écrit là, j'invente, il y a là. Compiler, voilà, compiler. Ce qui est très intéressant ici, c'est que du coup on a avec du code Python des performances du même niveau que les vrais langages compilés, comme le peuvent être par exemple C ou Fortran. C'est exactement ce qu'il y a annoncé ici. Ce qui est assez bluffa\"), ('060002', \" ce qu'il y a annoncé ici. Ce qui est assez bluffant quand même.\\n\\nAlors, je vais essayer de resituer tout ça pour que tout le monde comprenne bien les idées. Ce n'est pas parce qu'on est bon programmeur qu'on est forcément bon data analyst, data scientist, data engineer. C'est pas vrai. En revanche, un bon data analyst, un bon data scientist, un bon data engineer doit savoir bien programmer. Sinon, il sera cantonné au clic-bouton, à la merci des outils, et il ne peut jamais aller plus loin. Alor\"), ('060003', \"outils, et il ne peut jamais aller plus loin. Alors, bien programmer, ça veut dire quoi? Ça veut dire déjà connaissance de la programmation, ce qui serait pas mal déjà. Les connaissances des principes de l'algorithmie, par exemple les complexités algorithmiques, il faut les connaître. Vous avez une complexité linéaire d'un côté, une complexité quadratique de l'autre, même exponentielle, vous savez pas, vous allez bien, il faut faire les bons choix, on est bien d'accord là-dessus. Donc il y a tou\"), ('060004', \"x, on est bien d'accord là-dessus. Donc il y a tout un tas d'éléments pour savoir bien programmer, bien organiser son programme notamment, ainsi de suite, et il faut aussi connaître les technologies. Et ça, ça manque un peu parfois. Du coup j'entends des choses qui me font un peu rire, par exemple j'entends dire que Python comme R d'ailleurs sont lents parce qu'ils sont interprétés. Mais attendez déjà, il y a en fait une compilation à la volée qui permet d'avoir du bytecode qu'on lance avec un r\"), ('060005', \"i permet d'avoir du bytecode qu'on lance avec un runtime. Donc déjà les temps de calcul c'est pas la même chose qu'il y a 20 ans si vous voulez parce que les gens restent sur des idées anciennes souvent comme j'ai pu entendre pour Java aussi d'ailleurs très bien, on reste souvent sur des idées anciennes et on ne regarde pas les évolutions récentes. Très bien. Et bon, ça reste un peu... Ce n'est pas du vrai code compilé. Là, pour le coup, on a une solution qui permet d'avoir du vrai code compilé.\"), ('060006', \" solution qui permet d'avoir du vrai code compilé. Alors, on m'a dit qu'il y a des étudiants qui travaillent sur des projets en ce moment. Oui, ils utilisent Python intensivement. Ce serait pas mal de regarder un peu. Alors, il n'y a pas longtemps, d'ailleurs, j'avais fait une vidéo où j'ai dit que, parce qu'ils avaient également un projet sous R à l'époque où ils devaient programmer des algorithmes machine learning sous R et je me suis rendu compte que personne n'avait tenté la solution d'utili\"), ('060007', \"pte que personne n'avait tenté la solution d'utiliser RCPP qui permet d'intégrer du code C++ dans un projet R et du coup on a des temps de calcul qui n'ont absolument rien à voir avec du R standard du coup les portions de code qui sont des goulots d'étranglement ce serait pas mal qu'on puisse les améliorer en utilisant ces solutions technologiques. Alors cette solution-là existe également avec Python, on a Numba. Je vais faire une vidéo sur Numba d'ailleurs dans un avenir très proche, pour montr\"), ('060008', \" d'ailleurs dans un avenir très proche, pour montrer comment ça fonctionne, qu'est-ce qu'il y a d'intéressant là-dedans. Mais avant d'en arriver là, en réalité, il y a une solution qui est déjà pas mal, c'est Numba. Le gros intérêt de Numba, c'est que vous ne touchez pas à votre code, vous ne touchez pas à votre code, vous rajoutez juste des directives de compilation. Et le code va avoir un temps de calcul réduit dans des proportions, mais phénoménal. Vraiment, vraiment. Et c'est ce que je vais \"), ('060009', \"énal. Vraiment, vraiment. Et c'est ce que je vais montrer ici.\\n\\nComme d'habitude, je vais faire la vidéo, je vais montrer tout le code que j'utilise là, je le mettrai en ligne. Comme ça, tout le monde verra bien ce qui se passe. Alors, Numba est là. Il y a quelques éléments ici. Il y a la doc qui est là. Très bien. Voilà, ça permet d'avoir des... Alors, moi, je regarde dans des teladocs. Et en réalité, ce n'est pas la première fois que j'en parle. J'avais fait une vidéo là-dessus il y a 5 ans. V\"), ('060010', \". J'avais fait une vidéo là-dessus il y a 5 ans. Voilà, 5 ans. Donc, je connaissais un peu quand même. J'avais fait un tutoriel, pas une vidéo. J'avais fait un tutoriel là-dessus il y a 5 ans. Et dans le tutoriel, justement, je montrais comment l'utiliser avec un tri par insertion ici, voir les temps de calcul, ainsi de suite. Et l'idée c'est de rajouter tout simplement un décorateur. Alors ce qui est intéressant, c'est que 5 ans plus tard, 5 ans plus tard, bien sûr, Numba a évolué, heureusement\"), ('060011', \" plus tard, bien sûr, Numba a évolué, heureusement. Et j'ai vu d'ailleurs qu'il y a une nouvelle directive de compilation qui est là. Ça c'est intéressant. Moi j'en étais resté à ça, cette solution-là, et j'ai vu qu'il y a maintenant un nouveau décorateur ici. Alors j'ai lu la doc bien sûr, j'ai regardé en détail, Voilà, très bien. Et justement, ils en parlent ici, les spécifications. Ou justement, depuis la version 0.59 de Numba, on a NJIT à la place de JIT. Alors bien sûr, j'ai essayé de regar\"), ('060012', \"place de JIT. Alors bien sûr, j'ai essayé de regarder quelle est la différence entre les deux. Et j'ai trouvé un tuto qui est pas mal, du monsieur Christian, où il explique justement la différence entre JIT et NJIT. Et l'idée, tout simplement, c'est qu'avec NJIT, on est plus rigoureux sur les types. pour que la compilation soit plus efficace. Une fois qu'on a vu ça, on va pouvoir commencer. L'idée, c'est que je vais reprendre la même idée que cette vidéo, que cette vidéo-ci. Je vais utiliser cet\"), ('060013', \"te vidéo, que cette vidéo-ci. Je vais utiliser cette fois-ci le tri par insertion pour voir la différence de temps de traitement avec du Python standard où il y a un JustTentent qui génère du bytecode qui est lancé par un runtime avec du Python compilé, vraiment compilé en langage machine cette fois-ci, en instruction machine cette fois-ci, et on verra la différence de temps de traitement. Alors aussi, il y a un élément que je vais essayer d'exploiter, c'est qu'il y a la possibilité de paralléli\"), ('060014', \"oiter, c'est qu'il y a la possibilité de paralléliser le code, et ça également, c'est une spécificité qui me paraît très intéressante. Et on va essayer d'exploiter ça pour voir vraiment si ça tient la route. Alors déjà, on va vérifier un peu les librairies. Donc moi, j'ai installé au mois de décembre, début décembre, la dernière version d'Anaconda. Et dans l'environnement base, il y a la liste des packages qui sont là. Et parmi la liste des packages disponibles, dans l'environnement base, il y a\"), ('060015', \"ges disponibles, dans l'environnement base, il y a bien Numba. Donc Numba est installé par défaut. C'est ça qu'il faut retenir ici. c'est que Numba est installé par défaut avec l'environnement base de Anaconda. Tout est nickel, il n'y a rien installé en plus. C'est ça qui est intéressant. Vous chargez la distribution Anaconda, vous l'installez, et dans l'environnement base, vous avez déjà Numba. Parce qu'avec Citon, il y aura du boulot. Je vous le dis tout de suite. Je suis en train de préparer \"), ('060016', \"e dis tout de suite. Je suis en train de préparer la démo à essayer de voir pour que ça soit schématique et tout ça. Et il y a quand même pas mal de manip à faire avant qu'on puisse réellement travailler. Là, pour le moins, il n'y a rien à faire. Tout est prêt. Allons-y alors. Donc, je vais réduire ça, parce qu'on n'en a plus besoin. Et, voilà, je redémarre ici pour qu'on soit sûr qu'il n'y a pas de problème. Donc, j'ai installé Anaconda. Voilà, très bien. Et je suis bien dans l'environnement de\"), ('060017', \"très bien. Et je suis bien dans l'environnement de base. C'est ce qu'on voit ici. Alors, du coup, j'ai la version 3.12.7 de Python, c'est toujours important de vérifier les versions. Je vais utiliser NumPy de manière intensive, donc j'ai la version 1.26.4, et je vais générer aléatoirement 5000 valeurs. Voilà. Numérique, on est d'accord là-dessus. Voilà, c'est ce que je fais ici. Voilà, 5000 valeurs avec le générateur de NumPy. J'ai mis un CID ici, 2024, parce qu'on est en train de finir 2024. Vo\"), ('060018', \", 2024, parce qu'on est en train de finir 2024. Voilà. Comme ça, tout le monde pourra reproduire exactement. De toute manière, ce notebook sera en ligne, donc vous pourrez le récupérer. Voilà, donc il a généré le vecteur. Alors, qu'est-ce qui se passe? Vérifiez si Numba est vraiment si efficace que ça. Voilà, je vais programmer une fonction qui calcule l'extrême en utilisant un tri par insertion. C'est-à-dire qu'il prend un vecteur de valeur, il fait un tri par insertion, et il y a un paramètre \"), ('060019', \"fait un tri par insertion, et il y a un paramètre side, si side vaut 0, il renvoie le minimum, si side vaut 1, et si side est différent de 0, il renvoie le maximum. Voilà, c'est pour ça. Très bien. Donc, je fais une copie interne pour que le vecteur ne soit pas manipulé. Très bien. Et ensuite, je fais le tri ici, et je renvoie bien le minimum, si side est égal à 0, et le maximum, les dernières valeurs, si side est différent de 0. Voilà, très bien. Donc, je lance ça. Voilà. Ensuite, je vais créer\"), ('060020', \". Donc, je lance ça. Voilà. Ensuite, je vais créer un problème qu'on va pouvoir paralléliser par la suite. C'est pour ça que j'ai fait ça. Là, paralléliser ça, ça me paraît compliqué quand même. Très bien. J'imagine que les fondus de parallélisation trouveront peut-être une solution pour paralléliser un tri par insertion. Peut-être. Bon, moi, je ne me suis pas risqué à ça. Puis, il faut que, quand je mets quelque chose, quand j'explique quelque chose, il faut que ce soit lisible. Et là, il ne fa\"), ('060021', \"hose, il faut que ce soit lisible. Et là, il ne faut perdre personne. Du coup, j'ai créé un autre programme, enfin, une autre fonction qu'il utilise, Extrémum, qui renvoie à l'étendue. Donc, l'écart entre le max et le min, tout simplement. Et en fait, il y a une boucle où i vaut 0 pour avoir le minimum, et i vaut 1, mais différents 0 en l'entourant. En l'occurrence, pour avoir le maximum. Là, pour le coup, il lance les deux fonctions. Là, il s'est séquentiel. Mais on voit très bien que ça va êtr\"), ('060022', \"t séquentiel. Mais on voit très bien que ça va être très facile à paralléliser, parce que les deux calculs sont indépendants. Les deux fonctions peuvent être appelées de manière indépendante. Très bien. Donc, j'ai une fonction qui renvoie à l'étendue. Il le rend, si vous voulez. Du coup, je vais le lancer sur mon vecteur, sur mon vecteur que j'ai généré au-dessus. Et vous avez un certain temps de calcul. Ça va durer dans les 8 secondes à peu près. Très bien. Et il me renvoie ici l'étendue de max\"), ('060023', \". Très bien. Et il me renvoie ici l'étendue de max. Alors, comme c'est une loi uniforme entre 0 et 1, l'étendue vaut 1, presque quasiment. Je suis là après. Vous êtes d'accord là-dessus. Une fois qu'on a ça, du coup, je vais essayer de déterminer le temps de calcul. Allons-y. Time it. Alors, pendant qu'il s'énerve là-dessus, je vais faire un petit aparté. Donc là, j'ai mon gestionnaire de tâche de Windows, et ma machine, qui est assez vieille, quand je l'ai acheté, elle était très bien. C'est un\"), ('060024', \"and je l'ai acheté, elle était très bien. C'est un Core i7, qui n'est pas terrible, il marche quand même, c'est l'essentiel, il y a 8 coeurs. Et la fréquence nominale, en fait, c'est 3 GHz. Mais comme je suis en train de m'énerver sur un seul coeur, il peut m'être en turbo mode. C'est ça. C'est pour ça que la vitesse réelle est plus élevée que la vitesse de base. Il s'est mis en turbo mode simplement parce que je ne fais pas de multitrade. Du coup, l'utilisation par mon programme Python du proce\"), ('060025', \"p, l'utilisation par mon programme Python du processeur tourne autour de 20%. C'est ce qu'on voit ici. Vous voyez bien, autour de 20%. Très bien. Donc là, j'utilise vraiment qu'un seul cœur et je m'énerve méchamment dessus. Il s'énerve méchamment dessus pour calculer. C'est le time hit qui va lancer plusieurs fois la fonction et calculer au fur et à mesure les temps de calcul, enfin calculer justement les durées d'exécution et faire la moyenne après. C'est pour ça que c'est long ici. Mais on voi\"), ('060026', \"rès. C'est pour ça que c'est long ici. Mais on voit bien. Ça, c'est intéressant parce que quand on ne va pas paralléliser, on va voir que cette valeur-là va changer. C'est ça. À la place de 20, il va changer. Allons-y. Il y a Zoom parce qu'en fait, j'enregistre mes vidéos avec Zoom tout simplement. Oh là là, moi, je ne suis pas dans la technique vidéo. Tout ça, je prends les outils les plus simples. J'ai pris le Zoom gratuit. Voilà. Et ensuite, j'enregistre. C'est ça l'essentiel. Pour moi, avant\"), ('060027', \"'enregistre. C'est ça l'essentiel. Pour moi, avant tout, c'est un outil pédagogique. C'est ça l'intérêt ici. Donc, dans mes TD, mes étudiants, ce serait pas mal, qui regardent un peu quand ils ont des problèmes de performance, j'aimerais bien, ce serait pas mal, qui regardent un peu Numba. Et pour voir comment accélérer leur code. C'est ça qui est intéressant. Alors, on a vu qu'avec Python standard, qui fait déjà de la compilation à la volée, sauf qu'il génère du bytecode, pas du code machine. I\"), ('060028', \"f qu'il génère du bytecode, pas du code machine. Il génère du bytecode qui est exécuté par un runtime. Voilà, ça prend 9,18 secondes. OK, très bien. On va voir si on peut améliorer ce temps de calcul-là. C'est ça l'idée. Alors, qu'est-ce qui se passe? Donc, j'importe Numba cette fois-ci, et j'importe la directive de compilation. Une fois que j'importe la directive de compilation, Je reprends la même fonction, exactement la même, sauf que j'ai changé le nom, tout simplement. Et devant, je mets la\"), ('060029', \"ngé le nom, tout simplement. Et devant, je mets la directive de compilation. Voilà. Très bien. Donc là, pour le coup, il va compiler le code. Il va compiler le code. Cette fonction-là va être compilée en langage machine, cette fois-ci. C'est bien ce qui est annoncé ici. C'est ce qui est annoncé ici en grande pompe. On est d'accord là-dessus. Voilà. Ya! C'est quand même pas mal, ça. Numba compilé numérique can approach, speed off, et c'est off ok, on va voir ça alors du coup la même fonction, sau\"), ('060030', \" on va voir ça alors du coup la même fonction, sauf que j'ai changé le nom de la fonction, j'ai rajouté la directive de compilation ici je l'ai chargée en mémoire elle est compilée là, de la même manière l'étendue, qui appelle l'extremum number cette fois-ci on est bien d'accord, je mets également la directive de compilation, voilà, donc maintenant l'extremum est compilé et l'étendue qui utilise d'extrémum et compilé également. Voilà. Du coup, je vais lancer de nouveau. Voilà. Très bien. On a le\"), ('060031', \" vais lancer de nouveau. Voilà. Très bien. On a le même résultat. Heureusement, si on n'a pas le même résultat, on arrête tout. Ça veut dire qu'il y a un problème. Vous êtes d'accord? On a le même résultat. Là, c'est 9,3 secondes quand je l'ai lancée, la cellule, là. Et là, c'est 4 secondes. Déjà, c'est assez prometteur, si vous voulez. On va essayer de déterminer le temps de calcul réel en réitérant l'expérience avec TAMIT. Très bien, je lance ici. De nouveau, je regarde. C'est tellement rapide\"), ('060032', \"ci. De nouveau, je regarde. C'est tellement rapide que je n'ai pas eu le temps de l'afficher. 37 millisecondes. Vous avez vu, là, c'était 9,18 secondes. Et là, c'est 37 millisecondes. Juste en rajoutant les directives de compilation de Numba. Ça arrache la tête. Je trouve extraordinaire qu'on n'en parle pas plus. Parce qu'il y a le langage. Par exemple, il y a des personnes qui me parlent de Julia, que j'ai beaucoup regardé par ailleurs. J'ai beaucoup hésité, j'hésite toujours à changer certains\"), ('060033', \"ucoup hésité, j'hésite toujours à changer certains de mes cours pour utiliser Julia. Parce que Julia, tu ne me dis pas, on fait justement ce type de mécanisme-là. Mais je me dis, mais attends, on a l'équivalent de ça avec Numba, justement. Et c'est ce qu'on voit ici. Et on voit le temps. Vous avez vu? Je vais lancer la calculatrice. Et on est passé de 9,180 millisecondes à 37,2 millisecondes. 246 fois plus rapide. On a réduit les temps de calcul d'un facteur de 246, d'un ratio de 246. juste en r\"), ('060034', \"d'un facteur de 246, d'un ratio de 246. juste en rajoutant deux directives de compilation. Bon, ben voilà. Donc, il faut regarder un peu. Alors, à un moment donné, j'avais dit, ce code-là, en fait, il est parallélisable. On peut lancer de manière indépendante, de chercher de minimum et de maximum sur le même vecteur. Du coup, il y a un outil pour ça. Dans Numba, il y a la fonction pRange. Donc, au lieu de faire une boucle avec un range de 2, qui va générer 0 et 1, je vais utiliser ici parent, qu\"), ('060035', \"va générer 0 et 1, je vais utiliser ici parent, qui va lancer du coup l'extremum number en parallèle. C'est ça l'idée. Donc là, je mets la directive de compilation, et cette fois-ci, je dis que je veux paralléliser le calcul. Je veux paralléliser le calcul. Très bien. Ensuite, c'est bien de le dire, mais si on ne lui donne pas des instructions spécifiques, il ne peut pas en profiter si vous voulez. Il va essayer de lui-même. Mais on peut l'aider nous en programmant mieux les choses, tout simplem\"), ('060036', \"nous en programmant mieux les choses, tout simplement. Et c'est là qu'il faut savoir bien programmer. C'est là qu'il faut comprendre les structures des algorithmes. Sinon, vous lancez, vous ajoutez des instructions sans savoir exactement ce que vous faites. Très bien. Donc ici, la boucle qui va chercher d'un côté le minimum et de l'autre côté le maximum, je vais les lancer en parallèle. Très bien. Et dans la directive de compilation ici, je dis que je veux que le code soit parallélisé. Avec le t\"), ('060037', \"ue je veux que le code soit parallélisé. Avec le type irange que j'ai changé ici, à la boucle. Allons-y. je relance vous avez vu sans parallélisation c'était 1,4 seconde avec parallélisation 0,8 seconde de nouveau je fais un time hit pour avoir une idée plus précise de la durée d'exécution je vais lancer et je lance cette fois-ci et regardez au lieu de s'énerver à 20% il s'énerve à 40% bah oui parce qu'il utilise du coup les deux il a parallélisé les calculs. Et comme il parallélisait les calcul\"), ('060038', \" les calculs. Et comme il parallélisait les calculs, il utilise mieux les ressources disponibles. Voilà, deux fois plus de ressources ici. Et regardez, c'est passé à 18 millisecondes. 18 millisecondes. Rappelez-vous, 37,2. Avec la parallélisation, on a divisé par 2 à peu près, ce qui est normal, parce qu'on a lancé les deux instructions parallèles ici. En utilisant deux ressources qui étaient disponibles par ailleurs. Du coup, non, c'est normal que ça divise par deux le temps de calcul par rappo\"), ('060039', \"ue ça divise par deux le temps de calcul par rapport à la version non parallélisée. Très bien. Alors, qu'est-ce qu'il reste à dire? Ce qu'il reste à dire, c'est que, quand on écrit un programme, il faut connaître les principes de la programmation. Très bien, ça paraît que c'est le B à bas quand même. Et il faut aussi connaître les technologies et les outils. Sinon, on ne peut pas avancer. Et justement, si je parle de Python, avec Python, on peut compiler des portions de code qui nous paraissent \"), ('060040', \"compiler des portions de code qui nous paraissent importantes en utilisant tout simplement Numba. Et vous avez vu que la modification du code est ultra minime. C'est juste rajouter des directives de compilation. Et ça vous permet de diviser le temps de calcul par un facteur monumental. Vous avez vu, si je refais ici, 9,180 divisé par ici c'était combien là 18,000 secondes divisé par 18, 510 on a réduit le temps de calcul d'un ratio de 510 ben voilà ça nous permet d'être plus efficace voilà, exce\"), ('060041', \"là ça nous permet d'être plus efficace voilà, excellent travail à tous.\"), ('070001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de la possibilité d'insérer du code C++ dans un projet, dans un programme rédigé avec le langage R. L'objectif est de rendre l'ensemble plus performant, plus puissant, et plus rapide. C'est ça.\\n\\nL'idée n'est pas nouvelle. Il existe beaucoup de documentation sur le web. Par exemple, ce document-ci m'a vraiment aidé à prendre en main cette idée et à la mettre en œuvre. Il y a aussi des documents comme celui-ci qui sont très intéressants. P\"), ('070002', \"ments comme celui-ci qui sont très intéressants. Par exemple, ce document en ligne parle de la compilation à la volée où il génère du bytecode. Mais on peut aller plus loin en générant du code binaire que l'on peut insérer dans notre projet. C'est l'idée avec RCPP.\\n\\nLe package que nous allons utiliser ici est RCPP. Il y a aussi des documentations comme celle-ci qui sont excellentes. Par exemple, ce document m'a beaucoup servi. Il y a un autre document qui est plus généraliste et qui montre comme\"), ('070003', \"ument qui est plus généraliste et qui montre comment installer les compilateurs. Je vais en reparler plus tard. Sur YouTube, vous trouverez beaucoup de documentation, assez ancienne, qui date de 2000, mais il y en a aussi qui datent de 9 ans.\\n\\nL'idée est la suivante : on a développé un projet sous R, et il y a des portions de code qu'on aimerait optimiser. Ces portions de code prennent beaucoup de temps et sont des goulots d'étranglement. Soit le code est complexe, soit c'est une partie du code \"), ('070004', \" code est complexe, soit c'est une partie du code qui est appelée très souvent. On aimerait les optimiser en les écrivant, en les compilant, et en les faisant compiler en C++. C'est ça l'idée.\\n\\nCette idée m'amuse beaucoup, parce que, en réalité, je l'ai beaucoup regardée il y a une trentaine d'années, quand je développais beaucoup en Pascal. À l'époque, j'utilisais un profiler pour détecter les parties de code qui n'étaient pas optimales, et pour essayer d'améliorer ces bouts de code, je dévelop\"), ('070005', \" essayer d'améliorer ces bouts de code, je développais des inserts en assembleur. J'ai retrouvé des sites web qui expliquent ça, ça me rappelle des temps anciens. J'étais jeune encore à Eplo, et ça me rappelle cette idée d'insérer des bouts de code dans un langage plus performant pour rendre votre projet plus rapide.\\n\\nBien sûr, il y a aussi des solutions plus récentes. Par exemple, j'avais montré qu'on peut développer des DLL, des librairies dynamiques, compilées, dans un langage quelconque, com\"), ('070006', \"miques, compilées, dans un langage quelconque, comme avec Lazarus, et ces DLL peuvent être importées dans un projet en Python, par exemple. Ça participe de la même idée.\\n\\nUne fois ce préambule passé, essayons de revenir à une vue globale. Qu'est-ce qui se passe ? Je fais un cours de programmation à l'université, de programmation R. Un cours de programmation R, je ne prends pas les étudiants à utiliser R, je leur apprends à programmer sous R. Et en définitive, ils doivent développer un package où\"), ('070007', 'n définitive, ils doivent développer un package où ils programment des algos de machine learning. Et pour les guider, je leur indique un ouvrage qui me paraît intéressant, c\\'est \"Programmer de manière efficace sous R\".\\n\\nParmi les solutions possibles, il y a la programmation parallèle. Mais parfois, c\\'est un peu le canon pour tuer la mouche, parce qu\\'en réalité, la programmation parallèle, le problème, c\\'est qu\\'il faut qu\\'on étudie en détail l\\'algorithme et qu\\'on arrive à le décomposer en série d'), ('070008', \"orithme et qu'on arrive à le décomposer en série d'opérations qu'on peut appeler de manière parallèle. Et ça, ça requiert quand même de sacrer compétence, ça reste du domaine de la recherche. Ce ne sont pas tous les algos qui ne sont pas réalisables et arriver à les paralyser correctement en assurant la synchronisation des calculs, ça dépasse largement le cadre de notre cours, souvent, souvent, sur un projet en tous les cas, voilà, ils ont un mois pour le faire, donc bon, très bien. Et moi je le\"), ('070009', \"s pour le faire, donc bon, très bien. Et moi je leur dis, mais il y a des outils qui permettent d'inspecter votre code, notamment, donc j'avais fait un tuto là-dessus il y a longtemps, voilà, c'est là, programme ici, c'est en 2019, où je dis qu'un des outils très intéressants, c'est d'utiliser un profiler. Un outil de profiler permet d'inspecter votre code et de regarder le temps de traitement, le temps d'exécution de chaque bout de code et de voir où sont les goulots d'étranglement. Donc là, j'\"), ('070010', \"ir où sont les goulots d'étranglement. Donc là, j'avais fait un tuto là-dessus. Le tuto, il est là, programmé efficacement sous R. Et notamment, je montre l'utilisation d'un profiler pour détecter les portions de code qui sont gourmandes en temps de calcul et savoir comment on peut en tirer parti pour le rendre plus efficace en le programmant de manière plus efficace. Enfin, plus efficacement, justement. C'est ça l'idée. Cette idée sous R, elle existe également sous Python. Là, je parle du profi\"), ('070011', \"xiste également sous Python. Là, je parle du profiler de Spider cette fois-ci. Donc là, c'est quand ça? C'est en 2020. Ah oui, c'est quand j'avais demandé un projet aux étudiants de L3 à l'époque. Et je leur avais dit, mais il faut que votre programme soit efficace. Pour moi, il y a deux contraintes fortes. C'est bien de développer, mais il faut que ça puisse traiter des grands volumes de données. Et il faut que ce soit rapide. Un programme qui fait 10 secondes et un autre qui fait 10 minutes, c\"), ('070012', \"ait 10 secondes et un autre qui fait 10 minutes, celui qui fait 10 secondes, on imagine qu'il est plus intéressant quand on va l'utiliser réellement. Donc cette idée d'optimiser son code en utilisant des profiler pour voir où sont les goulots d'étranglement, c'est très intéressant. Et ça fait partie des compétences qu'on doit avoir. Une fois qu'on a détecté les goulots d'étranglement, qu'est-ce qu'on peut faire pour améliorer son code ? Alors il y a différentes pistes possibles, il y en a un déj\"), ('070013', \"y a différentes pistes possibles, il y en a un déjà, j'essaie de le réécrire mieux. Ça c'est une possibilité. Mais parfois on arrive aux limites du langage lui-même. Même s'il fait de la compilation à la volée avec des bytes de code, on est limité par le fait que c'est du byte de code en réalité. Et donc ce serait intéressant de pouvoir réécrire cette partie de code qui joue le rôle de goulot d'étranglement en un langage vraiment compilé cette fois-ci. Et en C++ notamment. Et c'est ça l'idée de \"), ('070014', \"is-ci. Et en C++ notamment. Et c'est ça l'idée de RCPP. L'idée de RCPP, c'est de vous donner la possibilité d'insérer du code en C++ dans votre projet. Et du coup, ce code C++ est compilé, et la fonction correspondante, vous pouvez l'appeler par la suite. Avec des gains de temps de calcul mirifiques. Mais oui, pas mal, pas mal. Ça dépend des circonstances, bien sûr, ça dépend de ce qu'on programme. Mais dans l'exemple que je mettrais en ligne, on voit qu'on divise par 40 le temps de calcul. Ce q\"), ('070015', \" voit qu'on divise par 40 le temps de calcul. Ce qui est quand même intéressant.\\n\\nAlors justement, pour montrer ça, je vais programmer le triabule. Je sais très bien que triabule, ce n'est pas optimal du tout, c'est de complexité quadratique si vous voulez. Mais justement, c'est ça l'idée, c'est que sur un bout de code qui est de complexité quadratique, est-ce que vraiment on va gagner ? C'est ça l'idée. Alors pour pouvoir fonctionner, bien sûr, il faut qu'il y ait un compilateur, c'est plus sur\"), ('070016', \"il faut qu'il y ait un compilateur, c'est plus sur votre machine, sinon vous ne pouvez pas fonctionner. Alors, pour installer le compilateur C++, dans ce document-ci, c'est décrit, RCPP for Everyone, j'aime bien le RCPP for Everyone, il montre comment on peut l'installer sur différents systèmes. Et nous, celui qu'on va utiliser, parce que je suis sur Windows, parce qu'une machine d'université, c'est juste pour ça, on va utiliser Rtools. Donc, j'ai installé Rtools. Rtools, c'est un outil qui sert\"), ('070017', \"i installé Rtools. Rtools, c'est un outil qui sert à compiler, notamment les packages. Voilà. Et vous le récupérez là. Et moi, j'ai pris la version 4.4, tout simplement. Voilà. Très bien. 4.4. R4.4. Voilà. Qui est là. Donc, je l'ai installé et il l'a installé ici. Alors, bon, ce n'est pas très cool, parce qu'il conseille de l'installer dans la racine de votre disque C. Je ne suis même pas sûr dans la salle informatique qu'on puisse le faire, ça. Je ne pense pas, en réalité. Voilà. Mais bon. donc\"), ('070018', \"Je ne pense pas, en réalité. Voilà. Mais bon. donc il faut plutôt moi juste c'est sur ma machine personnelle c'est à vous de voir comment vous installez ça mais bon, peut-être qu'on peut déroger cette règle là en tous les cas, j'ai lu la doc dans la doc ils disent, il faut l'installer à la racine je l'installe à la racine donc Rtools 4.4 est là et notamment on a les compilateurs C++ qu'on va pouvoir utiliser très bien donc ça c'est important bien sûr il faut qu'on ait les compilateurs sinon on n\"), ('070019', \" sûr il faut qu'on ait les compilateurs sinon on ne peut pas fonctionner Ensuite, l'installation de RCPP, installe package RCPP, tout simplement. C'est varié le nom, moi, je n'ai pas eu à le faire. Tout simplement, j'ai installé Rtools, et ensuite, j'ai installé le package RCPP. Très bien, allons-y alors. Une fois que c'est bon là, du coup, voilà, j'ai créé mon petit programme. Alors, pour intégrer du code C++, il y a différentes manières de faire. Je vais y revenir après. Donc, dans un premier \"), ('070020', \"e. Je vais y revenir après. Donc, dans un premier temps, j'ai créé un programme qui calcule l'étendue d'un vecteur, donc le max moins le min, après avoir trié le vecteur avec un triabule. Encore une fois, c'est un exercice de style. Vous êtes d'accord là-dessus ? N'allez pas vous énerver avec un triabule. De toute manière, le sort de R le fait très bien. Mais bon, moi, c'était pour montrer l'idée, c'était pour montrer l'intérêt de passer par un code compilé. C'est juste ça. Donc là, j'ai pris le\"), ('070021', \"ode compilé. C'est juste ça. Donc là, j'ai pris le code qui est sur ici. J'ai pris le code, j'ai pris le moins optimisé possible. Voilà, donc c'est celui-là, sans l'optimisation. Et je l'ai traduit ici. Donc il y a une double boucle, donc il y a une complexité quadratique, on est bien d'accord là-dessus. Une double boucle, en réalité, des boucles imbriquées. Voilà. Forcément, c'est en haut de N2. Et à la fin, je renvoie le résultat. Alors, on va le lancer. Donc, on va le lancer là. Très bien. Dé\"), ('070022', \"le lancer. Donc, on va le lancer là. Très bien. Déjà, il faut que je me mette. Où est-ce qu'il doit chercher ? Parce que le fichier C++, il va falloir qu'il le cherche. En fait, mon projet est là. J'en parle rapidement. Donc, ça, c'est le code R que vous voyez en ce moment. Et le code C++ du triabule, mais écrit en C++, j'ai mis dans le fichier CPP. D'accord. Très bien. Donc, je charge ici la fonction, très bien, et ensuite, je génère un vecteur aléatoire de 6000 valeurs. Voilà, donc je vais trè\"), ('070023', \"aléatoire de 6000 valeurs. Voilà, donc je vais très rapidement, vous avez les 6000 valeurs générées avec une loi uniforme, entre 0 et 1. Alors, je vais calculer l'étendue, et en réalité, quand j'appelle la fonction la première fois, donc il appelle la fonction, mais en fait, il la compile à la volée en mémoire, en le convertissant en bytecode. Vous allez d'accord là-dessus. Alors, ça prend un peu de temps, mais on a le résultat qu'on veut obtenir. Voilà le résultat, donc, c'est l'écart entre max\"), ('070024', \". Voilà le résultat, donc, c'est l'écart entre max et min. On comprend à peu près que le max, c'est 1 et le min, c'est 0. Elle allait après, puisqu'on a généré ici de manière aléatoire. Donc, c'est énorme. Ce résultat-là est tout à fait attendu. Une fois que je fais ça, je vais utiliser maintenant, donc, la fonction étendue, elle est en mémoire. Compilée. Elle est compilée en mémoire. C'est un byte de code. Très bien. Donc, OK. Et je vais l'appeler avec MicroBenchmark. J'utilise MicroBenchmark p\"), ('070025', \"er avec MicroBenchmark. J'utilise MicroBenchmark pour mesurer le temps de calcul. Très bien. OK, donc une fois que c'est bon ça, il est en train de le relancer 5 fois, et il va calculer le temps de calcul. Donc on va voir ça, et on a bien ici, il me dit que le temps médian en seconde, en seconde, c'est 1,75. Alors il a bien refait les calculs, parce qu'en réalité je crée une copie locale du vecteur. Donc ce n'est pas le x qui est manipulé, parce qu'après il n'y a plus de tri possible, il n'y a p\"), ('070026', \"qu'après il n'y a plus de tri possible, il n'y a pas besoin d'avoir un tri, on est d'accord. là, pour le coup, c'est bien le x, je fais une copie locale et je trie la copie locale. On est d'accord là-dessus. Donc, il refait bien toutes les opérations. On est d'accord là-dessus. Très bien. Et donc, le temps médian, c'est 1.75. OK. On va écrire le même code en C++ qu'on va compiler. Et ensuite, on va appeler la fonction et on va voir ce que ça donne en termes de temps de calcul. Voilà. Donc, je ch\"), ('070027', \"e en termes de temps de calcul. Voilà. Donc, je charge la librairie RCPP et ensuite, je vais compiler mon code qui réalise le triabule en C++. Donc le code, il est là. Mais pour qu'on ait les deux en même temps, je vais ouvrir dans notre plate++. On voit bien. Qu'est-ce qui se passe ? J'insère l'entête, très bien, rcpp.h. Ensuite, je dis que la fonction usine namespace ici, très bien, c'est plus plusieurs, mais bon, j'ai repris tout simplement les codes que je trouve en ligne, voilà, include des\"), ('070028', \"s codes que je trouve en ligne, voilà, include des cours C++, je l'ai fait il y a très très très longtemps je suis passé à C Sharp maintenant, dans mes enseignements mais bon, j'ai suffisamment de souvenirs de C++, il faut comprendre exactement ce que j'écris bon, j'ai tellement programmé aussi dans différents langages enseigné différents langages que bon, je vois à peu près ce qui est important et bien donc il y a l'include, il y a l'usine qui est là et je dis que la fonction que je vais écrire\"), ('070029', \"st là et je dis que la fonction que je vais écrire là, je veux l'exporter alors déjà il y a un premier élément important, c'est qu'il faut qu'on regarde les équivalences entre les types, voilà, donc là ici x c'est un vecteur, un vecteur de R, donc on utilise le type numérique vecteur. Là pour le coup il faut bien mettre les types dans la fonction, et x ici j'ai mis x, voilà, très bien. Ensuite qu'est-ce qui se passe ? Voilà, c'est une fonction qui va renvoyer un double, ok, qui va calculer le ma\"), ('070030', \"i va renvoyer un double, ok, qui va calculer le max et le min et qui renvoie les 40 les deux. Ok, ensuite je récupère la longueur du vecteur, c'est ce qui se passe ici, très bien, voilà. et ensuite, je clone le vecteur. C'est l'équivalent de cette instruction, la ligne 5 ici, on clone bien les vecteurs, on crée une copie locale. On est d'accord là-dessus, pour ne pas avoir manipulé le vecteur initial. Ensuite, tout le reste est pareil. On a bien deux boucles imbriquées, avec exactement les mêmes\"), ('070031', \"deux boucles imbriquées, avec exactement les mêmes instructions. Et à la fin, les indices commencent à 0 en C++. C'est pour ça qu'il y a un décalage ici. Par rapport à là, là ici, mais on a bien les différents éléments. Alors ce code-là, du coup, il faut que je le compile et que je le charge dans R. C'est ça l'idée. Très bien, c'est ce qui se passe. Donc ce code-là, il est là, il est dans le dossier, il est ici. Il faut que je le charge, c'est ce qui se passe ici, et que je le compile. C'est ce \"), ('070032', \" qui se passe ici, et que je le compile. C'est ce qui se passe ici. Alors, si vous regardez un peu la documentation, il y a une autre manière de faire. C'est indiqué ici, même ici plutôt alors la stratégie que j'utilise c'est j'écris un fichier C++ sur le disque et je l'importe et je le compile il y a une autre manière de faire c'est d'utiliser le cpp fonction et de décrire votre code en chaîne de caractère là on n'a pas besoin des includes, on n'a pas besoin des exports donc on se dit que c'est\"), ('070033', \"'a pas besoin des exports donc on se dit que c'est plus simple mais c'est la première idée que j'avais eue mais en utilisant cette solution-là, je me suis rendu compte en réalité que s'il y avait des bugs, par exemple il y a des mauvaises instructions et il y a des bugs, il envoie des messages qui ne sont absolument pas lisibles. Et du coup, on ne comprend pas ce qui se passe. En revanche, quand on crée un fichier.cpp explicite, avec les includes et tout ça, et qu'on lance le compilateur dessus,\"), ('070034', \" et tout ça, et qu'on lance le compilateur dessus, s'il y a des erreurs de compilation, vous recevez des messages qui sont explicites avec des indications sur la partie du code qui est mise en cause. et du coup en termes de débourgage c'est nettement mieux donc mon conseil à moi c'est créer plutôt un fichier C++ voilà et ensuite mettez les différents éléments pour pouvoir le manipuler il suffit de reprendre les 3 éléments qui sont là tout simplement et ensuite vous le sourcez et quand vous le so\"), ('070035', \"ent et ensuite vous le sourcez et quand vous le sourcez, s'il y a des problèmes à la compilation vous obtiendrez des messages explicites c'est ça l'idée j'ai vérifié plein de fois donc là ça marche il est en train de le sourcer là, il l'a compilé ça y est on a un programme compilé en mémoire là pour le coup c'est pas un bytecode c'est vraiment compilé en C++ alors du coup je vais appeler la fonction une fois pour vérifier qu'on a le même résultat qu'ici si on a pas le même résultat il faut s'inq\"), ('070036', \" qu'ici si on a pas le même résultat il faut s'inquiéter là très bien oui on a exactement le même résultat et du coup on va lancer le benchmark en lançant la fonction 5 fois Encore une fois, il fait un vrai tri parce qu'il a cloné le vecteur. Donc, il répète bien le tri, en réalité. Il a bien cloné les vecteurs. On est bien d'accord. Très bien. OK, je lance ici. C'est super rapide. Et là, du coup, le temps médian aux cinq itérations, pour cinq essais, c'est 40 millisecondes. Alors, si on lance r\"), ('070037', \"sais, c'est 40 millisecondes. Alors, si on lance rapidement, regardons. ici c'est 1754 millisecondes puisque c'est en secondes donc en millisecondes c'est 1754 divisé par 40 est égal 43 fois plus rapide donc ça montre bien que là pour le coup c'est plus intéressant alors du coup si on termine cette histoire Alors, quand on développe des projets R, il faut le faire bien. On est d'accord là-dessus et il faut écrire son code correctement et le rendre le plus rapide possible. Néanmoins, parfois, on \"), ('070038', \"e le plus rapide possible. Néanmoins, parfois, on peut avoir des goulots d'étranglement. Alors, si on a des goulots d'étranglement, il y a différentes solutions possibles. La paralysation, l'écriture du code, l'étude de l'algorithme pour voir comment le contourner pour le rendre plus efficace. On peut faire des thèses là-dessus. Et il y a une solution qui est très simple, tout simplement. c'est d'utiliser le package RCPP pour pouvoir tourner votre code R en code C++ et de le compiler. Et c'est l\"), ('070039', \"e code R en code C++ et de le compiler. Et c'est la fonction compiler que vous appelez dans votre programme. Et ça permet de lever plus ou moins les différents goulots d'étanglement qu'il pourrait y avoir dans le programme. On ne peut pas faire tous les codes comme ça, bien sûr, mais des parties de code sensibles, ça peut être une idée de le rendre plus efficace en utilisant la possibilité de réécrire ces bouts de code qui ralentissent votre programme en C++. Voilà, excellent travail à tous.\"), ('070040', 'ramme en C++. Voilà, excellent travail à tous.'), ('080001', \"Bien, c'est parti. Dans cette vidéo, je vais parler de la possibilité de combiner R et Python dans un projet. Nous allons créer un notebook R, Markdown. À l'intérieur de ce notebook R, nous allons utiliser des instructions Python via le package Reticulate. J'ai mis la page qui est là. Il est assez intéressant, et nous allons essayer de voir les fonctionnalités qu'il propose.\\n\\nRevenons un peu en arrière pour prendre du recul et essayer de voir les choses. Pour moi, tout le monde me connaît, je n'\"), ('080002', \" choses. Pour moi, tout le monde me connaît, je n'ai pas d'attachement particulier aux outils. Il faut trouver l'outil le plus efficace par rapport à ce qu'on veut faire et par rapport aux contraintes qui s'imposent. C'est l'idée. Une fois qu'on a dit ça, bien sûr, il y a aussi un environnement. Il faut écouter ce que disent mes étudiants, et il faut aussi écouter ce que disent les entreprises qui embauchent mes étudiants. Mais une fois qu'on a le choix, c'est à nous de savoir ce qu'on veut fair\"), ('080003', \"e choix, c'est à nous de savoir ce qu'on veut faire et dans ce contexte-là, quels sont les outils les plus appropriés. C'est toujours mon idée.\\n\\nIl y a un autre aspect également qui pour moi est très important : on ne doit pas faire une religion d'un outil. Dans certains cas, il peut être avantageux de combiner des outils. Par exemple, il n'y a pas longtemps, j'avais montré qu'on peut faire appel à du code R dans NIME, un logiciel que je trouve vraiment pas mal du tout, et on peut aussi faire ap\"), ('080004', \"raiment pas mal du tout, et on peut aussi faire appel à du code Python dans NIME. Il n'y a pas de jaloux, c'est vraiment un aspect qui peut être intéressant.\\n\\nVoilà, donc c'est dans ce cadre-là que je me suis intéressé à Reticulate. C'est un outil qui n'est pas récent, on est d'accord là-dessus. Ça fait plusieurs années déjà, j'en avais un peu entendu parler. Mais bon, il y a toujours un moment où il faut qu'on regarde dans des tests ce que c'est. Et c'est pour moi, ce moment-là, c'est aujourd'h\"), ('080005', \". Et c'est pour moi, ce moment-là, c'est aujourd'hui. En réalité, c'est que je travaille beaucoup sur Huguenface en ce moment pour voir ce que je peux en faire. J'ai fait plusieurs projets sous Python et je me suis dit que je dois pouvoir le faire sous R. Donc j'ai trouvé un paquet de HuguenfaceR que j'ai essayé de faire fonctionner, mais je n'ai jamais pu le faire fonctionner. À cette occasion-là, j'ai vu qu'il s'appuyait en réalité sur Reticulate, ce package-là. Et du coup, je me suis dit qu'i\"), ('080006', \"te, ce package-là. Et du coup, je me suis dit qu'il faut que je regarde Reticulate. Si vous voulez avoir plus d'infos, il y a des documentations sur internet. Par exemple, il y a des vidéos qui montrent comment l'utiliser, comme celles d'Air Ladies de Baltimore, qui est vraiment une série de vidéos sur leur site que je trouve très intéressante.\\n\\nBon, il est 2020, on voit bien que c'est pas récent tout ça. C'était il y a quatre ans. Il y en a d'autres également. Donc mon idée à moi, c'est de voir\"), ('080007', \"tres également. Donc mon idée à moi, c'est de voir ce qu'on peut en faire et pour que mes étudiants puissent s'en inspirer et voir également comment ils peuvent fonctionner là-dessus dans leurs projets.\\n\\nOften, les outils sont imposés dans les projets. Quand on leur donne des projets, on impose des outils pour qu'ils prennent en main. Mais il y a des projets également où ils ont le choix des outils. Et là, du coup, souvent, ça se joue entre R et Python. Et l'idée de pouvoir combiner R et Python \"), ('080008', \"Python. Et l'idée de pouvoir combiner R et Python me paraît intéressante. Pour qu'on puisse fonctionner, il faut qu'il y ait une distribution Python sur votre machine, ainsi que R et RStudio. Je vais utiliser RStudio, bien sûr. Il faut également qu'il y ait une distribution Python sur votre machine. Moi, j'utilise Anaconda. Mon Anaconda à moi est là, il est à cet endroit-là. Vous avez vu le chemin, il est sur mon disque dur C, le disque C, utilisateur. Ensuite, Rico, c'est moi, donc vous, c'est \"), ('080009', \"ateur. Ensuite, Rico, c'est moi, donc vous, c'est votre nom. Et ensuite, Anaconda 3. Et dans Anaconda 3, on a accès, en fait, à l'environnement base, qui est déjà pas mal. Dans l'environnement base, il y a toute une série de paquets déjà disponibles, des paquets de data science. Vous avez déjà City Learn qui est installé par défaut. Vous avez Matplotlib, vous avez Pandas. Vous avez une série d'outils qui sont déjà intéressants, qui sont disponibles. OK, très bien.\\n\\nNumPy, SciPy, SciPy, enfin CPy\"), ('080010', \"es. OK, très bien.\\n\\nNumPy, SciPy, SciPy, enfin CPy, enfin le truc quoi. Voilà, ainsi de suite. Alors l'autre possibilité intéressante également, c'est qu'on peut faire appel à un environnement. Par exemple, suite à ma série de vidéos sur Huguenface, j'avais créé un environnement Transformers, parce que j'utilise le package Transformers. Et donc dans mon projet R, là, avec Reticulate, je peux également faire appel à cet environnement. Et du coup, à cet environnement-là, et du coup, profiter des p\"), ('080011', \"à cet environnement-là, et du coup, profiter des paquets qui ont été installés dans l'environnement. C'est tout à fait possible. Les gammes. Alors, ce n'est pas ce que je vais faire ici. Moi, je m'en tiens à l'outil de base, donc je plug sur l'environnement base d'Anaconda, mais sachez qu'il y a les commandes pour le faire. Vous avez, par exemple, ici, vous avez useCondaEnv, et vous mettez dans votre environnement, et ça marche. J'ai testé, j'ai testé. Donc, j'ai mis ici un useCondaEnv, et j'ai \"), ('080012', \"testé. Donc, j'ai mis ici un useCondaEnv, et j'ai mis en transformers et ça a parfaitement marché. Là, je vais vraiment dans l'aspect le plus simple et j'utilise cette commande-là, très simple. Mais bon, on a bien cette solution-là. OK. Allons-y alors.\\n\\nVoilà mon notebook qui est prêt. Très bien. Comme d'habitude, bien sûr, il faut que vous ayez installé Reticulate. Donc, vous mettez install.package Reticulate. Très bien. Ensuite, une fois que je charge ici le package, je dis la version de Pytho\"), ('080013', \" charge ici le package, je dis la version de Python que je vais utiliser. Et la version de Python, j'avais dit, justement, c'est ici, il va le trouver là, le Python.exe, il va le trouver là, et ça correspond à l'environnement base, donc avec déjà pas mal de data science, notamment City Learn. Très bien, donc je veux afficher la version de Python qu'il a chargée, pour être sûr. C'est ce que je fais ici. Donc voilà, il charge la librairie, ensuite il fait la connexion avec Python, j'affiche la con\"), ('080014', \"il fait la connexion avec Python, j'affiche la configuration du Python qu'il a utilisé. Effectivement, il me dit le Python qu'il a utilisé avec les librairies, l'exécutable, le chemin, la version de Python. Du coup, moi, c'est la 3.12.7 que j'ai installée. Et l'architecture, ainsi de suite, avec la version de NumPy. Pour vérifier que ça marche, je lance ici. Effectivement, je lui envoie l'instruction 3 puissance 3. Il me envoie 27 avec les commandes à la Python. Donc, ça y est, je suis prêt. Une\"), ('080015', \"des à la Python. Donc, ça y est, je suis prêt. Une fois que c'est bon, je vais pouvoir lancer mon traitement.\\n\\nAlors, qu'est-ce que je vais faire? En réalité, je vais faire une combinaison de deux traitements. Je vais prendre ce fichier-là. C'est le fichier Art Disease. Il y en a plein de versions sur Internet. J'en ai pris une possible que j'ai un peu simplifiée. Il y a 209 observations, parce qu'il y a la première ligne, c'est la ligne des noms de variables. On a une variable cible qui est là,\"), ('080016', \" de variables. On a une variable cible qui est là, qui est qualitative, binaire, donc c'est l'occurrence d'une maladie cardiaque. Très bien, soit positif, la personne est malade, soit négatif, la personne n'est pas malade. Et ensuite, on a les descripteurs qui sont là, les variables explicatives, qui sont de type quelconque. Du coup, qu'est-ce que je vais faire? Je vais lancer d'abord une analyse factorielle des données mixtes, une sorte de généralisation de la CP où les variables sont soit quan\"), ('080017', \"alisation de la CP où les variables sont soit quanti, soit quali. C'est une sorte de généralisation aussi de la CM, en réalité, pour pouvoir me projeter dans un espace factoriel. Et là, je vais utiliser FactoMiner, qui est un package pour R. Et une fois que j'ai la description factorielle des individus, je ne vais prendre que le premier plan factoriel, je vais lancer une régression logistique. Voilà. Prédire la variable cible qu'elle a sur la base des variables qui correspondent aux variables la\"), ('080018', \"e des variables qui correspondent aux variables latentes, axe 1, composante 1 et composante 2. Très bien. Et là, pour le coup, je vais utiliser Scikit-learn. Alors, c'est un pur exercice de style, on est d'accord là-dessus. Mais le message ici qui est important, c'est de dire qu'un des intérêts d'avoir à disposition R et Python, c'est qu'on peut choisir les meilleurs packages selon leur domaine. C'est ça. Il y a des paquets qui sont excellents sous R, mais pas très bons sous Python, et à l'inver\"), ('080019', \"us R, mais pas très bons sous Python, et à l'inverse, il y a des paquets qui sont excellents sous Python, mais pas très bons sous R. Et la possibilité de combiner R et Python me permet justement de tirer parti de la possibilité d'utiliser ces deux langages à même temps.\\n\\nAlors encore une fois, c'est un exercice de style, parce que si je vais sous Python, par exemple, il y a un excellent paquet qui fait l'analyse factorielle, c'est Scientist Tools. Voilà et justement dans Scientist Tools, vous av\"), ('080020', \". Voilà et justement dans Scientist Tools, vous avez bien les factures des données mixtes donc voilà très bien que j'aurais pu ne pas utiliser FactoMiner de R, on est d'accord là-dessus. Donc c'est vraiment un exercice de style. On est bien d'accord là-dessus. Bien. Donc, ce qui est important, c'est de bien voir les commandes qui sont là. R, ça veut dire que dans Markdown, ce chunk-là, ça correspond à R. Ensuite, si je mets Python, pour le coup, là, ce sera un chunk où j'appelle du code Python. \"), ('080021', \"là, ce sera un chunk où j'appelle du code Python. Et il va l'interpréter en passant par Reticulate. C'est ça l'idée. C'est le canal qu'il va utiliser. Allons-y alors.\\n\\nPrêtement sous R, d'abord, j'importe les données. J'affiche les données. Voilà la description des données. Donc 1, 2, 3, 4, 5, 6. 1, 2, 3, 4, 5, 6. Là, on a les descripteurs. Et this is la variable de cible. Donc là, j'affiche. Il va prendre bien les descripteurs de age jusqu'à exercice en jina. Une fois que c'est bon là, je charg\"), ('080022', \"rcice en jina. Une fois que c'est bon là, je charge un FactoMiner qu'il faut installer. Et je lance la liste factorielle des données mixtes et je veux obtenir le premier plan factoriel. C'est tout. Pas besoin de graphique, je n'ai pas besoin. C'est ce que je fais ici. J'ai des valeurs propres et il me dit que sur les deux premiers facteurs, on accumule 42% de l'information disponible. Très bien. Là, ça reste discutable, combien de facteurs il faut prendre, ainsi de suite. Mais bon, ça je vous la\"), ('080023', \"t prendre, ainsi de suite. Mais bon, ça je vous laisse. Moi, mon objectif, c'est de montrer la combinaison et répitons. Bon, j'affiche les coordonnées des individus, voilà, et je projette les individus dans le plan factoriel pour avoir un graphique, voilà le graphique en question. Très bien, excellent.\\n\\nAlors, qu'est-ce qui se passe du coup? Je vais créer un data frame où je prends les coordonnées factorielles, dimension 1 et dimension 2, c'est ce que je fais ici, donc voilà mon data frame. Et à\"), ('080024', \"e que je fais ici, donc voilà mon data frame. Et à ce data frame-là, voilà, je vais adjoindre la variable cible disease. Voilà, très bien. Excellent. Voilà, c'est ce data frame-là que je vais maintenant passer sous Python et sur lequel je vais lancer la régression logistique de Scikit-learn. Alors, qu'est-ce qui se passe? Là, maintenant, on est sur du code Python. Voilà, c'est très important là. Là, maintenant, on est sur du code Python. Vous avez bien vu, hein? Voilà. Qu'est-ce qui se passe? Et\"), ('080025', \"z bien vu, hein? Voilà. Qu'est-ce qui se passe? Et le data frame que j'ai créé sous R, je vais le passer dans un Panda DataFrame. Et donc, je vais préfixer le data frame par R, pour dire que fact-cord, il faut le chercher dans l'environnement R. C'est ça qui est important. Donc, un premier point important, on est sur deux chunks Python, c'est ce qui se passe ici, c'est le premier point important. Et le deuxième point important, pour accéder à des objets R, dans l'environnement R, je vais préfixe\"), ('080026', \" objets R, dans l'environnement R, je vais préfixer l'objet par R. Et là maintenant, j'ai un Panda DataFrame. Voilà, c'est bien un Panda DataFrame. Alors pour voir les équivalences, vous les avez ici. C'est un tutoriel qu'on voit en ligne et il décrit justement les correspondances. Vous avez des correspondances entre les objets R et les objets Python. Et justement, là, j'avais un data frame R, et en le passant dans l'environnement Python, ça y est, c'est un Panda DataFrame. Donc vous avez des co\"), ('080027', \"t, c'est un Panda DataFrame. Donc vous avez des conférences sur les différents objets qu'on peut manipuler ici. C'est pas mal, c'est vraiment pas mal. Donc ça y est, maintenant, je suis dans Python, et j'ai bien un objet Python, c'est un data frame Panda. Alors j'affiche les infos pour être sûr de mon affaire c'est bien ce qui se passe ici c'est lent très bien j'affiche les premières lignes pour être sûr de mon affaire on a bien ces premières lignes là de notre plan data frame qui correspondent \"), ('080028', \"nes là de notre plan data frame qui correspondent aux premières lignes qu'on avait ici heureusement sinon il faudrait s'inquiéter on est d'accord là dessus bien qu'est ce qui se passe alors je suis pleinement dans l'environnement Python alors encore une fois très bien mais qu'est ce que je fais je vais lancer la régression logistique voilà les variables explicatives c'est les deux composantes et la variable cible si vous voulez c'est la maladie et j'affiche les coefficients donc là j'importe la \"), ('080029', \"t j'affiche les coefficients donc là j'importe la régression logistique je mets pas de paramètres je mets par même pas dans d'un domstice alors je fais tout plus simple du mettre dans le site mais bon ok très bien ça y est j'ai les coefficients donc le a1 x1 plus a2 x2 si vous voulez ça c'est a1 ça c'est a2 et j'ai également l'intercept ici. Voilà, le A0. Alors, qu'est-ce qui se passe? Je vais concaténer ces deux objets-là, ce vecteur-ci, c'est un vecteur à un élément, ce scalaire-ci, dans un ve\"), ('080030', \"n vecteur à un élément, ce scalaire-ci, dans un vecteur numpy. Donc, j'importe 9. Je suis toujours dans Python, là. Vous êtes d'accord là-dessus? Donc, je vais concaténer les coefficients A1, A2 avec l'intercept dans un vecteur coefficient. Voilà. On voit bien. 0, 9, 4. voilà, moins 36, voilà, moins 32, très bien. Et ensuite, qu'est-ce qui se passe? Voilà, je reviens dans l'environnement R et je récupère l'objet Python. Donc, je récupère les coefficients qui sont là, qui sont dans l'environnemen\"), ('080031', \"ficients qui sont là, qui sont dans l'environnement Python, je le récupère dans l'environnement R et pour pouvoir le faire, je préfixe l'objet avec Pi$. Donc, si je suis dans l'environnement Python et que je veux accéder à un objet R, je mets R. Si je suis dans l'environnement R et que je veux accéder à un objet Python, je mets Pi$. Pas besoin d'être prix Nobel pour comprendre ça, on est bien d'accord là-dessus. Ça doit pouvoir se comprendre relativement facilement. Donc c'est ce qui se passe ic\"), ('080032', \"tivement facilement. Donc c'est ce qui se passe ici. Je récupère les coefficients qui sont maintenant dans un vecteur R. On est d'accord là-dessus. Très bien. Et du coup, je vais essayer de tracer la frontière dans l'espace de représentation. Donc, je calcule les coefficients de l'équation explicite. Cette fois-ci, là, c'était les coefficients de l'équation implicite. Voilà, explicite cette fois-ci. Du coup, je trace les points dans l'espace factoriel. Je mets la classe d'appartenance et je trac\"), ('080033', \"oriel. Je mets la classe d'appartenance et je trace la frontière. Et voilà ici. Très bien. Très bien. Donc, qu'est-ce qui se passe? On voit qu'il y a des individus mal classés. Il y a des bleus parmi les rouges. Vous êtes d'accord là-dessus? Des positifs parmi les prédictions négatives. Et il y a des négatifs parmi les prédictions positives. Alors pour s'en assurer, tout simplement, qu'est-ce qui se passe? Je vais revenir dans l'environnement Python et je vais calculer la matrice de confusion. E\"), ('080034', \"hon et je vais calculer la matrice de confusion. Et on verra si c'est bien le cas. On verra si c'est bien le cas. Allons-y. Très bien. et on a bien la matrice de confusion qui est là c'est pas mal c'est pas mal du tout je trouve ça intéressant donc les 26 là c'est les 26 positives parmi les prédictions négatives voilà c'est les bleus qui sont là et les 18 là c'est les prédictions c'est les rouges parmi les bleus vous avez la frontière qui est là et les 18 qui sont là c'est les rouges, les négati\"), ('080035', \"et les 18 qui sont là c'est les rouges, les négatifs qui sont parmi les prédictions positives bon, ce n'était pas le vrai message ici le véritable message ici c'est de bien montrer que en réalité vous avez ici le résultat vous avez le notebook qui est généré automatiquement ça c'est pas mal, là je vais l'ouvrir avec Opera par exemple voilà et donc là il génère automatiquement le notebook, c'est pour ça que j'utilise souvent des notebooks moi, vous avez directement ici les résultats, et notamment\"), ('080036', \"s avez directement ici les résultats, et notamment la possibilité d'avoir... Bon, je n'ai pas clignité encore, c'est pour ça qu'il n'y a pas le graphique. Mais on a bien ici notre... Si je clignite, on aura le graphique définitif. Mais bon, ce n'était pas le message. Le principal message, c'est de dire que, en fait, via Reticulate, j'ai la possibilité de combiner dans un même projet du code R et Python, et de tirer partie de ce qui est le plus intéressant chez l'un et chez l'autre. Voilà. Pouvoi\"), ('080037', \"téressant chez l'un et chez l'autre. Voilà. Pouvoir les combiner au mieux possible. Et ça, ça me paraît vraiment un élément intéressant. Parce que ce qui fait la différence aussi entre ces deux outils-là, il y a plein de gens qui en parlent et tout ça, moi, bon, très bien, je laisse dire. Mais ce qu'il faut voir en réalité, c'est que la différence dans ces langages-là, c'est l'existence des packages aussi. C'est la disponibilité des packages. Voilà. Et c'est à nous de trouver les meilleurs packa\"), ('080038', \"là. Et c'est à nous de trouver les meilleurs packages possibles dans les différents environnements, si on doit pouvoir les combiner. Si on doit pouvoir les combiner. Mais on n'est pas obligé non plus. C'est ça. C'est cette liberté-là qui me paraît la plus intéressante. Et plus on connaît les outils, plus on a une vision globale sur les outils, et plus on a la liberté de choisir les outils les plus intéressants par rapport à notre projet, par rapport à nos contraintes, par rapport à notre environ\"), ('080039', 'ort à nos contraintes, par rapport à notre environnement. Très bien, excellent travail à tous.'), ('090001', \"Voici le texte corrigé et amélioré :\\n\\n---\\n\\nÀ partir de la transcription de l'audio d'une vidéo YouTube, corrige et améliore ce texte pour obtenir un français clair, fluide et sans fautes. Assure-toi d’éliminer les répétitions ou erreurs éventuelles, et préserve le sens général de la vidéo :\\n\\n---\\n\\nEt tic-tic. Ok, très bien. Alors, essayons de voir l'exemple qui est là. Voilà, donc je mets un nouveau document à classer. Voilà. Et je récupère les catégories. Allons-y. Et il m'a dit, c'est plutôt Ki\"), ('090002', \"tégories. Allons-y. Et il m'a dit, c'est plutôt Kitchen. Alors, j'ai remarqué que les probabilités fournies là ne sont pas les mêmes que les probabilités fournies ici. On est bien d'accord là-dessus. Donc, il y a eu des changements entre les versions entre-temps. Ça, c'est à vous de voir ça. Mais bon, ces versions, ça, c'est un bon... Mais bon, ça marche comme ça. D'où l'intérêt d'utiliser des environnements. OK. Alors, du coup, moi, je me suis dit, je vais essayer de nouveau de classer ces fame\"), ('090003', \"it, je vais essayer de nouveau de classer ces fameux documents. Voilà, je vais essayer de classer ici les commentaires en avis, soit like, soit dislike, en leur donnant quelques exemples de like et dislike. Alors, je n'ai pas utilisé les mêmes ici. En fait, ce fichier-là, il est en deux parties. Il y a une partie apprentissage et une partie test, j'ai gardé que la partie test ici, et donc je suis allé charger la partie apprentissage dans des anciens tutos que j'ai mis en ligne, sur Quanteda nota\"), ('090004', \"ens tutos que j'ai mis en ligne, sur Quanteda notamment. Sur Quanteda, je parle de classement de documents, et justement j'utilise la base IMDB avec Train et Test. Voilà, très bien. Donc je suis allé dans la partie Train, et j'ai pris trois documents au hasard comme like, et trois documents au hasard comme dislike. Donc c'est ce que je fais là. Regardez bien. Donc, pour la classe Like, j'ai mis un premier document qui est long, sur plusieurs lignes je l'ai mis. Du coup, j'ai mis trois guillemets\"), ('090005', \"es je l'ai mis. Du coup, j'ai mis trois guillemets pour qu'on voie bien. Il est sur plusieurs lignes. Donc, voilà le premier document qui commence là et qui finit là. Ensuite, le deuxième document qui commence là et qui finit là. Et le troisième document qui commence là et qui finit ici. Voilà, ça c'est pour la classe Like. Et donc, j'ai trois exemples. J'ai trois exemples pour la classe Like. Donc, trois exemplaires des commentaires pour chaque classe. Et ici, pour la classe dislike également, \"), ('090006', \"classe. Et ici, pour la classe dislike également, j'ai trois documents. J'ai un premier document qui est là. J'ai un deuxième document qui commence là et qui est fini un peu plus loin ici. Et j'ai un troisième document qui commence là, que j'ai choisi à hasard dans la base train, qu'on n'utilise absolument pas ici. Si, qu'on utilise du coup via ces trois exemples-là. Donc, voilà les trois exemples que je vais utiliser pour le classement des fichiers IMDB, les commentaires IMDB. Et de nouveau, je\"), ('090007', \"ers IMDB, les commentaires IMDB. Et de nouveau, je vais créer mon modèle. Donc, comme tout à l'heure, je prends le modèle pré-entraîné, et je vais le compléter, le pipeline pré-entraîné, et je vais le compléter avec le dispositif classique de classification, en lui passant en data mes commentaires. Voilà, mes commentaires étiquetés. Et on est bien dans le modèle Spacy. Ça y est donc très bien, il y a des options par ailleurs c'est à vous de regarder, mais bon ici on en a le plus simple alors du \"), ('090008', \"der, mais bon ici on en a le plus simple alors du coup je vais charger la base que je vais tester donc j'ai chargé ici, il faut la mettre, je l'ai mis là et pour avoir le nom vous avez copié chemin d'accès c'est comme ça que j'ai récupéré ce nom là donc qu'est-ce qui se passe, tout simplement j'ai pris la base IMDB voilà et je l'ai glissé ici tout simplement voilà, vous avez vu, dans l'espace de stockage de la session C'est fait déjà. OK, donc une fois que c'est fait ça, je vais le charger. Très\"), ('090009', \"e fois que c'est fait ça, je vais le charger. Très bien. Et j'affiche les infos. Il y a bien 100 observations. Il y a deux colonnes. Un pour les avis, un pour les commentaires. J'affiche les premières lignes. Voilà. Like, dislike. C'est exactement ce que j'ai fait ici. Voilà, c'est le même dispositif. Vous êtes d'accord là-dessus. Très bien. OK. Voilà. Je calcule la fréquence des classes. C'est quasiment 50-50. Et un 51 like et 49 dislike. Et si je prends le document numéro 0, par exemple, voilà\"), ('090010', \"je prends le document numéro 0, par exemple, voilà le commentaire, et c'est un avis positif, like. Alors, appliquons notre modèle, le modèle qu'on a construit ici. C'est ce modèle qu'on a construit ici, modèle classique, dans lequel on a acheté le pipeline de FewShot. OK. Alors, allons-y. Comment il va le classer? Il le classe en like. Bien ici. Voilà le commentaire. Ah non, ce n'est pas là. Ça, c'est déjà affiché le document et sa classe, son étiquette. Et donc, maintenant, j'applique le modèle\"), ('090011', \"iquette. Et donc, maintenant, j'applique le modèle. Et il dit que c'est un like à 73% de chance. La somme des deux probabilités, là, fait 1. Donc, si on a une fonction Softmax, là. OK, très bien. On voit à peu près. Donc, on sait à peu près faire fonctionner le dispositif. On va l'appliquer sur l'ensemble du corpus test. Donc, je fais une liste avec les prédictions. Très bien, je passe en revue les commentaires, j'applique le modèle classique, mon pipeline, avec les features de classification su\"), ('090012', \"n pipeline, avec les features de classification sur mon document, je récupère la classe, et ensuite je compare la probabilité de like à 0.5. Si elle est supérieure à 0.5, je prédis like, sinon je prédis dislike. C'est ça, hein? Si la prédiction like, là, voilà, est supérieure à 0.5, ça veut dire que la classe la plus probable, c'est like, et donc je prédilegue, sinon je prédile disley. Ensuite, ces prédictions-là, j'ai une liste, je la mets sous forme de vecteur numpy et je regarde les fréquence\"), ('090013', \"forme de vecteur numpy et je regarde les fréquences de prédiction déjà. Allons-y. C'est relativement rapide. C'est relativement rapide. Je ne sais pas si c'est rapide parce que c'est dans Google Collab ou si je le fais localement, que je ne peux pas tester du coup. Mais bon, très bien. Alors du coup, je crée la matrice de confusion. Voilà, et je calcule l'accuracy ici, qui est égale à 63%. Donc dans 63% des cas, je donne la bonne réponse. C'est moins bon que le gyroshot. Dans cette vidéo aussi, \"), ('090014', \"oins bon que le gyroshot. Dans cette vidéo aussi, le gyroshot, on était à 85%, ou 81%, enfin 80% et quelques. Voilà, donc là, je donne quelques exemples, et ça, justement, c'est la question qu'il faut se poser à chaque fois. Ce n'est pas parce que le paquet est sur Internet qu'il est bon. On est d'accord là-dessus. Il y a des paquets sur Internet, il y a des modèles pré-entraînés sur Internet. Il faut surtout pas en faire une religion c'est à nous de le récupérer ok très bien de l'utiliser de dé\"), ('090015', \"s de le récupérer ok très bien de l'utiliser de déjà comprendre le mode opératoire ce qui n'est pas évidence tout le temps et ensuite de voir comment l'utiliser et après d'évaluer ses performances voir si on peut vraiment l'adapter à notre problème et si ça marche pas il faut se poser des questions alors peut-être que les exemples qui sont là qu'on a donné là sont pas très bien choisis peut-être qu'il faut en donner un peu plus, je ne sais pas, c'est à vous de voir ça, mais bon, si je donne 250 \"), ('090016', \"'est à vous de voir ça, mais bon, si je donne 250 exemples, on est dans l'apprentissage supervisé standard, on est d'accord là-dessus, donc la question du few-shot ne se pose plus, très bien. Alors peut-être aussi que la librairie n'est pas assez performante, ou que le paramétrage que j'ai mis là n'est pas très bon, en fait, c'est à voir également. Peut-être qu'il y a des paramètres supplémentaires, là, c'est à vous de voir, très bien. Et comme je disais également, il y a un autre outil égalemen\"), ('090017', \"e disais également, il y a un autre outil également que l'auteur de librairie a mis en ligne, qui est celui-là. Regardez ici, il l'a mis là, très bien. voilà, très bien, qu'on peut utiliser aussi en combinaison avec Spacy, c'est Spacy, c'est cette fit, qui permet de faire du fusion, c'est à voir, c'est ce que je mets ici, c'est à vous de voir, tester, chargez-le également, installez-le, c'est très facile, je ne le mets pas ici parce que je ne veux pas faire une vidéo trop longue pour un sujet qu\"), ('090018', \"x pas faire une vidéo trop longue pour un sujet qui reste relativement simple à traiter finalement, mais il suffit d'installer tout simplement le package avec un pip install et ensuite de reproduire la démarche en se conformant au mode de fonctionnement de la librairie tout simplement voilà, excellent travail à tous.\\n\\n---\\n\\nJ'espère que cette version améliorée répond à vos attentes.\"), ('100001', \"Bien, c'est parti. Dans cette vidéo, je continue l'exploration des fonctionnalités et des pipelines du package Transformers, qui permettent de mettre en œuvre des modèles pré-entraînés que l'on peut télécharger à partir du site Hugging Face. C'est la troisième vidéo, et bien que cela m'ait pris un peu de temps pour regarder les différentes fonctionnalités et possibilités, j'ai maintenant une vision plus claire des choses. Je peux donc enchaîner les vidéos avec des cas d'utilisation qui permetten\"), ('100002', \"es vidéos avec des cas d'utilisation qui permettent de mieux voir les choses.\\n\\nDans les deux vidéos précédentes, j'ai réalisé une classification à partir de zéro et une analyse de sentiment, toujours dans le cadre du NLP. À partir de ces exemples, j'ai commencé à examiner en détail les différents pipelines disponibles et leurs applications. En tant qu'enseignant à l'université, je donne des projets aux étudiants, et dans tous mes projets, il y a une forme de transversalité. Je veux absolument qu\"), ('100003', \"une forme de transversalité. Je veux absolument qu'ils puissent mettre en œuvre différentes compétences dans le même projet. C'est très bien d'avoir des compétences bien ciblées, mais il faut aussi pouvoir les marier convenablement pour qu'il y ait une vraie valeur ajoutée.\\n\\nIci, nous avons des pipelines dans le NLP, l'audio, la vision par ordinateur, et le RAM. Ce qui est intéressant, c'est de pouvoir marier ces différents pipelines pour obtenir quelque chose de plus efficace. Un aspect particu\"), ('100004', \" quelque chose de plus efficace. Un aspect particulièrement intéressant est celui des pipelines multimodaux. Par exemple, le pipeline image2textpipeline permet de présenter une image et d'en obtenir une description. En utilisant ce pipeline en Python avec un modèle pré-entraîné, nous pouvons obtenir une description de l'image en anglais. Cependant, je souhaite obtenir cette description en français.\\n\\nEn regardant les pipelines disponibles dans le NLP, j'ai trouvé un pipeline pré-entraîné qui perm\"), ('100005', \"NLP, j'ai trouvé un pipeline pré-entraîné qui permet de faire de la traduction automatique. Je vais donc combiner deux pipelines : un premier pour décrire automatiquement une image et un second pour traduire cette description en français. C'est vraiment intéressant.\\n\\nIl est important de noter que les pipelines ont été créés par des data scientists et sont basés sur un corpus particulier, dans une vision et un contexte spécifiques. Cela ne garantit pas qu'ils fonctionneront parfaitement pour votr\"), ('100006', \"t pas qu'ils fonctionneront parfaitement pour votre problème. Il est donc crucial de bien comprendre le dispositif et de savoir réagir correctement en cas de problème.\\n\\nPour illustrer cela, je vais parser des images, en obtenir une description textuelle en anglais, et la traduire en français. J'ai préparé un notebook pour cela. Je vais utiliser Python 3.10 et le modèle pré-entraîné Google T5 pour la traduction. J'ai également testé TensorFlow, mais j'ai choisi PyTorch pour cette démonstration.\\n\\n\"), ('100007', \"is j'ai choisi PyTorch pour cette démonstration.\\n\\nJe vais commencer par charger les pipelines pré-entraînés. Le premier est pour la traduction de l'anglais au français, basé sur le modèle pré-entraîné Google T5. Le second est pour l'image2textpipeline, qui permet de parser une image et d'en obtenir une description textuelle.\\n\\nJe vais maintenant traiter les images. La première image est une photo de zèbres récupérée sur le site Ouest France. La deuxième est une photo personnelle prise lors de la \"), ('100008', 'uxième est une photo personnelle prise lors de la rentrée de cette année. La troisième est une photo prise au salon de l\\'auto en 2007, avec des voitures anciennes.\\n\\nPour chaque image, je vais utiliser le pipeline image2text pour obtenir une description en anglais, puis le pipeline de traduction pour obtenir une description en français. Voici les résultats :\\n\\n1. Pour la photo de zèbres, la description en anglais est \"Zebras in a field\". La traduction en français est \"Des zèbres dans un champ\".\\n2.'), ('100009', 'ion en français est \"Des zèbres dans un champ\".\\n2. Pour la photo de classe, la description en anglais est \"A large room with many people sitting at tables\". La traduction en français est \"Une grande salle avec beaucoup de personnes assises à des tables\".\\n3. Pour la photo du salon de l\\'auto, la description en anglais est \"Old cars displayed in a museum\". La traduction en français est \"Des voitures d\\'époque exposées dans un musée\".\\n\\nEn conclusion, les modèles pré-entraînés sont des outils puissant'), ('100010', \"les modèles pré-entraînés sont des outils puissants et intéressants. Ils ne tiennent qu'à nous de les utiliser à bon escient. La capacité à combiner différents pipelines est également un savoir-faire important que les étudiants doivent acquérir.\"), ('110001', \"Bien, c'est parti ! Dans cette vidéo, je continue d'explorer les fonctionnalités du package Transformers, qui permet de charger des modèles pré-entraînés disponibles sur Hugging Face. Aujourd'hui, nous allons nous concentrer sur DistilBERT dans le cadre de l'analyse des sentiments (sentiment analysis) en traitement du langage naturel (NLP).\\n\\nJ'ai trouvé un tutoriel intéressant qui explique comment utiliser DistilBERT pour cette tâche. J'ai donc décidé de reproduire ce tutoriel et d'aller plus lo\"), ('110002', \"écidé de reproduire ce tutoriel et d'aller plus loin en explorant ce que nous pouvons faire de plus avec ces outils.\\n\\nJe parlais précédemment de la classification zéro shot. En substance, je disais que nous sommes dans une période où nous avons une profusion de modèles pré-entraînés. Ces modèles ont été pré-entraînés sur de grands corpus de textes, d'images, ou d'autres types de données. Ils offrent de nouvelles possibilités pour nous, notamment en nous permettant d'exploiter des modèles pré-ent\"), ('110003', \"en nous permettant d'exploiter des modèles pré-entraînés sur de grandes bases de données sans avoir à créer nos propres modèles from scratch.\\n\\nCependant, il est crucial de savoir comment utiliser ces modèles et de délimiter clairement leurs possibilités. Il ne s'agit pas de sortir le canon pour tuer la mouche, par exemple. Il est important de choisir l'outil le plus approprié par rapport aux problèmes que nous essayons de résoudre.\\n\\nJ'ai installé le pipeline qui permet d'utiliser ces modèles à p\"), ('110004', \" le pipeline qui permet d'utiliser ces modèles à partir de la librairie Transformers. J'ai testé avec PyTorch et TensorFlow, et les deux fonctionnent bien. J'ai choisi TensorFlow pour éviter les warnings concernant l'adaptation des poids.\\n\\nJ'ai utilisé Python 3.10, car c'est la version recommandée par le package Transformers à la date de ma vidéo. J'ai également testé différents pipelines disponibles, comme la traduction automatique, la commentaire d'image, et la résumé automatique.\\n\\nPour cette \"), ('110005', \"re d'image, et la résumé automatique.\\n\\nPour cette vidéo, j'utiliserai un fichier de données disponible sur mon site de tutoriel. Cela vous permettra de reproduire les étapes à l'identique.\\n\\nJe charge maintenant la version de Python 3.10.15 et le package Transformers 4.46.3. Le pipeline se charge rapidement, car il utilise une copie locale des modèles pré-entraînés.\\n\\nJ'ai testé DistilBERT avec des phrases positives et négatives. Le modèle semble comprendre les sentiments de manière générale, mais\"), ('110006', \"omprendre les sentiments de manière générale, mais il peut parfois être trompé par des contextes spécifiques. Par exemple, il a mal interprété une phrase à propos de Borg, un joueur de tennis célèbre.\\n\\nJ'ai ensuite utilisé les commentaires IMDB pour évaluer la performance du modèle. Sur 100 commentaires, le modèle a correctement classé 79% des avis, ce qui est un bon résultat pour une analyse des sentiments.\\n\\nEn conclusion, les modèles pré-entraînés sont un outil puissant pour l'analyse des sent\"), ('110007', \"nés sont un outil puissant pour l'analyse des sentiments, mais ils ne sont pas infaillibles. Il est important de les adapter et de les utiliser de manière appropriée en fonction de nos besoins spécifiques. Utiliser ce package permet d'accéder facilement à des modèles pré-entraînés disponibles sur Hugging Face, ce qui peut être très utile pour de nombreuses applications.\\n\\nMerci d'avoir regardé cette vidéo. N'hésitez pas à poser des questions ou à laisser des commentaires si vous avez des suggesti\"), ('110008', 'laisser des commentaires si vous avez des suggestions ou des questions supplémentaires.'), ('120001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler du zéro-shot classification dans le contexte du NLP, dans le contexte du traitement statistique des données textuelles. Voici, naturel, langage processing. Ok, alors quelle est l'idée? L'idée, c'est qu'aujourd'hui, il y a une forme d'évolution dans notre domaine, c'est l'existence des modèles pré-entraînés. qui prennent une place de plus en plus importante. Posons un petit parallèle rapide, ça nous doit voir si c'est intéressant ou pas. \"), ('120002', \"e, ça nous doit voir si c'est intéressant ou pas. Si je fais un parallèle avec les voitures, par exemple, dans notre domaine, nous, avant, ce qu'on faisait, c'est qu'on construisait la voiture d'abord, en lien avec ce que nous souhaitons faire, et ensuite, on conduisait la voiture. Voilà, très bien, en lien avec ce que nous souhaitons faire. Là, ce qui change, c'est qu'on a des modèles pré-entraînés, qui sont prêtes à l'emploi. On a des voitures pré-entraînées, pré-construites, prêtes à l'emploi\"), ('120003', \"pré-entraînées, pré-construites, prêtes à l'emploi. Et c'est à nous d'utiliser dans notre domaine spécifique. Alors, on se dit, ouh là là, c'est quand même... Enfin, ça interroge beaucoup. Parce qu'il y a des gens qui disent justement que du coup, maintenant, data scientist, ce n'est plus nécessaire parce que les modèles pré-entraînés existent déjà. Il y a plein de LLM, il y a plein de trucs. Voilà, donc nous, on n'a plus besoin de construire les modèles parce qu'ils sont faits déjà. Bon, déjà, \"), ('120004', \" modèles parce qu'ils sont faits déjà. Bon, déjà, les modèles ont été construits par des data scientist. Donc, il faut toujours des data scientists. Très bien. Et puis, il n'en reste pas qu'il faut avoir une certaine expertise quand même sur ce que nous manipulons. Je prends un exemple très simple. Vous avez une Formule 1 qui marche très bien sur circuit, qui est excellente. Vous ne pouvez pas l'utiliser pour faire le pari d'Arca. Dès la première lune, elle va être ensablée, votre Formule 1. On \"), ('120005', \" lune, elle va être ensablée, votre Formule 1. On est d'accord là-dessus. Donc, ça interroge quand même. Il faut qu'on ait une certaine forme d'expertise. Et même plus loin que ça, si on ne comprend pas ce qu'on manipule, au moins d'un problème, on ne sait plus quoi faire. Vous avez une voiture, elle marche très bien, et du jour au lendemain, elle s'arrête et vous avez de la fumée qui sort du capot. Vous ne connaissez rien, qu'est-ce que vous faites? Vous commencez à vérifier la pression des pne\"), ('120006', \"tes? Vous commencez à vérifier la pression des pneus. On peut attendre longtemps comme ça, on est d'accord là-dessus. Donc, il ne reste pas mal que c'est une évolution, une révolution, je ne sais pas, mais c'est un aspect très important aujourd'hui qu'on ne peut pas négliger. Alors, ces modèles pré-entraînés là sont disponibles sur Internet. Il y a un des plus célèbres, c'est Huguen Face, qui est là. Donc on clique dessus, on voit bien, ils vous disent, voilà, on a des modèles pré-entraînés dans\"), ('120007', \"disent, voilà, on a des modèles pré-entraînés dans différents domaines, multimodal, dans le traitement des données images, enfin images ou vidéos, ainsi de suite, voilà, dans le donné textuel, voilà, audio également, ainsi de suite. Très bien, donc ça c'est intéressant, et aujourd'hui nous on va s'arrêter justement à ce modèle-là, Roberta Largemenelli. Vous avez la description, mais j'en reparlerai tout à l'heure. Alors, qu'est-ce qu'on veut faire? On veut parler du zéro-shot classification. Alo\"), ('120008', \"e? On veut parler du zéro-shot classification. Alors, zéro-shot classification, l'idée, c'est la suivante, c'est que vous prenez un modèle pré-entraîné qui a été construit dans un contexte que vous maîtrisez plus ou moins par des plus ou moins généralistes, votre modèle pré-entraîné, on ne sait pas très bien, il faut lire la doc pour le savoir exactement. Donc, c'est pour ça qu'on ne peut pas passer à côté de ça. Il faut que l'on me dira. Pour savoir dans quel contexte a été fait, qu'est-ce qu'i\"), ('120009', \"voir dans quel contexte a été fait, qu'est-ce qu'il y a derrière, comment on peut l'utiliser. Vous avez des cas d'usage qui sont là. Une fois que vous avez ce modèle pré-entraîné, vous allez l'utiliser pour classer vos données dans votre domaine très spécifique à vous. Sans apprentissage. Sans apprentissage. Il parle un peu ici, c'est une forme d'instance de transfert learning, mais en fait, c'est du transfert d'application. En réalité. Il n'y a pas d'apprentissage du tout. C'est juste un transf\"), ('120010', \"pas d'apprentissage du tout. C'est juste un transfert d'application. C'est ça l'idée. Et on espère que ça marche. Et ça marche plutôt bien. Pas mal en fait. Pourvu qu'on sache l'utiliser correctement. C'est ça surtout. Mais c'est quand même assez hallucinant. C'est quand même assez hallucinant. Alors, du coup, je me suis dit très bien, j'en vois. J'en vois des gens qui en parlent. Très bien, je me suis dit il faut que je regarde ça de toute manière dans mon cours de NLP. Je fais un cours de NLP \"), ('120011', \"re dans mon cours de NLP. Je fais un cours de NLP à l'université, TaxMining, j'appelle ça. On est plutôt sur de la construction de modèles nous-mêmes, parce qu'on est plutôt dans la partie, et c'est nous qui construisons les modèles, parce que c'est notre spécialité, c'est notre savoir-faire. Mais ça n'empêche pas qu'il faut aussi qu'on sache utiliser les modèles pré-entraînés pour pouvoir les exploiter au mieux. C'est dans ce sens-là que j'avais fait une série, une playlist également de Deep Le\"), ('120012', \" fait une série, une playlist également de Deep Learning, où je parlais notamment de transfert d'année. Donc là, on est dans une forme de transfert d'learning, mais on est dans un transfert d'application. L'idée, c'est ça, c'est que j'ai un modèle construit dans un contexte particulier qu'on ne connaît pas, plus ou moins généraliste, et on veut prendre ce modèle-là, préentraîné, pour l'appliquer dans notre domaine à nous. Voilà, sans adaptation, directement l'utiliser. Bon, on va voir si ça marc\"), ('120013', \"directement l'utiliser. Bon, on va voir si ça marche. Alors, bien sûr, il fallait trouver des packages pour l'utiliser. Et parmi les packages que j'ai trouvés intéressants, il y en a un que j'ai trouvé pas mal, c'est Transformers, qui est là. Voilà, ce package-là est disponible sur Internet, donc je l'ai installé tout simplement, j'ai créé un fichier d'environnement, et j'ai utilisé Transformers, qui est là, et ça correspond exactement à ce qu'on va faire. Alors Transformers, c'est un package qu\"), ('120014', \" va faire. Alors Transformers, c'est un package qui utilise de manière sous-jacente des librairies de deep learning. Donc il peut utiliser, voilà, soit JAX, soit PyTorch, soit TensorFlow. Alors dans un premier temps, j'avais fait un test avec TensorFlow, ça a marché, ça a très bien marché même, mais j'ai eu un warning qui m'a dit attention les modèles ont été construits par PyTorch et donc il faut qu'on adapte un peu les poids pour qu'ils soient compatibles avec un surflow voilà, ce que j'ai eu \"), ('120015', \"compatibles avec un surflow voilà, ce que j'ai eu comme bien cherché de warning du coup je me suis dit bah non c'est pas bon là si c'est bon ça a marché mais je me suis dit pour simplifier pour que ce soit plus lisible ce que je suis en train de faire je vais utiliser directement le librairie de PyTorch donc j'ai créé un environnement j'ai créé un environnement et cet environnement-là est très important, où il y a la Transformers et tous les librairies associées, il sera disponible sur ce site-l\"), ('120016', \"airies associées, il sera disponible sur ce site-là. Donc sur ce site-là, à chaque fois que j'ai fini une vidéo, je mets ici le titre de la vidéo, avec les différentes informations. Ensuite, je mets la description que je mets sur YouTube, et vous avez accès à la vidéo, bien sûr, mais vous avez accès à tous les outils qui m'ont permis de créer le tutoriel. Donc vous avez le notebook, vous avez les données, et vous avez le fichier d'environnement. Alors, regardons vite fait le fichier d'environnem\"), ('120017', \"Alors, regardons vite fait le fichier d'environnement, pour qu'on voie bien ce qu'il y a derrière. Donc, il est là, le fichier d'environnement. Je regarde rapidement. Voilà. Alors, déjà, j'ai pris Python 3.10. Il est où Python, là? Il est là. Voilà. Parce que sur la doc de Transformers, ils m'ont dit, nous, on fonctionne jusqu'à Python 3.10. J'ai fait un essai à 3.11, ça a marché aussi, mais bon. À Rome, faisons comme les Romains, ils disent 3.10 là, j'applique 3.10. J'essaie d'être simple parfo\"), ('120018', \" là, j'applique 3.10. J'essaie d'être simple parfois. Ensuite, une fois que c'est fait ça, dans mon environnement, j'ai installé Transformers, qui est là, et le back-end, le moteur, j'ai récupéré ici Torch. Ensuite, il a installé toutes les dépendances. Donc, ce fichier IAML, je le mettrai en ligne. Ça vous permettra de reproduire à l'identique ce que je fais. Sinon, ça n'a aucun intérêt. On est d'accord là-dessus. Alors, qu'est-ce que j'ai à dire d'autre? Non, je n'ai plus d'autres choses à dir\"), ('120019', \"e d'autre? Non, je n'ai plus d'autres choses à dire. Sur le site de Transformers, il y a plein de cas d'usage. C'est là que je me suis inspiré. Je regardais tout ce qu'ils faisaient ici. J'ai appliqué, tout simplement. J'ai appliqué. C'est pas mal, même dans l'audio. Ça me donne des idées à faire, de projets à demander à mes étudiants. Trêtement des données audio, c'est pas mal. Il y a aussi les traitements vidéo, images, ainsi de suite, bien sûr. On peut y passer des journées entières. J'ai pas\"), ('120020', \". On peut y passer des journées entières. J'ai passé plusieurs jours à regarder en détail, voir, à délimiter dans quels... Et là, justement, je vais faire une série de vidéos sur ces modèles pré-entraînés. Allons-y, alors. Donc, moi, j'ai commencé là et je suis prêt. Alors, qu'est-ce qui se passe? J'affiche la version de Python. OK, c'est la 3.10.15. Ensuite, j'affiche la version du package que j'ai utilisé, 4.46.3. Tout ça est décrit dans le fichier d'environnement, on est d'accord là-dessus. E\"), ('120021', \"hier d'environnement, on est d'accord là-dessus. Ensuite, je charge le pipeline qui va me permettre de définir le gyrosho de classification. Donc, ce modèle-ci. En utilisant ce package-ci. Très bien. Alors, bien sûr, il faut qu'il ait un modèle pré-entraîné. Alors, au départ, je n'avais pas mis de modèle pré-entraîné. J'ai fait que ça. Voilà, j'ai fait que ça. Et il est allé chercher un modèle par défaut. le modèle par défaut qu'il a fait, c'est celui-là. Donc, Roberta, large MNLI. Je suis allé \"), ('120022', \"celui-là. Donc, Roberta, large MNLI. Je suis allé voir sur Internet ce que c'est. Ça a duré un peu longtemps, le chargement, parce qu'il est allé chercher le modèle dans le dépôt sur HuginFace et il l'a chargé localement. Voilà, très bien. Et donc, vous avez la description du modèle ici. Donc, il faut vraiment le lire en détail. Moi, ce que j'ai vu, c'est que c'est en anglais. Très bien, ça m'a... Et, vas-y, ce pas étanche. Après, il y a d'autres éléments. Il faut lire en détail. Et vous avez de\"), ('120023', \" éléments. Il faut lire en détail. Et vous avez des cas d'usage que je vais essayer de reproduire ici, justement. Très bien. alors ce modèle pour entraîner il le charge, il le met sur votre disque du coup à la prochaine utilisation, regardez je charge et c'est extrêmement rapide alors regardons où est-ce qu'il l'a mis en fait, ça c'est très important ça en réalité, bon il n'est pas là c'est pas là que je vous l'ai montré, c'est celui-là plutôt il l'a mis dans un répertoire cache  Cache, c'est mo\"), ('120024', \" l'a mis dans un répertoire cache  Cache, c'est mon disque dur. Vous avez vu, hein? Users, très bien. Rico, c'est moi..cache, voilà. Et dans.cache, il a créé un sous-dossier Uginface. Et dans Hub, il a mis les différents modèles. Alors, ça, c'est pour tout à l'heure. C'est pour le sentiment d'analysie. Je vais faire une vidéo tout à l'heure, cet après-midi, là-dessus. Nous, ce qui nous intéresse, c'est celle-ci. Très bien. Et en allant dessus, voilà, j'ai différentes informations. Et notamment, \"), ('120025', \"ilà, j'ai différentes informations. Et notamment, le snapshot, j'ai vu le modèle pré-entraîné qui fait justement les poids du modèle, qui font le chiffre 1,4 à peu près, 1,3. Donc c'est pour ça que le chargement a été super rapide. Mais la première fois, il va aller sur Internet et charger 1,3. Sur le site de HuginFace. Encore une fois, HuginFace, c'est un site où vous avez plein de modèles pré-entraînés qu'on peut utiliser à notre guise, à nous de savoir ce qu'on veut en faire. On est d'accord \"), ('120026', \"de savoir ce qu'on veut en faire. On est d'accord là-dessus. C'est devenu une référence aujourd'hui. Vous ne connaissez pas Huguen Face, il faut se poser des questions. Les étudiants, là, je leur parle. Il faut absolument qu'on ait une expertise là-dessus et qu'on fasse différents essais. D'accord. Bien. Alors, une fois que c'est fait ça, j'ai fait quelques essais. Je suis allé sur le site de Roberta Large et Ménélie, le modèle printemps. Et j'ai essayé de voir, voilà une phrase en question, un \"), ('120027', \" essayé de voir, voilà une phrase en question, un de deux pour voir le monde. Est-ce qu'il s'agit de travel, de cooking et de dancing? Voilà. Et il le charge et il dit, ah, vous avez bien vu, là il y a fonction de transfert softmax parce qu'à somme des probas fait 1 et il dit la réponse la plus probable c'est travail là j'ai pris un autre tuto cette fois-ci, celui-là que j'ai trouvé pas mal j'en ai cherché plein, ils sont plus ou moins de qualité, bref celui-là il est pas mal parce qu'il est trè\"), ('120028', \", bref celui-là il est pas mal parce qu'il est très schématique, il est très simple il n'y a pas grand chose, mais j'ai repris cet exemple et surtout ça m'a permis de voir les aspects importants, donc il est là, l'adresse est là vous avez l'adresse là très bien, je suis allé voir ça donc j'ai essayé de reproduire, vous avez le texte qui est là est-ce qu'il s'agit de science, est-ce qu'il s'agit d'histoire ou de sport et il a fallu calculer, il dit, il y a 84% de chance que ce soit de la science,\"), ('120029', \"t, il y a 84% de chance que ce soit de la science, bon ça m'a permis de savoir aussi comment accéder aux différents résultats donc il met en première position la classe la plus probable et la proba associée. Voilà ce qu'il va faire en réalité. Donc, si c'est de cooking la classe la plus probable, il l'aurait mis en première position et la proba associée. C'est important de savoir ça, parce que sinon, on se dit... Alors, plus intéressant, ça, c'est décrit sur le site de Roberta, Roberta MNL, larg\"), ('120030', \"t décrit sur le site de Roberta, Roberta MNL, large MNL. ils disent qu'il y a des problèmes de genre. Voilà ce qui est décrit ici. Très bien. Et voilà un exemple ici. Ils disent que le directeur, le grand patron, a une poignée ferme. Genre à la Valardi. Ceux qui connaissent la BD connaissent très bien Valardi et sa poignée très ferme. De J.G. Joseph Gilin. Très bien. Et est-ce que c'est un homme? Est-ce qu'on parle d'un homme ou d'une femme? bon très bien, un directeur c'est souvent un homme j'i\"), ('120031', \"très bien, un directeur c'est souvent un homme j'imagine mais ça tu vois ça c'est on dit qu'il y a un biais de genre c'est pas l'IA qui a un biais de genre l'IA c'est juste une mécanique mathématique qu'on applique en informatique si vous voulez, c'est les données qui ont un biais de genre donc les biais qu'il y a un biais raciste, sexiste tout ce qu'on voulait qu'on voit dans les documents accessibles en ligne forcément sont reproduites quand l'IA parce que lui il reproduit purement et simpleme\"), ('120032', \"IA parce que lui il reproduit purement et simplement il cherche des régularités et il essaie de les reproduire c'est ça que fait les algos de machine learning donc si les documents sont foireux forcément l'IA est foireuse le modèle est foireux on est d'accord là dessus donc c'est pas la faute du modèle c'est pas la faute des mathématiques et de l'informatique on est bien d'accord c'est la faute des documents originels c'est ça l'idée c'est ça parce que j'entends des choses par l'IA et machin l'I\"), ('120033', \"ce que j'entends des choses par l'IA et machin l'IA et l'IA il faut savoir ce que c'est exactement parce que des fois on ne sait pas très bien des fois j'ai des doutes quand j'entends les gens parler ok, alors bon, cet exemple-ci, on va voir qui va dire que c'est un homme ok, mais moi j'ai vu quelque chose qui m'a intéressé c'est que on peut contextualiser le classement donc il y a une fonctionnalité supplémentaire qui me paraît pas mal c'est que donc là, c'est le texte à classer là, c'est les é\"), ('120034', \" donc là, c'est le texte à classer là, c'est les étiquettes possibles et là, c'est on peut contextualiser le classement Et là, ici, la contextualisation, c'est de dire que ce document parle de profession. C'est pas mal, ça. Ça permet d'affiner le traitement que vous allez mettre par ailleurs. Alors, j'ai fait des tests par la suite. Voilà, bon, ça marche plus ou moins. Mais bon, ça change en tout cas les résultats. Donc, il en tient compte. Après, comment le spécifier au mieux? Ça reste une hist\"), ('120035', \", comment le spécifier au mieux? Ça reste une histoire pas évidente. Pas évidente du tout. Voilà. Mais bon, le biais du genre, on le voit bien. il dit que c'est un homme, directeur forcément c'est un homme, c'est les documents qui le disent, ok, ça c'est une chose, mais moi surtout je vois ici qu'on peut contextualiser, pour affiner les résultats, et il y a un paramètre supplémentaire. Alors pour voir l'ensemble des paramètres, il faut regarder dans la documentation des transformers, qu'est-ce q\"), ('120036', \"ans la documentation des transformers, qu'est-ce qu'il y a quand vous créez un modèle, un pipeline de zéro-chaud classification, quand vous créez un pipeline de zéro-chaud classification, voilà, très bien quels sont les paramètres possibles c'est ça l'idée qu'il faut voir ok, alors je me dis ok, très bien, ça marche mais bon, si on fait des expérimentations plus larges, est-ce que ça classe vraiment bien donc j'ai pris la base IMDB tout simplement puis j'ai utilisé par ailleurs cette base là, pa\"), ('120037', \"t puis j'ai utilisé par ailleurs cette base là, par exemple, j'ai utilisé dans celui-là voilà, texte manning avec quand t'es d'A sous R à l'époque, sous R, très bien j'avais utilisé cette base IMDB où l'idée c'est de voir les appréciations par rapport à un film donc il y a deux informations possibles par film il y a un, les commentaires qui étaient faits sur le film et ici, la teneur de la vie qui est soit j'aime, soit j'aime pas like ou dislike et en fait, le choix du terme est important pour q\"), ('120038', \"et en fait, le choix du terme est important pour qu'il puisse faire l'association entre la classe d'une part et le commentaire d'autre part. C'est essayer justement de faire prout, pas prout. On s'amuse comme on peut, vu le temps que j'ai passé, à essayer déjà de rien qu'installer les librairies, ça m'a pris des... Il y en a un autre, je vais faire un tutoriel, c'est incessamment sous peu, où j'ai passé presque une journée à installer les librairies que je n'ai jamais réussi à faire. Du coup, je\"), ('120039', \"ies que je n'ai jamais réussi à faire. Du coup, je me suis rabattu sur Google Collab. Ouh là, ça a forcé d'être très rapidement. Très bien. Mais bon, très bien. Donc, j'ai essayé de mettre, quand c'est positif, de dire pas prout. et quand c'est négatif de dire put. Là, il ne marche pas parce qu'il n'arrive pas à trouver l'association. En réalité, qu'est-ce qu'il fait? Il calcule tout simplement des vecteurs numériques, des embeddings, si vous voulez, et il regarde l'association entre la descript\"), ('120040', \"lez, et il regarde l'association entre la description, le texte, si vous voulez, les commentaires, et la classe. Et il fait l'association entre les deux, c'est aussi bête que ça. Alors, bon, du coup, je vais quand même charger. Donc là, j'ai chargé les documents, vous avez vu, très bien. Et l'idée, justement, comme je suis passé un peu vite là-dessus, l'idée, c'est de trouver, de se classer, de voir s'il classe bien. Est-ce qu'il arrive à lire les commentaires et à les classer de la bonne manièr\"), ('120041', \"s commentaires et à les classer de la bonne manière sur l'avis qui a été donné, sur la teneur de l'avis qui a été donnée? Trouver l'association entre la classe d'appartenance qui est là et l'avis qui est là. On est bien dans le cadre du traitement statistique des données textuelles. On est bien d'accord là-dessus. Bien. Alors, j'ai chargé la base. Voilà, il y a 100 individus. J'ai mis 100 seulement. Pourquoi? Parce que c'est très long, les calculs. Donc, du coup, j'allais faire une pause d'aille\"), ('120042', \"s. Donc, du coup, j'allais faire une pause d'ailleurs. Voilà, mais bon, ça permet déjà d'avoir une certaine idée. J'affiche les premières lignes. Vous avez bien les avis qui sont là. Vous avez les commentaires. Et l'idée, c'est de faire l'association entre les commentaires et les avis. Voilà. Et la description des avis, il y a 51 avis like et 49 avis dislike, puisqu'il y a 100 individus en tout, 100 documents en tout dans notre corpus. OK. Du coup, qu'est-ce que j'ai fait? Du coup, j'ai essayé d\"), ('120043', \"p, qu'est-ce que j'ai fait? Du coup, j'ai essayé d'appliquer à plus grande échelle le classement de chaque document. Et ici, j'ai mis les labels possibles. Les labels possibles, il y a like et dislike. Ensuite, j'ai créé ici, je vais lancer tout de suite. J'ai créé ici une liste dans laquelle vont être collectées les prédictions du modèle. Très bien. Et j'y taire sur l'ensemble des commentaires. Alors, qu'est-ce qu'il y a? J'appelle classifier, qui est le pipeline, qui est le pipeline qui a été \"), ('120044', \"ui est le pipeline, qui est le pipeline qui a été défini avec gyroshote classifier. Je lui passe le document, voilà, très bien, et je lui indique les labels possibles. Et j'ai une prédiction, et je prends le label 0. La première, en fait, il met en première position celle qui a la classe la plus probable. Très bien. Donc, ici, à la sortie de la boucle, j'ai une liste de prédictions, où j'ai like, dislike, ainsi de suite. Tout simplement, cette liste de prédictions, je la mets sous forme de vecte\"), ('120045', \"ste de prédictions, je la mets sous forme de vecteur en un pi et j'affiche les dix premières. Alors, ça prend du temps encore un peu, mais bon, j'en ai mis que 100 pour que ce soit rapide, justement. Alors, j'ai un deuxième commentaire, mais ça, je l'avais déjà dit, mais je répète beaucoup là-dessus. Le choix des noms des classes est très important pour qu'ils puissent faire l'association entre les documents et la catégorie. j'ai essayé, j'ai fait prout et pas prout là j'ai mis  pas prout, et là\"), ('120046', \"t prout et pas prout là j'ai mis  pas prout, et là j'ai mis prout. Ça n'a pas marché. Ça n'a pas marché du tout, parce qu'en réalité, il n'arrive pas à calculer, quand il calcule les vecteurs et tout ça, qu'il calcule l'association, il est totally à l'ouest, en réalité. Donc, le zéro-saut de classification marche plutôt bien. On va voir. On s'éloigne du hasard. Là, le hasard, c'est 50%. Il y a moitié, moitié. Très bien. On va s'éloigner significativement du hasard. Mais ça marche bien parce que \"), ('120047', \"tivement du hasard. Mais ça marche bien parce que les noms des classes ont été choisis de manière... de manière judicieuse. Le choix du nom des classes joue beaucoup ici, en réalité. C'est ça que j'ai compris, finalement. Parce que j'ai regardé en détail, ça me paraissait extraordinaire, mais je me suis rendu compte que quand ils calculent les embeddings, en réalité, le choix des noms des classes, il faut vraiment être extrêmement vigilant et choisir de manière judicieuse le bon nom des classes.\"), ('120048', \"isir de manière judicieuse le bon nom des classes. Normalement, il doit avoir fini. Ça prend un peu de temps, mais bon, ça doit aller ici. C'est un peu plus long parce que j'enregistre la vidéo en même temps. Je me disais pourquoi ça peut l'ordre. Du coup, je vais suspendre un peu la vidéo. Je reprends la main quand il a fini. Me voici de retour. Ça y est, il a terminé. Vous avez bien vu. Et on a les prédictions qui sont là. Alors, qu'est-ce que je fais? Je fais la matrice de confusion entre les\"), ('120049', \"je fais? Je fais la matrice de confusion entre les avis observés, c'est la première colonne qui sont là, et les avis prédits par le modèle. Et ça marche plutôt pas mal. Oui, d'accord, là-dessus. Je calcule ici, on a un accuracie de 85%. 85% des commentaires ont été classés correctement. On a attribué la bonne classe, la bonne étiquette. Ça marche plutôt bien. Mais encore une fois, c'est ce que je mets ici en commentaire, comme du coup vous aurez le notebook en ligne. Essayez de mettre des noms t\"), ('120050', \"le notebook en ligne. Essayez de mettre des noms totally bateaux. mettez je ne sais pas Tarzan et Hakim ou je ne sais pas Zembla ou je ne sais pas des trucs totally à l'ouest, voilà, et vous allez voir qu'il a du mal parce que quand il va calculer les embeddings ça lui pose un vrai souci en réalité donc le zéro shot classification fonctionne bien pourvu qu'on choisisse de manière judicieuse les noms des classes c'est ça la vraie conclusion de cette histoire mais voilà je vous enjeu à regarder ic\"), ('120051', \"te histoire mais voilà je vous enjeu à regarder ici il y a une page aussi Wikipédia sur le zéro shot Donc, regardez et ça vous permet de voir, de regarder tout ça. Sachant qu'un peu plus tard, je vais faire une autre vidéo sur le fichot, cette fois-ci classification, où on lui donne quelques exemples quand même pour qu'il puisse affiner, mais avec une autre librairie. Et là, également, on verra que le choix de la librairie est très important. donc un, GeroShot c'est comme du transfert applicatio\"), ('120052', \"c un, GeroShot c'est comme du transfert application, j'ai un modèle construit dans un domaine, je l'applique dans un autre domaine, pour classer les individus, c'est transfert d'application, très bien, en utilisant des modèles pré-entraînés, il y en a d'autres, il y en a plein, des modèles pré-entraînés sur Hogan Face, moi j'ai utilisé celui-ci, très bien, et ça marche plutôt pas mal, je l'ai appliqué à plus grande échelle ici sur les classements des commentaires de films sur IMDB, mais ça march\"), ('120053', \" des commentaires de films sur IMDB, mais ça marche plutôt pas mal parce que le choix du nom de classe doit être choisi de manière judicieuse. Voilà la principale conclusion. Pour aller plus loin, j'essaie de voir la contextualisation avec l'hypothèse template. Quelle que soit la tentative que j'ai faite, ça n'a pas marché et même ça a dégradé les résultats. C'est une option qui existe, qui est pas mal. Dans l'idée, ça n'est pas mal, mais il faut savoir la manipuler également de manière judicieu\"), ('120054', \" savoir la manipuler également de manière judicieuse. Ici, je vous laisse tester par vous-même. mais bon c'est une possibilité qui existe très bien on va essayer de voir ça dans nos TD un de ces jours excellent travail à tous.\"), ('130001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de la mise en œuvre des algorithmes de deep learning avec Torch, mais sous R. C'est ça la petite subtilité ici. Changement de pâture excite le veau, disait un philosophe qui devait avoir aussi compris beaucoup de choses. Et une autre manière de dire également, c'est que l'ennui naquit un jour de l'uniformité. Alors, quand on parle de deep learning, souvent, on pense à Python, en tout cas dans la data science. Et il y a des personnes qui \"), ('130002', \"dans la data science. Et il y a des personnes qui pensent qu'on ne peut pas faire de deep learning sous R. Et la réponse est si, bien sûr que si, en réalité. La plupart des choses qu'on peut faire sous Python, dans ce cadre-là, en tout cas, dans ce cadre-là, on est bien d'accord là-dessus. Il faut bien cerner les choses. Donc, c'est l'application d'algorithmes deep learning. Ce qu'on peut faire sur Python, généralement, on doit pouvoir le faire sous R. Alors, c'est une chose de le dire, c'est un\"), ('130003', \"ous R. Alors, c'est une chose de le dire, c'est une autre de le montrer. Donc, moi, je m'acharne à le montrer, bien sûr. Voilà, comme ça, il n'y a pas de jaloux. Vous avez vu, vous le savez, tout le monde le sait maintenant. Moi, je n'ai pas de préférence pour les outils. Pour moi, le credo, c'est qu'on doit utiliser l'outil le plus adapté par rapport à nos objectifs et par rapport au contexte dans lequel on se situe. C'est ça, mon credo à moi. Je ne fais pas de religion des outils. Certainement\"), ('130004', \"e ne fais pas de religion des outils. Certainement pas. Alors, recétions vite fait les choses pour que tout le monde puisse voir comment il peut profiter de la vidéo, parce qu'encore une fois, toutes mes vidéos sont en ligne, publiques, et tout le matériel pédagogique associé, donc les données, les notebooks, les programmes, sont toujours en ligne. Je vais revenir de suite à cela. Alors, qu'est-ce qui se passe? Mes étudiants, vous commencez leur cours de deep learning bientôt. Pas avec moi, c'es\"), ('130005', \"cours de deep learning bientôt. Pas avec moi, c'est pour moi qui fait le cours, c'est un collègue, très bien. Mais moi, mon rôle en tant que responsable de formation, c'est de les préparer. Donc, généralement, je fais des ateliers préparatoires pour qu'ils voient déjà ce que c'est, à peu près les grands principes, et qu'ils voient à peu près les outils qu'on peut utiliser pour qu'ils puissent en profiter au mieux en TP. Il n'y a rien de pire qu'un étudiant qui est en TP, qui est assis devant la \"), ('130006', \"n étudiant qui est en TP, qui est assis devant la machine et qui ne sait pas ce qu'il faut faire. Ça, ce n'est juste pas possible. Donc, mon rôle à moi, c'est de les préparer. Je l'ai fait, par exemple, pour le cours de machine learning. J'ai fait des ateliers de préparation. Pareil, ils vont avoir en cours des biostat, je vais faire des ateliers sur l'inférence statistique, sur les comparaisons de population, sur les ANOVA, ENCOVA, pour qu'ils aient une vision globale des choses. Ensuite, comme\"), ('130007', \"ient une vision globale des choses. Ensuite, comme ça, ils pourront profiter au maximum du cours, avec les outils associés, là pour le coup j'utilise SAS. Et Deep Learning commence dans deux semaines, donc du coup je me suis dit, il faut absolument que je les prépare. Et à cet effet, j'avais fait une petite playlist. Donc dans la playlist, je montre les différentes cas d'études possibles, en tout cas dans le traitement des données tabulaires ou des données images, mais aussi des données textuell\"), ('130008', \"es données images, mais aussi des données textuelles en fait. Pour le coup, c'est un cours que moi j'assure. Donc ils peuvent très bien regarder, c'est ce que je fais avec Antep. Et je me suis rendu compte en regardant rapidement qu'il y a beaucoup de Python là-dedans quand même. Il y a beaucoup de Python là-dedans, c'est très bien, c'est très bien, c'est ce que j'utilise moi-même. Mais comme je suis aussi attaché à R, je suis attaché à Python comme je suis attaché à R, Je me suis dit, il faut q\"), ('130009', \"mme je suis attaché à R, Je me suis dit, il faut que je fasse un petit aussi vidéo pour la mise en œuvre des algos de Deep Learning avec Air, avec les librairies qui font référence. Alors ça, en fait, je l'ai déjà fait par le passé. J'ai fait une vidéo ici, où je montre qu'on peut utiliser Keraso Air, sachant que ça, c'était pendant le deuxième confinement, qu'on faisait les cours en distanciel, en décembre 2020. On se rappelle, on était cantonnés à la maison, on faisait des cours en ligne, très\"), ('130010', \"s à la maison, on faisait des cours en ligne, très bien. Mais ce tutoriel-là, j'en avais fait un déjà en 2018. Et 2018, donc ce n'est pas nouveau. Donc la possibilité de mettre en œuvre des algos Deep Learning Swear, ce n'est absolument pas nouveau du tout. Là, j'avais fait en 2018, le texte, il est là, j'ai fait en détail. Et on voit bien, c'était en avril 2018. C'est pour Keras, Keras Tinson Flow. Très bien. Voilà. Donc, je me suis dit, pourquoi pas faire la même chose pour Torch. Voilà. Il y \"), ('130011', \"i pas faire la même chose pour Torch. Voilà. Il y avait PyTorch, que j'ai montré ici. Là, j'utilise beaucoup PyTorch. là, je me suis dit, mais le torch, en fait, il est porté sous R, donc je dois pouvoir faire la même chose. C'est là, vous avez bien vu. Voilà, ils disent que torche pour R est une open source, je ne vais pas dire ce qu'il y a écrit là, vous êtes d'accord là-dessus. Donc ça, c'était un premier élément. Je me suis dit, je vais le faire. J'ai commencé à regarder, comme d'habitude, q\"), ('130012', \"ire. J'ai commencé à regarder, comme d'habitude, quand je veux regarder, je commence à faire des recherches sur Internet pour trouver des tutos qui me permettent de moi-même de cadrer mon travail pour que, Ce qui est important quand on fait un atelier avec des étudiants, mais c'est vrai pour tous les enseignements, même pour les tutos, c'est qu'il faut être schématique et que les différentes étapes soient lisibles. C'est vraiment très important. Comme ça, l'étudiant est capable de l'adapter pour\"), ('130013', \"Comme ça, l'étudiant est capable de l'adapter pour le mettre en œuvre sur son propre jeu de données. C'est ça l'idée. Il faut absolument que ce soit schématique, les différentes étapes sont bien décomposées, et il faut que ce soit parfaitement lisible. Comme ça, la transposition de l'étudiant à son propre problème aux données qu'il étudie doit être relativement simple. C'est ça l'idée. Alors, je vais regarder. Et notamment, là, il y a un livre qui est pas mal. Ce livre-là, vous pouvez l'acheter \"), ('130014', \"i est pas mal. Ce livre-là, vous pouvez l'acheter si vous voulez, mais en fait, il est en ligne aussi. Il est en ligne, il est là. Très bien. Et donc, j'ai regardé. Ah, c'est quand même assez abscon. Tiens, regarde, regarde. J'ai plutôt l'habitude de lire les documentations, mais c'est assez abscond, donc je vais quand même regarder. Je vois les idées, mais repris telles quelles, c'est juste impossible en réalité. Pourquoi? Parce que la plupart du temps, j'utilise soit des données générées aléat\"), ('130015', \"u temps, j'utilise soit des données générées aléatoirement pour montrer les fonctionnalités, mais comment les étudiants vont transposer ça à leurs données? Là, les données ne sont pas chargées, elles sont générées aléatoirement dans le programme lui-même. Soit ce sont des données qui sont impactées dans des packages, comme les fameuses bases listes. Mais là, encore une fois, oui, mais comment je transporte ça? Mes propres données? Mes données ne sont pas impactées dans un package. Donc, du coup,\"), ('130016', \"sont pas impactées dans un package. Donc, du coup, voilà. Donc là, déjà, j'ai regardé. Bon, c'était intéressant à lire. Je ne voudrais pas me dire du mal. C'est surtout pas moi. Tout le monde me connaît. Ce n'est vraiment pas ma manière de procéder. Je ne suis pas comme ça du tout. Ça ne m'intéresse pas d'être comme ça. Voilà. Mais quand même, ce n'est pas très accessible. C'est accessible si on a déjà un bon niveau de connaissance. Or, non. Moi, c'est un atelier d'initiation. Donc, je ne peux p\"), ('130017', \" c'est un atelier d'initiation. Donc, je ne peux pas amener les étudiants là-dessus. Donc, je n'ai regardé d'autres également. Voilà, j'en ai trouvé un aussi qui est là. Vous avez l'adresse là. Mais là, pareil. Ya! En plus, il montre différentes manières. Là, c'est les données ministres qui sont directement pactées. Très bien. Voilà. Ensuite, il y a l'opérateur pipe aussi. Ya! Pareil. Je comprends l'opérateur pipe. Bien sûr que je comprends. Mais quand ça commence à être en plusieurs successions\"), ('130018', \" quand ça commence à être en plusieurs successions, l'idée de l'opérateur pipe, c'est que le calcul précédent est intégré comme premier paramètre du calcul suivant. Si je suis schématique, c'est ça. OK, mais bon, là, on perd les étudiants avec ça. Voilà, donc ça commençait un peu à me prendre la tête, cette histoire-là, très bien, même si globalement, on comprend les idées, mais c'est vraiment pas très lisible. Voilà, donc pas épilé là, ah bah, donc il y avait celui-là aussi. alors là, je vais m\"), ('130019', \"onc il y avait celui-là aussi. alors là, je vais mettre ici l'adresse voilà, vous avez l'adresse qui est là, très bien en anglais on trouve quand même pas mal de choses c'est en français déjà là non plus je n'ai pas trouvé quelque chose de probant je l'ai trouvé mais bon, peut-être que ça m'a échappé bon, je n'ai pas trouvé quelque chose qui me paraît intéressant on pourra en parler ici, mais peut-être qu'il y en a un mais que je n'ai pas trouvé, mes requêtes de Google peut-être étaient limitées\"), ('130020', \" mes requêtes de Google peut-être étaient limitées je ne sais pas. Mais bon, là également, ce qui est intéressant ici, c'est qu'il montre qu'on a deux manières de faire. Soit avec l'outil séquentiel, qu'on peut trouver sous Keras d'ailleurs, soit en créant des classes. Mais là aussi, on va perdre les étudiants avec ça. Il se dit, mais on fait laquelle alors? Le premier ou le deuxième? Comment ça se passe? Et tout ça. C'est extrêmement compliqué. Voilà. Avec toujours ici, l'opérateur pipe. Sachan\"), ('130021', \"Voilà. Avec toujours ici, l'opérateur pipe. Sachant que le pipe, il est discutable. Je ne vais pas dire que ce n'est pas bien le pipe. Je n'ai pas dit ça non plus. Mais bon, dans une phase d'initiation, ça peut rendre plus compliquée la compréhension des étudiants quand ils sont dans une phase où ils appréhendent le problème, ils rentrent dans un domaine. Surtout que le PIPE, j'ai appris que depuis la version 4.1, il y a une nouvelle notation qui introduit également. Donc, il y a celle avec les \"), ('130022', \" introduit également. Donc, il y a celle avec les pourcentages qui sont là, qui sont issus de ma grille. voilà, très bien, mais il y a une nouvelle notation qui apparaît mais attends, c'est juste pas possible donc je me suis dit, ce qu'il faudrait faire c'est de trouver, de reprendre un de mes tutoriels sous Python et de faire exactement la même chose en parallèle sous R comme ça, c'est extrêmement lisible les différentes étapes et les étudiants auront la capacité de comparer les deux c'est ça l\"), ('130023', \"auront la capacité de comparer les deux c'est ça l'idée du coup j'ai pris quoi? j'ai pris ce paquet, ce tutoriel-là, et j'ai essayé de le reproduire pas à pas. Alors, du coup, le code, vous le trouverez ici, comme d'habitude, sur ce site-là. Donc, sur ce site-là, comme d'habitude, je mettrai, il est où là? Il est là. Vous avez le lien de la vidéo, et surtout, ici, vous aurez le lien avec le notebook et les données. Donc je vais utiliser les mêmes données et on aura ici, vous aurez le programme p\"), ('130024', \" données et on aura ici, vous aurez le programme pour Python.  faire aujourd'hui, que je vais rajouter tout à l'heure ici, je mettrai le code programme pour R. Et l'intérêt pour vous, bien sûr, c'est de récupérer les deux et de regarder en parallèle. Et là, on verra qu'effectivement, les fonctionnalités sont tout à fait transposables. Alors du coup, la séquence de travail, elle est là. Je vais me placer dans un cadre très connu, c'est tout simplement, j'ai un échantillon de données, je vais crée\"), ('130025', \"ment, j'ai un échantillon de données, je vais créer un perceptron multicouche dessus, avec deux neurones dans la couche cachée. Une fois que j'ai fait ça sur les données d'entraînement, je vais récupérer l'échantillon test, je vais faire les transformations idoines, comme j'ai fait ici, comme j'ai fait sur le train, je vais l'appliquer sur le test. Ensuite, j'applique le modèle Perceptron multicouche appris, entraîné sur les données test préparées, et j'aurai les deux colonnes, la cible observée\"), ('130026', \"s, et j'aurai les deux colonnes, la cible observée en test et la cible prédite sur l'échantillon test. Et ensuite, je vais calculer ma fusion classique. Mais c'est très basique, on est d'accord là-dessus, mais l'intérêt de partir sur un chemin très basique, c'est que chacune des étapes va être lisible. C'est ça l'intérêt de la faire. Alors, le perceptron, il est là. Je parle des données très rapidement, mais c'est des données que j'utilise, ultra classiques chez moi. C'est les breast cancer. Don\"), ('130027', \" classiques chez moi. C'est les breast cancer. Donc, on a des ponctions sur des cellules, des nodules. très bien, et on veut savoir si les cellules ponctionnées sont des cellules qui correspondent à une tumeur bénigne ou une tumeur maligne, donc la variable cible est là, bénigne et malignante et les variables prédictives les descripteurs sont des caractéristiques des cellules, taille, forme ainsi de suite, ok, très bien donc là on a la partie trend et là j'ai la partie test on a exactement la mê\"), ('130028', \"nd et là j'ai la partie test on a exactement la même structure des données mais on n'a pas les mêmes individus, il y en a 399 Là, il y en a 300. Sur l'apprentissage, il y a 399. Sur le test, il y a 300. Et mon réseau sera bien celui-là. Alors, ici, les six neurones de la couche d'entrée, qui sont les descripteurs, plus le biais, bias en anglais. Biais, c'est l'intercept, en fait. Donc, c'est la constante. Très simple. Donc, on a ici les différentes connexions, qui sont les coefficients à estimer\"), ('130029', \"es connexions, qui sont les coefficients à estimer, les points synaptiques, mais c'est les coefficients à estimer, tout simplement, qu'on va estimer sur les données d'entraînement. Ensuite, dans la couche intermédiaire, j'ai mis deux neurones dans la couche. Pourquoi? Parce que ça va permettre de dessiner les étudiants, de projeter les points dans l'espace intermédiaire qui est défini par la sortie de la couche cachée. Comme on est dans le plan, on pourra faire la projection dans laquelle est dé\"), ('130030', \"on pourra faire la projection dans laquelle est défini un séparateur linéaire. Donc ça également, je le dis dans mon cours introductif avec mes étudiants, il y a différentes manières d'avoir un perceptron multicouche. Soit on est dans l'espace originel et du coup on a des combinaisons de séparateurs linéaires, et l'ensemble est non linéaire du coup, voilà. Donc on a un modèle linéaire par morceau, ou bien on regarde ce qui se passe à la sortie de la couche intermédiaire, et là du coup on est dan\"), ('130031', \" la couche intermédiaire, et là du coup on est dans un autre espace où on essaie de trouver une séparation linéaire, qui est la combinaison en fait de ce qu'on a ici, qui est basée sur la combinaison de ce qu'on a ici. C'est ça l'idée. Très bien. Alors, dernier élément avant de quoi, c'est long mon préambule, mais voilà, c'est important de dire les choses. Je me suis dit, mais que me disent les liens opgénératives? Donc moi, j'utilise beaucoup Opera, c'est un navigateur que j'apprécie énormément\"), ('130032', \"era, c'est un navigateur que j'apprécie énormément. Voilà, il est là, très bien. Et il a un module qui fait de liens génératives. C'est ARIA, il s'appelle. Donc moi, j'utilise préaliment. Comme je suis plutôt enclin à utiliser des outils libres, donc j'utilisais celui d'Opera qui s'appelle ARIA, qui est liens génératives associées à Opera. voilà, et je lui dis voilà, peux-tu me proposer un programme en langage R qui implémente un perceptron multicouche avec la librairie torche, donc il me sera e\"), ('130033', \"couche avec la librairie torche, donc il me sera en code qui n'est pas trop mal qui n'est pas trop mal, là on a les éléments, on va voir ça tout à l'heure très bien, ensuite là on a le comportement du réseau, donc ça c'est les éléments du réseau, c'est les différentes parties qui composent le réseau les différents outils qui composent le réseau, et ensuite là on a le comportement du réseau avec forward, donc on a la classe ici qui est définie. C'est pas mal, très bien. Et ensuite, ici, on a les \"), ('130034', \"est pas mal, très bien. Et ensuite, ici, on a les outils qu'on va voir tout à l'heure. Ça, c'est la fonction de perte. Là, c'est l'algorithme d'apprentissage. Donc, il y a vraiment les éléments qu'il faut. Et ensuite, il y a l'entraînement. Alors là, pour tous, c'est un apprentissage, c'est une descente de gardien toujours, mais c'est un apprentissage qui est en fait online. C'est-à-dire que chaque individu de l'échantillon d'apprentissage est traité de manière individuelle. C'est ce qu'on a. Il\"), ('130035', \"aité de manière individuelle. C'est ce qu'on a. Il y a deux boucles intégrées ici, imbriquées ici. Là, c'est les époques. Donc, les époques par défaut, je ne sais pas combien il a mis. 10. Et ensuite, une fois qu'il... Donc, c'est le nombre de fois où on passe sur l'ensemble de la base. Et ensuite, on traite ici de manière individuelle la base. C'est une sorte de gradient stochastic. C'est un gradient stochastique, mais on traite les individus de manière individuelle. Donc, c'est comme un mini-b\"), ('130036', \" manière individuelle. Donc, c'est comme un mini-batch, mais dans le mini-batch, j'ai un seul individu. OK. Moi, je veux autre chose. Moi, je veux faire une descente de gradient où, à chaque époche, c'est tous les individus qui sont traités. une descente de gradient, pas stochastique pour le coup, donc du coup là, c'est pas ce qui est fait mais bon, ça pour le coup si on ne connait pas ces méthodes là on ne peut pas se comprendre parce qu'il y a une tentation souvent c'est des étudiants de prend\"), ('130037', \"une tentation souvent c'est des étudiants de prendre tel quel cela et de voir si ça marche ça ne marchera pas soit ça ne marchera pas, soit ça vous donne un truc que vous ne comprenez pas c'est encore pire en fait c'est mieux si ça ne marche pas, parce que moi vous savez qu'il y a une erreur alors si ça marche du coup on dirait qu'il n'y a pas une erreur et là, vous commencez à perdre du temps là-dessus, c'est juste impossible. Très bien. Mais bon, ça donne quand même une référence. Mais donc là\"), ('130038', \"n, ça donne quand même une référence. Mais donc là, c'est bien un online. C'est un dessin de gradient, toujours, mais les individus sont traités de manière individuelle. Très bien. OK. Et ensuite, là, on a la projection sur l'échantillon test. Et là, encore une fois, les individus de l'échantillon test sont traités de manière individuelle. Vous avez bien vu. OK. Très bien. Mais quand même, je trouve que c'est pas mal. Je trouve que c'est pas mal du tout. Ça marche plutôt pas mal. Alors j'ai vu ç\"), ('130039', \"du tout. Ça marche plutôt pas mal. Alors j'ai vu ça, puis j'ai dit, mais ok, il y a même des explications. C'est pas mal, comme il y a un génératif. Là, pour le coup, c'est intéressant. Il y a des vraies explications sur ce qu'il a fait. Très bien. Alors, bon, très bien, je me suis dit, mais ok, mais attends, moi, n'oublions pas que j'ai 6 neurones dans la couche d'entrée, plus le biais, plus l'intercept. Ensuite, dans la couche cachée, j'ai 2 neurones, et dans la couche de sortie, j'ai un seul \"), ('130040', \"urones, et dans la couche de sortie, j'ai un seul neurone. Voilà. Donc je lui ai dit, mais fais-moi ça. C'est ce que je mets ici. Vous avez vu, là, je lui ai dit, J'ai une neurone dans la couche d'entrée, deux neurones dans la couche cachée et un neurone dans la couche de sortie. Et bien, il a corrigé le code. C'est quand même pas mal. Il a corrigé le code, voilà. Input size 6, idem size 2, le output size 1, et il m'a proposé le code. Très bien. C'est pas mal, c'est vraiment pas mal du tout. Trè\"), ('130041', \"C'est pas mal, c'est vraiment pas mal du tout. Très bien. Alors, en revanche, ici, vous avez vu, dans la sortie intermédiaire, il a fait un relu. Voilà. Là, il fait un relu, alors que moi, c'est un sigmoïde que je veux. Ça, c'est un premier élément. Et dans la couche de sortie, il ne fait pas de correction. Il ne met pas la fonction de transfert. Vous avez bien vu, dans la couche intermédiaire, à la sortie de la couche intermédiaire, il applique un relu. C'est ce qu'il fait là. Donc, la fonction\"), ('130042', \"un relu. C'est ce qu'il fait là. Donc, la fonction d'activation relu. Et à la sortie de la couche de sortie, il renvoie le lien air. Pourquoi? Parce que là, tout simplement, c'est un cross-anthropie qu'il utilise. Il est où, là, la fonction de coût? Ah non, là, c'est un mescelos. Ah ben là, pour le coup, ça va poser problème. Oui, là, c'est linéaire. Oui, là, ça pose un souci. Très bien. OK. Donc, du coup, je lui dis, mais non, non, non, attends. Moi, je veux une fonction d'activation sigmoïde e\"), ('130043', \" Moi, je veux une fonction d'activation sigmoïde en couche cachée et en couche de sortie. Je suis pénible, mais on va voir ce que dit l'idée générative. Et il m'a fait les bonnes modèles. Vous avez bien vu. Là, à la couche de sortie, il m'a fait le sigmoïde. Enfin, dans la couche intermédiaire, il applique la fonction d'activation sigmoïde, la fonction de transfert, et en sortie du réseau, qui est linéaire ici, il réapplique une autre fonction sigmoïde. Et on a bien ce réseau-ci. Bien, bien, bie\"), ('130044', \"gmoïde. Et on a bien ce réseau-ci. Bien, bien, bien. Du coup, ce code-là, il n'est pas mal, en fait. Il n'est pas mal. Le seul truc qu'on ne voit pas très bien ici, c'est les données. PrennData, là. Du coup, là, il ne me dit rien. Je remonte en haut. est-ce qu'il me l'a fait? Il m'a dit, voilà, remplacer par vous. Mais du coup, comment je vais préparer mes données? C'est mon fichier Excel que je vais manipuler ici. Voilà. Et ici également, et regardons, il ne m'a rien mis. Du coup, je reste touj\"), ('130045', \"ardons, il ne m'a rien mis. Du coup, je reste toujours coincé parce que dans les autres tutos que j'ai regardés, dans les autres tutos que j'ai regardés, on a soit les données impactées, les ministres, soit des données qui sont générées aléatoirement avec des randoms. Ce qui est encore plus embêtant, si vous voulez. Je ne trouve plus ici, mais c'est ce que j'ai trouvé sur Internet. Donc, c'est ça mon idée. Mon idée à moi, c'est de reprendre cette vidéo-là. Vous avez les liens pour récupérer le n\"), ('130046', \" vidéo-là. Vous avez les liens pour récupérer le notebook et tout ça, et d'essayer de voir comment je peux l'appliquer en utilisant R. Après, ce long, long, long préambule, mais c'est très important qu'on comprenne ce que je veux faire. Parce que je me suis rendu compte que parfois, et on va sur des raccourcis et du coup, on a des idées préconçues et on n'a pas ce que vous souhaitez obtenir. Mais en réalité, ce que l'étudiant souhaite obtenir et moi, ce que je veux mettre en avant, peut-être que\"), ('130047', \"moi, ce que je veux mettre en avant, peut-être que ce n'est pas toujours en phase. Donc, le préambule est toujours très important. Allons-y alors. Mais du coup, j'ai préparé ici, voilà, mon petit...  et ce qui serait intéressant d'ailleurs, attendez, je vais montrer ça, c'est que je vais mettre en parallèle ce que j'avais fait, j'ouvre dans l'autre fenêtre, j'arrive tout de suite, j'en ai pour deux secondes, binary classification, et je vais récupérer, je l'ouvre avec un autre navigateur que j'a\"), ('130048', \"pérer, je l'ouvre avec un autre navigateur que j'apprécie énormément, c'est DuckTuckGo, je l'ai ouvert, normalement ça va s'ouvrir, il est là. Donc l'idée, il est là, c'est de reproduire ça, mais sous R. C'est ça l'idée, c'est vraiment ça l'idée, c'est de reproduire ce tutoriel-là, mais en utilisant R, c'est ça l'idée. Allons-y. Très bien, je vais laisser ouvert ici, et on va voir qu'est-ce que ça donne. Alors moi, je vais réduire un peu ça du coup. Bon, le fenêtre ici, je n'ai pas besoin plus q\"), ('130049', \"up. Bon, le fenêtre ici, je n'ai pas besoin plus que ça, et j'affiche les caractéristiques des données nickel, ensuite j'affiche les premières lignes pour bien vérifier on a bien les différentes informations voilà les données, tout ça et avec en dernière position la classe donc les descripteurs, les x si vous voulez je vais les isoler dans une structure à part. C'est ce qui se passe ici. Une fois que c'est fait ça, ces variables-là, je dois les standardiser. Si je regarde le code Python, on est \"), ('130050', \"tandardiser. Si je regarde le code Python, on est à cette étape-là, et là, la standardisation, j'utilise ici un standard Scaler. Là, sous R, je vais utiliser l'outil Scaler. Comme sous Python, là, je ne l'ai pas fait sous Python, mais je vérifie si les données sont bien centré-réduites. Donc j'affiche les moyennes et j'affiche les écarts-types. Les moyennes sont nulles, aux précisions près des librairies de calcul, et les écarts-types sont bien égales à 1. Donc c'est des opérations standards qua\"), ('130051', \"gales à 1. Donc c'est des opérations standards quand on veut lancer des perceptrons. On charge les données, on isole les X, elles sont toutes numériques ici pour nous faciliter la vie, et ensuite je fais une standardisation. Là pour le coup c'est un centrage d'éduction. Très bien. De la même manière pour la variable de cible, j'affiche les distributions, et je vais les coder en 0.1. Voilà. Donc ici, j'ai utilisé un map sur Python. Voilà. Et là, j'utilise un as numérique avec un condition. Et on \"), ('130052', \"'utilise un as numérique avec un condition. Et on a bien les mêmes distributions là et là. Très bien. Puisque là, c'est nickel également. OK, moi je continue. Je charge la librairie Torch. Donc j'ai fait un install.package de Torch. j'ai fait un install.package de ReadExcel pour lire les fichiers Excel. Et là, pour le coup, il faut bien préciser, quand j'ai appelé la première fois à la librairie, il y a une question qui m'a été posée en disant « il y a des programmes additionnels qu'il faut inst\"), ('130053', \"il y a des programmes additionnels qu'il faut installer ». Il faut faire oui, bien sûr. Et en fait, il y a une boîte de dialogue qui apparaît en dessous de RStudio. Donc, je ne l'ai pas vue au départ. Moi, j'attendais pendant des plombes. Je me suis dit « mais qu'est-ce qui se passe? Pourquoi il y a un plantage ou quoi? » Et c'est juste par hasard que j'ai vu qu'en dessous, il y avait une fenêtre en réalité. donc je les cliquais dessus et j'ai vu qu'il fallait installer tout un programme supplém\"), ('130054', \" qu'il fallait installer tout un programme supplémentaire. C'est ce que j'ai fait, on le fait qu'une seule fois. Mais il faut bien voir qu'on doit le faire. Très bien, donc ici je charge les torches et les X là je les transforme en tenseurs. Que c'est manipuler torches, exactement comme ici. Torches flottent, tensor, torches flottent, tensor, torches, tensor. On joue, hein. Vous avez vu, hein, c'est quand même pas mal. Très bien, on a les dimensions ici. Très bien, j'affiche les premières valeur\"), ('130055', \"ons ici. Très bien, j'affiche les premières valeurs. Très bien. Ok, ensuite, qu'est-ce qui se passe? Très bien, on a bien les mêmes valeurs d'ailleurs, bon, aux précisions près. Alors, ensuite, qu'est-ce qui se passe? Très bien, je fais la même pour la cible, je fais le codage de la cible et je le mets en tensor. Le codage est déjà fait et je le mets en tensor. Voilà les différentes valeurs. Très bien. Alors, avec torche, il y a différentes manières de faire, mais la manière de faire la plus int\"), ('130056', \"res de faire, mais la manière de faire la plus intéressante, à mon sens, c'est de passer par des classes. Donc la structure et le comportement du réseau sont définis par une classe. Exactement comme ici. Alors là, c'est le perceptron simple. Je vais aller au niveau du perceptron multicouche cette fois-ci. Il est où? Il est plus loin. Il est là. Donc ce que j'ai fait là, là, avec Python, je vais le faire avec R. On le voit ici. à un moment donné il y avait une pub dessus dans les années 90 entre \"), ('130057', \"l y avait une pub dessus dans les années 90 entre Pepsi et Cola et il y avait un scientifique avec une éprouvette et tout ça qui faisait des manipulations chimiques puis à la fin il disait c'est exactement la même chose les gens de ma génération se rappellent bien de cette pub là Pepsi Coca, là c'est pareil regardez donc j'ai défini la classe, très bien ensuite j'ai le constructeur là c'est init, là c'est initialize ensuite j'ai mis la liste des outils que je vais utiliser. Il y a P neurones de \"), ('130058', \"outils que je vais utiliser. Il y a P neurones de la couche d'entrée, ça va être 6 quand je vais appeler le réseau, quand je vais instancer le réseau. Il y a 2 neurones de la couche intermédiaire, et de la couche intermédiaire vers la couche de sortie, il y a 2 à 1. Donc c'est bien cette structure-là. Il y en a 6 d'abord, vers 2, et ensuite, on a de 2 vers 1. Voilà, c'est exactement ce qu'on a là. Et la fonction de transfert, ça va être sigmoïde, sigmoïde. Donc c'est bien ce qu'on a là. La sigmo\"), ('130059', \" sigmoïde. Donc c'est bien ce qu'on a là. La sigmoïde et la sigmoïde. Alors, ce qui est intéressant, c'est que c'est exactement la même chose que vous avez mis. J'ai fait exprès de mettre les mêmes termes, les mêmes noms de fonctions. Donc là, j'ai mis les différents éléments. J'ai mis forward1, on voit bien, forward2, voilà. Et ensuite, le forward qui combine forward1 et forward2. On voit bien, très bien les idées ici. Alors, qu'est-ce qu'il reste à faire? J'instancie exactement pareil, j'insta\"), ('130060', \"te à faire? J'instancie exactement pareil, j'instancie la classe. Je n'ai pas lancé, peut-être. Sinon, ça va planter. Très bien. Une fois que c'est en mémoire, j'instancie. C'est-à-dire que je crée l'objet et j'affiche les caractéristiques du réseau. Et là, il met le nombre de poids calculé ou le nombre de coefficients estimés. Il dit que de l'aïr 1, il y en a 14. Et pour l'aïr 2, il y en a 3. Pourquoi? Regardez bien. Là, il y a 6 variables, plus l'intercept. Biasse en anglais. Mais biais en fra\"), ('130061', \" l'intercept. Biasse en anglais. Mais biais en français, ça fait bizarre, mais c'est bien ça. Très bien. Pourquoi il y en a 14 paramètres estimés? Pourquoi il y en a 14 coefficients estimés? Parce que de cette couche intermédiaire vers les couches d'entrée, il y en a 6 plus 1, ça fait 7. Plus, voilà, pour le deuxième neurone de la couche cachée également, il y en a 7. 7 plus 7, ça fait 14. C'est pour ça qu'il vous dit qu'il y en a 14 ici. Ensuite, il y en a 3. Pourquoi? Parce que de la couche ca\"), ('130062', \", il y en a 3. Pourquoi? Parce que de la couche cachée vers la couche de sortie, il y a deux neurones dans la couche cachée, donc un paramètre et un deuxième paramètre de coefficient estimé, plus le troisième qui est le biais ici. Et donc on a bien la bonne structure du réseau. OK. Ensuite, je définis deux choses. Un, la fonction de coût. Donc là, je prends une MSLOS. D'un côté, j'ai des valeurs 0, 1. Et d'autre côté, j'ai des valeurs, des probabilités estimées qui vont entre 0 et 1. Du coup, ut\"), ('130063', \"ilités estimées qui vont entre 0 et 1. Du coup, utiliser une MSLOS, c'est une très bonne idée ici. On peut en utiliser d'autres. ok, de la même manière l'algorithme d'optimisation c'est une descente de gradient stocatique que je vais utiliser ça c'est exactement la même chose là et là alors très bien, donc j'essaye, je vais tester déjà le réseau bon là je l'ai appris tout de suite là ce que je vais faire moi c'est autre chose c'est que déjà je vais appliquer le réseau sur des données d'apprentis\"), ('130064', \"is appliquer le réseau sur des données d'apprentissage pourquoi je fais ça? parce que quand vous instanciez votre réseau là il initialise avec des points aléatoires Et donc, si vous appliquez ce réseau avec des points aléatoires, avec des coefficients définis aléatoirement sur votre données, vous allez obtenir la perte initiale. Le MSLOS initial. Donc là, j'applique et ça me donne une idée des structures. On a bien une proba en sortie. Vous avez bien vu. J'affiche la dimension de la proba. Et en\"), ('130065', \"bien vu. J'affiche la dimension de la proba. Et en fait, c'est une matrice à une seule colonne. Donc du coup, j'applique cette première colonne-là seulement, qui est une probabilité, vous avez bien vu là, et je vais le confronter aux valeurs 0,1, qui est encodée de la variable cible. Donc d'un côté, on a une proba qui est variante 0 et 1, et on va confronter ça avec le 0,1, qui est l'encodage de la variable cible. Bien sûr, il faut que les probas soient le plus proches possible de 0 ou 1, pour q\"), ('130066', \" soient le plus proches possible de 0 ou 1, pour que l'affectation soit parfaite, si vous voulez. Ok. Que les valeurs des probabilités soient bien tranchées dans le bon sens. Du coup, je calcule ici la perte initiale 0,23. Vous, vous aurez une valeur différente, ce qui est tout à fait normal. Pourquoi? Parce que les poids sont définis aléatoirement. Il n'y a pas eu d'andom stat, là. Donc, c'est vraiment aléatoire, en réalité. Vous êtes d'accord là-dessus? Qu'est-ce qu'il faut faire, alors? Il fa\"), ('130067', \"à-dessus? Qu'est-ce qu'il faut faire, alors? Il faut lancer l'apprentissage. Alors, dans mon exemple sous Python, regardez, j'avais défini une fonction parce que je l'ai utilisée deux fois, simplement. donc je n'allais pas faire un copier-coller, un copier-coller soit interdit, on est bien d'accord là-dessus. Donc là, j'avais défini une fonction pour l'appeler deux fois. Bon, là, je ne l'appelle qu'une fois, donc je ne voyais pas l'utilité de créer une fonction, donc je l'ai fait directement ici\"), ('130068', \"er une fonction, donc je l'ai fait directement ici. Ce qui est intéressant, je vais le lancer parce que ça prend un peu de temps, et comme ça, j'ai le temps de présenter. Là, c'est le nombre des poches. Bon, là, je n'avais plus mis 6, 6, 10 000, là, je n'avais mis que 5 000 parce que sinon, ça prend trop de temps. Voilà, et ensuite, j'ai créé un vecteur pour récupérer les valeurs des époques à chacune des étapes. Ensuite, là, j'interre sur le nombre des poches.  donc les poches et les passages e\"), ('130069', \"bre des poches.  donc les poches et les passages en entier sur la base d'apprentissage. Très bien. Ensuite, je remets à zéro tous les gradients. J'applique le modèle sur les données, avec les coefficients calculés à l'étape ITER. Très bien. J'ai les sorties, c'est les probas, on est bien d'accord là-dessus, et je calcule le critère en confrontant ces probas d'affectation avec les valeurs de la variable cible. C'est ce qu'on avait fait ici, de manière très scolaire. ça, là on a calculé probat et \"), ('130070', \"ière très scolaire. ça, là on a calculé probat et on calcule le critère c'est ce qu'on fait ici, on calcule les probat on calcule les critères donc la valeur de la perte je récupère dans un vecteur voilà, et ensuite je calcule les gradients et je fais la rétropropagration et on a la mise à jour des coefficients très bien, il a terminé là, vous avez vu, du coup la première valeur de la perte on la connaissait, j'avais calculé à la main tout à l'heure, c'est 0,23 et la dernière valeur de la de la \"), ('130071', \"ure, c'est 0,23 et la dernière valeur de la de la MSLOS c'est 0,03 voilà ici Voilà, valeur de la perte ici. C'est plutôt, il y a une certaine... Du coup, j'affiche la graphique de décroissance, exactement comme tout à l'heure. Là, je suis allé jusqu'à 10 000. Là, ce n'était pas nécessaire, en fait. Donc, je suis allé jusqu'à 5 000, tout simplement. En utilisant un gradient simple ici. Voilà. Du coup, qu'est-ce qui se passe maintenant? Je veux évaluer mon modèle sur l'échantillon test. Est-ce qu'\"), ('130072', \"luer mon modèle sur l'échantillon test. Est-ce qu'il tient la route ou pas? Exactement comme il se passe ici. Donc, je vais charger les données test. Voilà. Très bien. Et je vais isoler les X pour pouvoir les transformer. Très bien également, c'est ce que j'ai fait ici. Très bien. Voilà. Et je vais les standardiser en calculant les paramètres calculés sur l'échantillon d'apprentissage. La moyenne et l'écart-type calculés sur l'échantillon d'apprentissage. Regardez ici. Là, on ne fait pas de fit.\"), ('130073', \"ntissage. Regardez ici. Là, on ne fait pas de fit. N'allez surtout pas faire de fit. On ne fait qu'un transforme. Donc là également, il faut que je ne fasse qu'un transforme. C'est-à-dire que je fasse le SCAL en utilisant la moyenne calculée sur l'échantillon d'apprentissage et l'écart-type calculé sur l'échantillon d'apprentissage. Mais est-ce que j'ai ces informations-là? La réponse est oui. En fait, quand j'avais fait le SCAL sur l'échantillon d'apprentissage, il avait calculé les données tra\"), ('130074', \" d'apprentissage, il avait calculé les données transformées et il avait conservé en mémoire la moyenne et l'écart-type qu'il a utilisées. Voilà. Donc, il y a des attributs qui sont là. Il y a la matrice qui est là, des données transformées. et vous avez des attributs, c'est la moyenne calculée sur les échantillons d'apprentissage, il appelle ça Scalette Center, et l'écart-type calculé sur les échantillons d'apprentissage. Du coup, quand je vais appeler Scal ici, il faut que j'appelle quelque cho\"), ('130075', \"ppeler Scal ici, il faut que j'appelle quelque chose qui ressemble à Transform. Voilà, c'est exactement ce que j'ai fait là. J'appelle Scal sur les données, sur les données test, sur les descripteurs en test, et le paramètre de centrage, c'est la moyenne calculée sur les échantillons d'apprentissage, et le paramètre de normalisation, c'est l'écart-type calculé sur l'échantillon d'apprentissage. Du coup, j'affiche les premières valeurs. On a bien ces valeurs-là. On a bien la cocomitance des valeu\"), ('130076', \"ces valeurs-là. On a bien la cocomitance des valeurs. Et je le mets sous format de transport, en sort. Une fois que j'ai ça, je peux appliquer les données sur l'échantillon test. C'est ce que je fais là. Et une fois que c'est ça, j'ai les probas d'affectation. Comme j'ai les probas d'affectation, qui s'est en format tensor ici, il faut que je la mette en vecteur numérique. C'est ce que je fais là. Et c'est pour pas d'affectation, je vais la transformer en une affectation. S'il est supérieur à 0,\"), ('130077', \"former en une affectation. S'il est supérieur à 0,5, je prédis malignante. Et s'il est inférieur à 0,5, je prédis strict à 0,5, c'est malignante. Et inférieur à 0,5, c'est bénigne. C'est ce que je fais ici. Très bien. Qu'est-ce qu'il reste à faire alors? Ce qu'il reste à faire, c'est la matrice de confusion. Je fais la matrice de confusion et je calcule l'accuracie qui est de 97%. C'est à peu près ce qu'on devrait obtenir ici. Il me semble, alors je regarde ça. C'est quelque part par là, je me s\"), ('130078', \" je regarde ça. C'est quelque part par là, je me suis embrouillé un peu là, mais c'est quelque part par là, voilà. C'est pareil. Voilà, donc c'est exactement tout ce schéma-là que j'ai mis en œuvre. C'est bien vu. Le train, j'ai fait le scale, j'ai créé le perceptron. Il est prêt. Ensuite, j'ai importé le test. J'ai appliqué le scale, toujours en utilisant la moyenne et l'écart-type calculé sur le train. Ensuite, j'ai fait le predict. J'ai dû transformer les probas d'affectation en classes predi\"), ('130079', \"nsformer les probas d'affectation en classes predict. Et ensuite, j'ai ici les classes observées sur l'échantillon test, les classes predict sur l'échantillon test. Et j'ai créé la matrice de confusion. et j'ai calculé l'accuracie à un moindre taux d'erreur. Alors, qu'est-ce qui est intéressant? Ce qui est intéressant, c'est ce que je vais expliquer dans mon cours très rapide que je ferai devant mes étudiants, c'est que je leur dis qu'il y a deux manières de voir un perceptron miti-couche avec u\"), ('130080', \" manières de voir un perceptron miti-couche avec une couche cachée. C'est que soit on est dans l'espace originel, dans cette partie-là, et on a des séries de droites qui vont s'imbriquer, ou bien on est dans un espace intermédiaire, et dans l'espace intermédiaire, on a un séparateur linéaire. Du coup, j'aimerais voir comment se situent les points dans l'espace intermédiaire. Et est-ce que les classes sont séparables linéairement dans l'espace intermédiaire? Oui, visiblement, puisqu'on a un accur\"), ('130081', \"ermédiaire? Oui, visiblement, puisqu'on a un accuracy de 97%. Mais c'est mieux de le voir, on est d'accord. Donc, qu'est-ce qui se passe? Rappelez-vous, je reviens dessus, il y avait un forward 1 pour obtenir la sortie de ce réseau-là, il y avait un forward 2 dans mon classe de calcul, pour obtenir la sortie de ce réseau-là. Donc, si j'applique mon forward 1 sur mes données test, j'obtiens ce qui est à la sortie d'ici. Vous êtes d'accord? Regardez deux secondes notre code. Elle est où la classe?\"), ('130082', \"z deux secondes notre code. Elle est où la classe? Elle est là. On voit bien, il y a un forward1 qui fait une première sortie, et ensuite il y a un forward2 qui fait la sortie finale. On voit bien l'imbrication ici. Là, typiquement, j'aurais pu utiliser un pipe, mais regardez bien, là, forward1 me sort out1, la sortie de la couche cachée, Et j'applique le forward 2 sur le out 1 pour obtenir out 2. Donc là, j'aurais pu faire un pipe directement et ne pas utiliser out 1 ici. Mais là, c'est là qu'o\"), ('130083', \" ne pas utiliser out 1 ici. Mais là, c'est là qu'on perd les étudiants avec ça, au début. Après, quand ils ont l'habitude de manipuler le pipe et tout ça, ils comprennent bien les enchaînements. Mais au début, quand c'est dans une phase de compréhension, on n'allait pas faire des choses comme ça, parce que très rapidement, ce ne sera pas lisible. Et vous allez perdre les étudiants avec ça, clairement. Donc, toujours schématique, toujours lisible dans les phases d'initiation. Après, quand les étu\"), ('130084', \"dans les phases d'initiation. Après, quand les étudiants ont pris confiance et qu'ils ont compris les schémas globaux, là, on peut aller dans les subtilités. Mais on ne commence jamais avec des subtilités. Je dis ça. Bien. Du coup, qu'est-ce qui se passe alors? Pour obtenir la sortie intermédiaire, je prends les données test, là, et j'applique forward. Et j'ai une matrice, effectivement, avec deux colonnes. Il y a 300 lignes, là. Bon, j'ai affiché que les cinq premières lignes, parce que sinon, \"), ('130085', \"é que les cinq premières lignes, parce que sinon, ça fait un affichage ingérable. Voilà. Mais il y a bien 300 lignes et deux colonnes. Pourquoi deux colonnes? Parce que là, il y a deux sorties. Très bien. Alors, du coup, là, c'est un tensor. Ça ne va pas être possible de manipuler directement pour faire un plot dessus. Donc, je l'ai transformé en une matrice. Je l'ai appelé hidden. C'est bien les mêmes valeurs là et là. On est d'accord là-dessus. Sauf que la matrice, on peut la manipuler. Et du \"), ('130086', \" Sauf que la matrice, on peut la manipuler. Et du coup, j'ai fait un plot, voilà, en illustrant les classes avec... Je vais mettre ça plus grand d'un coup. En illustrant les classes avec des couleurs. Rouge pour les bénignes et bleu pour les malignantes. C'est ce qui se passe ici. J'ai mis la légende de toute manière. Et voilà. Du coup, la séparation, on voit bien, elle va être plutôt par là. Et du coup, on voit bien les rouges qui sont du côté des bleus et les bleus qui sont du côté des rouges.\"), ('130087', \"es bleus et les bleus qui sont du côté des rouges. Voilà ce qui va se passer ici. On est d'accord là-dessus. Et on a bien entre 0 et 1 et 0 et 1. Pourquoi? Parce qu'on a appliqué une fonction d'application en sigmoïde, tout simplement. Très bien. J'ai appliqué la même idée ici. Le graphique peut être un peu différent. Ici, parce que peut-être que j'ai mis autre chose comme fonction de tension. Ah non, j'ai bien fait la même chose. Mais bon, comme les poids sont... C'est un apprentissage, c'est u\"), ('130088', \" les poids sont... C'est un apprentissage, c'est une heuristique. Donc, on n'a pas forcément les mêmes graphiques. Quand vous allez faire, vous allez répéter ça, on n'aura pas forcément les mêmes graphiques. Mais on aura bien des nuages de points séparables linéairement. Ça, c'est sûr. Très bien. Voilà. Et du coup, on peut récupérer les coefficients qui sont là. C0, C1, C2, je peux les récupérer en réalité. C'est ce que je montre ici. Voilà. Donc, on a les poids ici dans la loyer 2. Les poids as\"), ('130089', \", on a les poids ici dans la loyer 2. Les poids associés. Très bien. Et on a l'intercept qui est là, le C0. C1, C2, C0. On les a là. Du coup, c'est tout à fait possible de tracer la frontière linéaire dans l'espace intermédiaire défini par la couche que j'ai. Je vous laisse ça, ça je l'ai fait ici. C'est ce que j'ai fait ici. Je me suis amusé à le faire. Mais vous, à titre d'exercice, vous pouvez le faire. Mais le principal message, il est là. Le principal message, il est là, c'est que ce que j'\"), ('130090', \" principal message, il est là, c'est que ce que j'ai fait avec PyTorch pour Python ici, je peux très bien le reproduire en utilisant torche pour R. Tout à fait. Et là où c'est très intéressant, c'est qu'on voit bien la similitude des étapes et du coup du code entre ces deux approches-là. Donc là, j'ai repris ce que j'avais fait ici, et j'ai repris ce que j'ai reproduit ici. Qu'importe le flacon pourvu qu'on est livré à...  Quelqu'un qui avait compris bien des choses, je me plais aussi à répéter \"), ('130091', \"pris bien des choses, je me plais aussi à répéter ça. Mais c'est vrai, pour le coup, là on voit bien, on peut faire exactement la même chose. En tout cas, dans ce domaine-là, je ne dis pas que c'est généralisé, parce que je connais également mes camarades informaticiens qui disant « mais non, mais avec si on peut faire ça, non, non. » Ok, très bien, très bien, très bien. De toute manière, moi aussi, règle numéro X, c'est que, on met toujours le code en ligne, comme ça tout le monde peut récupére\"), ('130092', \"ode en ligne, comme ça tout le monde peut récupérer et reproduire ce qui est montré. C'est ça aussi, ça va être un élément qui me paraît extrêmement important dans tous les domaines liés à la recherche. Voilà, excellent travail à tous. Bon, demain, quand même, on va travailler sous Python, mais sachez que c'est possible de le faire sous R. Excellent travail à tous.\"), ('140001', 'Bien, c\\'est parti ! Alors, dans cette vidéo, je vais parler de la comparaison entre R et Python. Oh là là, je sais très bien que c\\'est un sujet polémique, je connais très bien mes amis informaticiens, tout de suite ce genre de discussion sur le langage, ça vire à l\\'hystérie. On est bien d\\'accord là-dessus. Les pros aussi disent \"oh là là\", mais moi je veux que ça. Les pros disent ainsi de suite, ainsi de suite, et on n\\'échappe pas à ça dans la data science, il y a les pros R d\\'un côté, les pros '), ('140002', 'ta science, il y a les pros R d\\'un côté, les pros Python de l\\'autre, et on n\\'arrive pas à discuter parce que, souvent, on ne sait pas de quoi on discute. Et c\\'est ça, c\\'est ça que je vais essayer d\\'essayer de remettre un peu d\\'aplomb dans cette vidéo aussi.\\n\\nQuand on compare deux outils, deux langages, qu\\'est-ce qu\\'on compare réellement, et comment on va le comparer ? C\\'est ça l\\'idée, c\\'est ça l\\'idée. Souvent, c\\'est \"avec R, je peux faire ci, avec Python, je peux faire ça\", mais généralement, c\\''), ('140003', 'c Python, je peux faire ça\", mais généralement, c\\'est qu\\'une facette du problème, ça tient au package, mais il y a d\\'autres éléments en ligne de compte qu\\'il faut absolument voir.\\n\\nIl y a différents aspects à considérer. Le premier aspect, c\\'est le langage en lui-même. La syntaxe, est-ce que vraiment c\\'est l\\'élément de différenciation ? Je ne sais pas, on peut programmer de la même manière dans les deux langages, bien sûr, il y a des fonctionnalités supplémentaires dans l\\'un pour le langage, je '), ('140004', \"tés supplémentaires dans l'un pour le langage, je ne te fais pas dans l'autre, ainsi de suite, mais grosso modo, on est à peu près, ça tient la route, tous les deux tiennent parfaitement la route. Déjà, la syntaxe du langage en lui-même, on peut faire de la programmation objet dans les deux cas, on peut faire de la programmation objet dans les deux cas, je répète des fois qu'on ne m'entendrait pas bien, donc là, je ne vois pas très bien encore, il pourrait y avoir un débat sans fin.\\n\\nLe deuxième\"), ('140005', \"l pourrait y avoir un débat sans fin.\\n\\nLe deuxième élément important, c'est la technologie. On a dans les deux cas des interpréteurs, mais qui en réalité font de la compilation à la volée. Ils génèrent du bytecode qui est lancé par un runtime. Ça, généralement, les étudiants ne le savent pas trop. Et moi, je leur dis, parce qu'il y a un débat aussi sur le fait que c'est rapide, pas rapide, langage interprété, mais en réalité, ces langages-là, maintenant, en fait, intègrent la compilation à la vo\"), ('140006', \"ntenant, en fait, intègrent la compilation à la volée. Alors ça, j'en parle très rapidement, parce que je vais montrer ici, justement, pour l'intégration à la volée.\\n\\nEt là, il y a un document que j'ai trouvé de 2023 de Luc Tierney. Luc Tierney, je suis allé voir qui ce monsieur-là, donc ce monsieur-là est très impliqué dans le développement de R. On est absolument d'accord là-dessus. Donc, j'ai lu en détail son article, il explique comment il procède, qu'est-ce qu'il y a d'important là-dedans, \"), ('140007', \"ocède, qu'est-ce qu'il y a d'important là-dedans, je n'ai pas tout lu, il y a 123 pages, mais bon, les principales idées, je les ai bien vues, en tous les cas, et ce qui est important de bien voir, c'est que quand vous lancez du code R, la première fois que vous le lancez, il va le compiler à la volée en bytecode lancé par un runtime. Ça, c'est un élément important. Très bien.\\n\\nAlors, qu'est-ce qui se passe du côté du Python ? Du côté du Python, il y a la même chose. Il y a un just-in-time égale\"), ('140008', \"il y a la même chose. Il y a un just-in-time également. Donc, il génère également du bytecode. Donc là, j'ai trouvé un article. J'ai vu ce monsieur-là. Je lui ai dit, qui c'est ce monsieur-là ? Je suis allé voir. Et là, on a sa page GitHub. Très bien. Il est très impliqué dans ses pythons de développement. C'est quoi ces pythons ? En réalité, c'est l'implémentation de référence du langage. C'est écrit sur Wikipédia. Voilà, très bien. Et effectivement, on a un compilateur qui génère un bytecode. \"), ('140009', \"ment, on a un compilateur qui génère un bytecode. On est bien d'accord là-dessus. Donc là, du coup, la vraie différenciation en termes de performance pure se fera au niveau du bytecode, en réalité. C'est ce qu'on va essayer de voir aujourd'hui, justement. Cet aspect-là, on va essayer de voir.\\n\\nAlors, on a vu, un, la syntaxe. Donc, ce n'est pas un élément de différenciation. Enfin, je ne pense pas. Deux, la technologie, finalement, elles ont des technologies similaires. Et trois, il y a aussi des\"), ('140010', \"echnologies similaires. Et trois, il y a aussi des algorithmes qui sont implémentés dans les packages. Donc justement, il y a un exemple qui me revient souvent. À un moment donné, dans mon cours de NLP, il y a un étudiant qui se lance dans une organisation logistique. Il est sous R, il lance sa plante, il va sous Python, hop, ça marche. Il m'a dit que Python est mieux que R. J'ai dit, mais attends, c'est une simplification, ça. Explique-moi déjà sous R quelle fonction tu as utilisé de quel packa\"), ('140011', \"sous R quelle fonction tu as utilisé de quel package. Il m'a dit, j'ai utilisé GLM. Je lui ai dit, GLM, c'est PackageStats, très bien, et ce qui est implémenté, c'est un fichier scoring. Donc on est dans un cadre où il avait une centaine d'individus et des milliers de variables, de tokens, si vous voulez. Du coup, je lui ai dit, mais dans un fichier scoring, il va calculer la matrice est sienne. Donc si tu as 7000 descripteurs, il va créer la matrice 7000-7000. Donc c'est normal qu'il ait planté\"), ('140012', \"rice 7000-7000. Donc c'est normal qu'il ait planté en réalité. Il me dit, ah bon, je suis, bien sûr. Il y a bien le vecteur gradient d'un côté, mais il calcule la matrice sienne. Donc, bon, voilà, très bien. Et j'ai dit, et sous Python ? Ah ben, sous Python, j'ai utilisé Skit-learn. OK, mais j'ai dit, mais Skit-learn, la régulation logistique, en réalité, elle est basée sur une descente de gradient. Donc, c'est un autre algorithme. Il n'a pas la matrice sienne. On n'a pas les variances des coeff\"), ('140013', \"matrice sienne. On n'a pas les variances des coefficients. On a uniquement les coefficients directement. C'est normal, justement, parce que c'est du à l'algorithme utilisé. Donc, c'est deux algorithmes différents, en réalité. Ce n'est pas une question de langage. C'est qu'on a des packages qui ont des implémentations différentes. Donc, c'est à nous de regarder quel est le package qui est utilisé et quel algorithme il y a derrière. Donc, on compare des choses qui ne sont pas comparables. Quand on\"), ('140014', \"e des choses qui ne sont pas comparables. Quand on dit que dans R, la régression logistique plantée, pas dans Python, c'est parce qu'on a des packages différents avec des implémentations différentes, des algorithmes différents.\\n\\nSachant que, puisqu'on parle des packages également, il y a la question justement de la qualité de l'implémentation des packages. Il y a certains packages qui sont très bien programmés et il y a des packages qui sont très mal programmés. C'est comme la blague des inconnu\"), ('140015', \" mal programmés. C'est comme la blague des inconnus, il y a les bons chasseurs et les mauvais chasseurs. Là, c'est pareil. Il y a des packages qui sont extrêmement bien programmés, puis il y a des packages qui sont de qualité relativement approximative. Donc, c'est à nous de vérifier ça, c'est à nous de regarder. Mais ce n'est pas une question de langage, encore une fois, ni de technologie, là, pour le coup. On est bien d'accord là-dessus. Très bien.\\n\\nAlors bien sûr, il y a aussi quelque chose q\"), ('140016', 'ien.\\n\\nAlors bien sûr, il y a aussi quelque chose qui revient souvent, c\\'est \"mais avec R on peut faire ci, avec Python on peut faire ça\". Voilà, donc c\\'est bien également. Donc on me dit par exemple avec Python, il n\\'y a pas des analyses factorielles comme FactoMiner ou ADE4. Je dis mais non, il faut s\\'enseigner. Souvent on reste sur des idées anciennes, mais ces outils-là évoluent très vite et les packages arrivent très vite également. Donc c\\'est à nous d\\'être à date, d\\'être tout le temps en fa'), ('140017', \"t à nous d'être à date, d'être tout le temps en faire de la veille technologique pour regarder ce qui est disponible. Si je prends l'analyse factorielle, typiquement, il y a des packages excellents, comme F-Analysis, d'Olivier Garcia, qui est là, ou il y a également Scientist Tools, de Duverrier. Excellentissime ! Donc, n'allez pas dire que sous Python, on ne peut pas faire des analyses factorielles. Si, il y a des outils excellents qui font de l'analyse factorielle. A contrario, il y a des gens\"), ('140018', '\\'analyse factorielle. A contrario, il y a des gens qui me disent \"Mais sous R, on ne peut pas faire du deep learning.\" Oh là là, les gars, il faut s\\'enseigner un peu, peut-être. Si, en fait. Si en fait, sous R, il y a TensorFlow, il y a Keras, c\\'est même Torch R qui permet d\\'utiliser la librairie Torch, justement. Donc, faire du deep learning sous R, c\\'est tout à fait possible également. Mais là, encore une fois, c\\'est une question de disponibilité des packages, ça n\\'a rien à voir avec la techno'), ('140019', \"té des packages, ça n'a rien à voir avec la technologie, on est d'accord là-dessus. Donc, c'est à nous de discerner ça, c'est à nous de regarder également quand je compare sur quel thème je compare. Et là, on peut monter une expérimentation pour regarder réellement Qu'est-ce qui est intéressant ? Qu'est-ce qui n'est pas intéressant ? Après, il y a des questions de chapelle. Il y a des questions de la pratique. Mais ça, ça dépasse le cadre de la technologie. Vous arrivez quelque part, tout le mon\"), ('140020', \"echnologie. Vous arrivez quelque part, tout le monde fonctionne sous Python. Vous n'allez pas dire, vous êtes des banquignoles, moi, j'utilise QR. Ou à l'inverse, vous arrivez dans un organisme quelconque qui fonctionne sous R et vous allez leur dire, non, les gars, vous êtes totalement dépassés. Tout le monde utilise Python maintenant. Il faut tout remettre à zéro. Non, ça ne marche pas comme ça. Ça ne marche certainement pas comme ça. Vous arrivez quelque part, vous regardez les outils et à pa\"), ('140021', \"vez quelque part, vous regardez les outils et à partir de là vous vous adaptez. C'est pour ça que moi généralement dans tous les cas, dans ma pratique, à moi même si on utilise de plus en plus Python, clairement mais quand même je veux absolument que les étudiants soient toujours à date sous R, restent performants sous R, il faut qu'ils soient complétents d'une part et performants sous R. Alors, bien sûr l'autre élément qui est très important également, c'est que, en fait, programmer c'est pas j\"), ('140022', \"lement, c'est que, en fait, programmer c'est pas juste appeler des fonctions il y en a qui pensent Je ne veux pas dire des noms, mais il y a des personnes qui pensent que faire du machine learning, c'est trend test split, fit, predict et confusion matrix. Ça y est, je sais faire de la data science. Mais non, ce n'est pas appris des commandes. À un moment donné, il faut savoir programmer, il faut implémenter des fonctions complexes. Et là, il faut savoir programmer, il faut savoir optimiser ses p\"), ('140023', \" savoir programmer, il faut savoir optimiser ses programmes. Vous avez un programme qui, pour une tâche, prend 10 secondes, et tu as un autre programme et pour la même tâche, ça prend 10 minutes, la réponse est vite faite. Donc le programmeur joue aussi un rôle fondamental dans l'histoire. Ce n'est pas une histoire de R ou Python, c'est une question de compétence du programmeur, de performance du programmeur. Il faut savoir programmer, il faut savoir implémenter et optimiser son code. J'en parle\"), ('140024', \"voir implémenter et optimiser son code. J'en parle souvent à mes étudiants, je leur dis, quand vous créez votre code, je leur demande de programmer des packages, très bien, de programmer des applications. À un moment donné, j'ai dit il faut absolument que votre programme soit correct déjà c'est pas mal voilà il faut qu'ils soient optimisés il ya aussi la lisibilité du code mais ça c'est un autre aspect voilà et je leur parle justement des outils de profiling de code là je suis sur sur air studio\"), ('140025', \"de profiling de code là je suis sur sur air studio là et il ya des outils qui permettent de comptabiliser chaque ligne donc là c'est un tuto que j'ai fait il ya quelques années voilà 2019 où je montrais que en réalité quand on programme voilà on a des outils qui permettent de surveiller la qualité, la rapidité des bouts de code, pour savoir où est-ce qu'il faut optimiser. Et dans ce tutoriel-ci, justement, je montre que dans RStudio, il y a un profiler qui permet de surveiller le temps d'exécuti\"), ('140026', \"ofiler qui permet de surveiller le temps d'exécution de chaque bout de code ou de chaque bloc de code. Et ça nous permet, nous, par la suite, de venir travailler là-dessus pour l'optimiser. Alors, ces outils, ça existe sous R dans RStudio. Forcément, ça existe sous Python. Et moi, je montre qu'on peut l'avoir également ici dans Spider. Dans Spider, c'est à peu près la même époque, 2019. Généralement, je mets l'un et l'autre tout le temps. On a les outils pour le faire. Donc là également, je peux\"), ('140027', \"s outils pour le faire. Donc là également, je peux avoir un suivi du temps de calcul de chaque bout de code pour essayer de voir comment ça fonctionne et les temps d'exécution de chaque bout de code. C'est ce qu'on a ici. On a le feed, fait ça, 30 secondes, 30 secondes. Et moi, du coup, j'ai la possibilité d'aller chercher les endroits où c'est intéressant pour optimiser mon code. Bien.\\n\\nAprès, ce long préambule, mais bon, c'est très important, parce que je connais suffisamment mes amis informat\"), ('140028', \"arce que je connais suffisamment mes amis informaticiens, pour savoir que comparaison et répétons, généralement, ça va partir en sucette. Donc, moi, ce que je veux ici comparer, justement, c'est de comparer la qualité de l'interprèteur. Et par extension du coup, la qualité du just-in-time, du compilateur à la volée, tout simplement, qui génère du bytecode qu'on peut exécuter. Alors pour comparer ça, je vais me placer dans un cas très particulier, c'est la programmation du triabule. Alors triabul\"), ('140029', \" c'est la programmation du triabule. Alors triabule, il est là, très bien, c'est un algorithme de tri, qui est en haut de N2, généralement, on a là. Mais moi, ce qui m'intéresse, c'est quoi ? C'est de voir, un, donc la qualité du just-in-time dans les deux cas, R et Python, avec les différents aspects. Un, les accès à un vecteur, lecture et écriture, un vecteur, très bien. C'est lecture et écriture sur une case, donc atomique, très bien, ça c'est le premier aspect que je veux voir, de manière ré\"), ('140030', \" le premier aspect que je veux voir, de manière répétée. L'autre aspect que je veux voir, c'est la gestion des boucles, est-ce qu'il y a performante ou pas. Si je prends le cas de R, par exemple, il y a une vingtaine d'années, quand j'ai commencé à monter mon cours de R et le faire à mes étudiants, à l'époque, faire des boucles, c'était juste suicidaire sous R. Mais ça, c'était il y a 20 ans, si vous voulez. Aujourd'hui, les boucles sous R sont extrêmement performantes. Donc ça, ça fait partie d\"), ('140031', \"xtrêmement performantes. Donc ça, ça fait partie des éléments de performance du Just-in-Time. Donc un, l'accès au vecteur. Deux, la qualité de la programmation des boucles dans le Just-in-Time. Et l'autre aspect important que je vais essayer de voir, justement, c'est la gestion des branchements conditionnels. Voilà, donc c'est les trois aspects que je veux voir pour essayer d'évaluer la qualité du Just-in-Time. Les accès aux vecteurs, lecture-écriture, un vecteur, les boucles, voilà, boucles for\"), ('140032', \"iture, un vecteur, les boucles, voilà, boucles fort ici, mais bon, les boucles en général, très bien, et l'autre aspect que je veux voir, c'est les branches pour conditionner. Et on a ces différents éléments dans le triabule, justement. Donc là, je vais programmer le triabule super simple, je vais programmer le basic de chez basic. Et bien, alors, une fois que j'ai bien casé ça, donc c'est ça que je vais essayer de voir ici, c'est la qualité de la technologie, le just-in-time compilaires, en reg\"), ('140033', \"a technologie, le just-in-time compilaires, en regardant ces trois aspects-là. Voilà, je montre le code. Alors, pour voir ça, hop là, j'avais oublié cette histoire-là, je vais montrer ça rapidement, très bien, hop là, je vous montre ici, voilà, je vais travailler sur deux jeux de données. Voilà, c'est deux vecteurs. Il y a un premier vecteur ici, très bien, on a les valeurs, là, c'est des valeurs, il y a 5000 individus, il y a 5000 valeurs. Je vais faire un triabule qui a une complexité quadrati\"), ('140034', \"is faire un triabule qui a une complexité quadratique, donc je ne vais pas me lancer avec 100 000 valeurs, on est bien d'accord là-dessus. L'idée ici, c'est de voir la qualité du lasting time, on est bien d'accord. Très bien. Donc, je ne vais pas comparer un triabule avec un quicksort, on est bien d'accord là-dessus. C'est comme comparer un fichier scoring avec une descente de gradient, on est bien d'accord là-dessus. Très bien. Ça aussi, c'est un élément très important, il faut discerner ce qui\"), ('140035', \"n élément très important, il faut discerner ce qui est du langage, de la technologie, et ce qui est de l'algorithme. Donc, on est bien sur du triabule. Donc, il y a un premier jeu de données qui est là, 5000 individus, on a une plage de valeur, c'est la loi uniforme. Je suis générée avec une loi uniforme, des valeurs aléatoires. Et j'ai généré un deuxième jeu de données, toujours 5000 valeurs, avec cette fois-ci une loi normale. Une centrée réduite. Très bien. Pour voir, c'est une vraie différen\"), ('140036', \"te. Très bien. Pour voir, c'est une vraie différence, mais normalement, il n'y en a pas. Il ne devrait pas y en avoir. Mais on va voir ça, justement. Du coup, j'ai fait les deux omplémentations. Sous Python, voici le code. et sous R, voici le code. C'est exactement le même code. Alors, on n'allait pas me dire, mais pourquoi il a utilisé Spider, il aurait pu utiliser VS Code ? Peut-être que c'est plus rapide. Ouh là là ! Parce qu'il y en a qui me disent ce genre de choses. Mais non, c'est l'inter\"), ('140037', \"disent ce genre de choses. Mais non, c'est l'interpréteur Python qui compte. On est bien d'accord là-dessus. Que l'éditeur de code soit Spider ou VS Code, ça ne va pas changer. On est bien d'accord. C'est le fait que, là, je travaille avec, ici, l'interpréteur Python 3.12.4 et là, c'est l'interpréteur R 4.4.1. C'est ça qui est important ici. Donc la version du compilateur, enfin de l'interpréteur, lui, il est important. L'éditeur de code ne joue pas. Ne joue pas. On est bien d'accord là-dessus. \"), ('140038', \"pas. Ne joue pas. On est bien d'accord là-dessus. Donc j'ai programmé exactement la même chose. Regardez bien. Là, j'ai le code Python à gauche. Très bien. Donc voilà. Là, c'est la fonction. Donc je vais faire le triambule. Et ce que renvoie ma fonction, c'est l'étendue. Donc c'est le max moins le min, tout simplement, pour avoir une valeur unique. Je vais comparer. Normalement, je dois avoir les deux mêmes valeurs. On est d'accord là-dessus. Très bien. Donc là, les implémentations sont les même\"), ('140039', \"s bien. Donc là, les implémentations sont les mêmes. J'ai bien fait l'attention, je vais tout vérifier de toute manière. On a bien exactement le même code. Très bien, vous avez bien vu. Là, je récupère le vecteur. Très bien. Ensuite, je fais les doubles boucles. De là, voilà. Très bien. Et je renvoie le max monumine. Exactement pareil ici. Là, il n'y a pas de souci. Ensuite, qu'est-ce qui se passe ? Dans les deux cas, je charge le jeu de données. Je charge de dossier. Je charge le jeu de données\"), ('140040', \" Je charge de dossier. Je charge le jeu de données. Le premier fichier dans un premier temps. Voilà. et je fais le calcul de l'étendue. Oui. Normalement, on doit avoir les mêmes valeurs, sinon il y a un problème. Vous êtes d'accord là-dessus ? Et une fois que c'est fait ça, je vais calculer l'exécution de 10 fois cette fonction-là pour voir. Là, en fait, quand il l'appelle la première fois, ça dure un peu plus longtemps parce qu'il fait justement la compilation à la volée. Donc c'est pour ça que\"), ('140041', \" la compilation à la volée. Donc c'est pour ça que je l'appelle ici les deux fonctions de manière explicite. Vous êtes bien d'accord là-dessus ? Comme ça, il a la possibilité de faire la compilation à la volée. Ensuite, quand je lance le benchmark, soit là ou là, là, c'est déjà compilé. Donc, c'est le code compilé qu'il appelle, le bytecode, en réalité. On est bien d'accord là-dessus. Voilà. Alors, qu'est-ce que j'ai à dire de plus ? Je fais bien attention. Là, c'est Python 3.12.4 et R4.4.1. Il \"), ('140042', \" attention. Là, c'est Python 3.12.4 et R4.4.1. Il faut regarder. Moi, je ne suis pas ça en détail, parce que ça m'intéresse, mais pas à ce point-là. Mais les versions, les différentes versions, évoluent très vite. Si je prends le cas de Python, je vois des choses qui changent. Regardez, 3.11, 3.12, il se passe des choses. Donc, c'est à nous de suivre ça. Ce n'est pas les nouveautés comme le top 50, mais quand même, c'est important parce que ça joue énormément sur la performance. Le même programm\"), ('140043', \"ue énormément sur la performance. Le même programme que vous avez écrit il y a six mois, vous le lancez avec une version récente de Python, les performances ont changé parce que la technologie évolue très vite. Et elle est améliorée. Là, c'est vraiment très bien écrit pour le coup. Là, également, je conseille la lecture de ce code, de ce document-là. C'est plutôt bien fait et ça permet de voir les efforts qui sont faits par les promoteurs de R d'un côté et de Python de l'autre pour voir ce qui s\"), ('140044', \"un côté et de Python de l'autre pour voir ce qui se passe. Allons-y alors. Je ferme ça et je mets ça ici et je vais lancer sous R. bien, donc je vérifie le code, voilà, je charge en mémoire la fonction, je vais charger les données voilà, je fais run de ça et je fais on a bien les différentes valeurs très bien, on a les 5000 valeurs du coup je vais appeler la fonction très bien, donc on a ici le résultat, regardez bien le résultat 0999 857 ok, une fois que c'est bon là, je vais lancer l'outil qui\"), ('140045', \" fois que c'est bon là, je vais lancer l'outil qui permet de benchmarker, l'outil de benchmarking qu'est-ce qu'il va faire ? Il va lancer 10 fois la fonction et il va mesurer le temps de calcul. Je charge la librairie et j'appelle microbenchmark. Il va lancer 10 fois la fonction et il va calculer le temps de calcul moyen. Nous, on va attendre. Ça arrive assez relativement. J'ai fait quelques tests avant, donc normalement, il n'y a pas trop de soucis. Il m'a dit que le temps moyen, c'est 1,19. Le\"), ('140046', \"cis. Il m'a dit que le temps moyen, c'est 1,19. Le temps médian, ici, c'est 1,18. ok, je relance une deuxième fois voilà, donc le tout ça fait 18 secondes du coup parce que j'ai lancé 10 fois, voilà, mais on doit avoir à peu près les mêmes résultats sachant que, comme j'ai appelé la fonction ici la ligne 26, donc il a fait la compilation à la volée ça y est, c'est un code copilé qui est en mémoire, c'est le bytecode qui est en mémoire et qui l'appelle ici, on a bien ici un 18, un 20, ça se fait \"), ('140047', \"pelle ici, on a bien ici un 18, un 20, ça se fait exagerer du coup on va voir, est-ce que c'est mieux ou c'est moins bon sous Python, on est d'accord là-dessus Donc, rappelons-nous de ça. Je vais mettre ça par là, comme ça on le voit un peu. Et je mets spider ici. Et on va voir si le temps est meilleur ou moins bon. Allons-y. Donc là, c'est pareil. Très bien. Je charge. J'appelle V et on a bien les valeurs. Très bien. Ensuite, là également, j'appelle la fonction. On doit avoir le même résultat. \"), ('140048', \"elle la fonction. On doit avoir le même résultat. 0 999 857 sinon c'est un problème, on est absolument d'accord dessus. Et du coup, je vais mesurer le temps de calcul. Alors, il y a la librairie timeit, mais elle n'est pas bon. Il faut passer soit la code en chaîne de caractère, soit passer par une fonction lambda, ce qui fait du coup un élément supplémentaire dans la pile. Donc, je ne voulais pas utiliser ça. Donc, c'est pour ça que j'utilise ici la fonction magique timeit. Donc, je répète une \"), ('140049', \"i la fonction magique timeit. Donc, je répète une seule fois, mais dix itérations. Exactement comme microbenchmark ici. Times égale à 10, N10. Et je lance. très bien donc il faut faire exactement la même chose il va lancer dix fois l'affaire et on va voir si c'est vraiment plus rapide ou pas ou si c'est plus lent c'est à nous de voir ça justement voir ça donc je lance ici moi je faut attendre un peu ça prend une vingtaine de secondes je vais pas arrêter la vidéo donc j'essaie de meubler un petit\"), ('140050', \"arrêter la vidéo donc j'essaie de meubler un petit peu là suspense on va dire suspense très bien ça va arriver bientôt dit-il et voilà et ça prend 2,2 secondes ça semble moins bon on peut se se poser des questions. On peut se poser des questions parce que là, dans R, j'ai du code purement natif. On est d'accord là-dessus. J'utilise vraiment les outils de base. Il n'y a aucun package utilisé là. On est d'accord. Là, dans Python, effectivement, j'utilise aussi du code natif, sauf que là, à cet end\"), ('140051', \"tilise aussi du code natif, sauf que là, à cet endroit-là, je manipule un vecteur numpy. Donc, c'est un package. Donc là, il y a un package qui intervient là. On est d'accord là-dessus. Alors, c'est numpy. Bon, numpy, vous connaissez. Je suis allé sur la page web. Je n'avais pas commencé à dire que Numpy est mauvais. On est d'accord là-dessus. Surtout qu'ils disent que c'est optimisé, ainsi de suite, ainsi de suite. Ok, très bien. Néanmoins, quand même, il y a un écart là. On est bien d'accord l\"), ('140052', \"d même, il y a un écart là. On est bien d'accord là-dessus. Je vais relancer une deuxième fois pour être sûr. Je ne sais jamais pourquoi il y a un écart là. Donc, je relance de nouveau. Mais bon, j'ai plutôt confiance en Numpy, si vous voulez. Mais là, ce ne sont pas des calculs matriciels que je fais. On est d'accord. c'est que je fais un accès atomique sur des cellules, des vecteurs, sans lecture, sans écriture. Très bien. Donc, ce n'est pas des configurations où ce type de librairie est très \"), ('140053', 's configurations où ce type de librairie est très performant. Vous êtes d\\'accord là-dessus ? Et voilà, de nouveau, il y a un écart. Alors, je vois déjà les pro-R qui disent \"Alléluia, c\\'est excellent!\" Et puis, les pro-Piton qui disent \"Mais c\\'est n\\'importe quoi!\" « Oh, voilà, ce n\\'est pas vrai, il l\\'a mis dans des mauvais outils, enfin, je ne sais pas ce qu\\'ils m\\'ont aimé dire. » Bon, moi, comme je réfléchis un peu, ça m\\'arrive, très bien, je me suis dit, mais OK, tout ce que j\\'ai fait, c\\'est p'), ('140054', \" suis dit, mais OK, tout ce que j'ai fait, c'est pareil, dans les deux cas, on a bien les deux mêmes choses, là. On est d'accord. La seule différenciation qu'il y a, c'est qu'ici, voilà, je passe par une librairie, en réalité. Là, c'est un objet numplique, je passe. si je fais type là de v.d.values voilà il me dit bien que c'est un vecteur numpy alors j'avais dit également que vecteur numpy ça paraît plutôt performant bon, mais si je vais vraiment sur des objets natifs cette fois-ci donc voilà p\"), ('140055', \"t sur des objets natifs cette fois-ci donc voilà par exemple les listes alors je peux poser des questions les listes quand même, les listes en réalité c'est un vecteur de pointeur en vrai à peu près puisqu'on peut mettre des éléments hétérogènes Mais là, pour le coup, le liste, c'est vraiment un élément qui est natif à Python. Donc, peut-être qu'on aurait des performances différentes dans ce cas-là. Du coup, je vais modifier mon code. Voilà. Donc là, regardez bien, en entrée, là, c'est un vecteu\"), ('140056', \" là, regardez bien, en entrée, là, c'est un vecteur numpy que je récupère en entrée. Je le transforme en liste et tout le code, je ne le change pas. Et on va voir si c'est mieux ou pas bien. Donc, je supprime la console pour être sûr d'avoir une console nickel. Et de nouveau, je vais relancer. Mais donc, ce qui change, c'est là. C'est juste là. C'est-à-dire que cette fois-ci, je ne passe pas par un vecteur d'un pi, je passe par une liste, qui est un objet natif de Python. Bien. Je relance ici. E\"), ('140057', \" un objet natif de Python. Bien. Je relance ici. Ensuite, je fais le calcul. Normalement, je dois obtenir le même résultat que sous R. Sinon, ça va être un vrai souci. Très bien. et relançons de nouveau le time it. On va voir. J'avoue qu'on va avoir une surprise. On va avoir une vraie surprise que je n'attendais absolument pas. Là, pour le coup, ça m'a un peu scié. Mais bon, comme quoi, il faut bien réfléchir et il faut regarder ce qu'il y a derrière. Regardez le temps. Ce n'est plus deux second\"), ('140058', \"ière. Regardez le temps. Ce n'est plus deux secondes. C'est placé à une seconde. Et on est dans le même ordre d'idées que sous R. Je vais relancer une deuxième fois. Ça, c'est quand même assez étonnant. Et en fait, j'en arrive à une conclusion. Déjà, il faut bien connaître les outils pour pouvoir les utiliser de manière optimale. Ça, c'est une première conclusion très importante. La deuxième conclusion qui est extrêmement importante, c'est qu'en réalité, R et Python font de la compilation à la v\"), ('140059', \"réalité, R et Python font de la compilation à la volée. Et en termes de technologie, sous cette compilation à la volée-là, ils sont assez équivalents. Ça, je l'ai remarqué à plein de fois parce que je passe beaucoup de temps à développer. Voilà, bon, pour mes TD, je ne développe plus des outils que je mets en ligne, c'était une autre époque, mais je passe beaucoup de temps à développer pour mes TD, pour voir les exercices que je peux faire aux étudiants, pour vérifier les projets, et ainsi de su\"), ('140060', \"udiants, pour vérifier les projets, et ainsi de suite. Et j'ai remarqué, cette conclusion-là, c'est que les technologies sont assez équivalentes en termes de performance du just-in-time, du compilateur, si vous voulez, qui compile du bytecode, on est d'accord là-dessus. Là également, je fais une petite parenthèse de seconde, qui est très importante. R et Python génèrent du bytecode. Donc, ils ont besoin d'un runtime derrière. En revanche, si je prends un autre cas qui est Julia, j'entends beauco\"), ('140061', \"rends un autre cas qui est Julia, j'entends beaucoup parler de Julia, ça fait des années que je tourne autour. Ça fait un certain nombre d'années que je tourne autour. Julia, en revanche, lui, il ne génère pas du bytecode. Il génère vraiment un binaire natif. qu'on peut lancer du QID et on a des performances proches des langages compilés. Julia, typiquement, ce n'est pas du bytecode. C'est vraiment du binaire natif. Et donc, on a des performances proches des langages compilés. Je répète deux foi\"), ('140062', \" proches des langages compilés. Je répète deux fois parce qu'on n'aurait pas bien compris. Alors, la dernière conclusion qui est très importante, c'est qu'il ne faut pas dire tout ou n'importe quoi. Se lancer dans des comparaisons qui sont d'une vacuité souvent totalement innommable, ça ne sert à rien. Quand il y a les pros R d'un côté, les pros Python qui s'écharpent, ça n'a aucun intérêt. Parce que généralement, on ne sait pas ce qu'on compare réellement. Donc, il faut être très clair. Voilà. \"), ('140063', \"réellement. Donc, il faut être très clair. Voilà. Qu'est-ce qu'on compare vraiment ? Dans quel cas on se positionne ? Ensuite, il faut comparer ce qui est comparable. Donc là, dans mon exemple ici, j'ai pris exactement les mêmes codes. Regardez bien. J'ai pris exactement le même code. Voilà. En utilisant les mêmes procédures. Exactement. Très bien. Et l'autre aspect important, mais ça également, vous savez très bien que moi, je suis attaché depuis toujours, il faut que quand on monte une expérim\"), ('140064', \"s toujours, il faut que quand on monte une expérimentation, il faut que tout le code soit mis en ligne pour que tout le monde puisse le répéter à l'identique. Donc c'est pour ça que moi, je mets tout sur ce site-là. Donc là, je vais finir la vidéo et je vais rajouter un chapitre ici, R&P ton performance comparée. et les deux codes qui sont là, je vais le mettre ici pour que vous ayez accès. C'est sur ce site-là, j'enlève ça. Je mettrai ici le code, comme ça, tout le monde peut comparer. Et ça do\"), ('140065', \"e, comme ça, tout le monde peut comparer. Et ça doit être une règle. Quand quelqu'un vous dit, c'est un tel, c'est ci, et pas l'autre, et tout ça, mets ton code en ligne. Et quand tu mets ton code en ligne, nous, on peut le charger, on peut le vérifier. Bon, avec GitHub et tout ça, je vois dans les publications maintenant qui a l'obligation de publier son code en ligne pour que ce soit vérifié par les rapporteurs, je trouve que c'est un excellent principe. À chaque fois qu'on compare, à chaque f\"), ('140066', \" principe. À chaque fois qu'on compare, à chaque fois qu'on dit quelque chose, ça doit être vérifiable par tout un chacun et reproductible. C'est tout l'intérêt de mettre le code en ligne. Bien, dernier message très important ici. En fait, il n'y avait pas de polémique dans l'histoire. Il nous appartient à nous de connaître parfaitement les différents outils et il nous appartient de choisir l'outil le plus adapté par rapport à nos objectifs et par rapport aux circonstances dans lesquelles, les c\"), ('140067', \"r rapport aux circonstances dans lesquelles, les contextes dans lesquels on se place. Et c'est ça qui est important. C'est notre rôle à nous, ça. Ce n'est pas l'outil qui doit faire le data scientist. C'est le data scientist qui choisit l'outil le plus adéquat par rapport à ce qu'il veut faire et le contexte dans lequel il se place. Voilà. Sur ce petit message, je vous souhaite à tous un excellent travail.\"), ('150001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de l'extension R pour NIME. Donc, la possibilité dans NIME de rentrer du code R et d'utiliser les fonctionnalités de R et des paquets associés. Alors, j'essaie de resituer les choses pour que tout le monde puisse profiter pleinement de cette vidéo. J'enseigne la data science, enfin le machine learning à l'université. Très bien. Et je privilégie essentiellement deux outils. Pour beaucoup, Python, avec des packages associés de machine lear\"), ('150002', \"Python, avec des packages associés de machine learning, et aussi R. Alors pourquoi? Parce que ces outils-là permettent de programmer, et donc ça nous permet d'aller plus loin que le simple clic-bouton, si vous voulez, sans que ce soit plus le ratif par ailleurs. Mais bon, ça permet d'aller plus loin que le simple... J'ai des commandes uniquement liées à l'appel des fonctions. Alors, ça n'empêche pas que je reste ouvert. Je reste ouvert, et parmi les outils que j'apprécie particulièrement, il y a\"), ('150003', \"les outils que j'apprécie particulièrement, il y a NIME. Alors, j'en ai beaucoup parlé, j'en ai beaucoup parlé de NIME, parce que j'apprécie vraiment beaucoup, c'est un outil particulièrement puissant, je trouve, vraiment, et il y a vraiment des possibilités assez intéressantes en termes de mise en œuvre des projets de machine learning. Alors, bien, ça reste un outil low-code, et parfois, on a besoin de faire soit des choses complexes, On peut quand même programmer. Il y a des boucles, il y a de\"), ('150004', \"and même programmer. Il y a des boucles, il y a des actions conditionnelles. Dans le workflow concret, on peut faire des choses assez sophistiquées. Mais parfois, on a besoin d'aller plus loin que ça et notamment d'utiliser la puissance de certains packages qui sont disponibles que sous R ou sous Python. Donc la possibilité de combiner les outils est un élément pour lequel je milite énormément. Je pense que c'est très intéressant de savoir utiliser un outil, de savoir utiliser plusieurs outils, \"), ('150005', \"er un outil, de savoir utiliser plusieurs outils, encore mieux de savoir les combiner pour attirer la quintessence. Donc voilà. J'avais fait une vidéo il n'y a pas longtemps. Dans cette vidéo-ci, justement, je montrais la possibilité d'intégrer du code Python dans NIME, Voilà, des composants avec du code Python dans les... Alors, pour qu'il n'y ait pas de jaloux, parce que je sais très bien que parfois, ça entraîne des discussions à ne plus finir ces histoires-là, je me suis dit, il faut exactem\"), ('150006', \" ces histoires-là, je me suis dit, il faut exactement que je fasse la même chose pour R. Donc, j'ai fait des requêtes Google, comme d'habitude, Google est mon ami, et j'ai trouvé... Alors, j'ai trouvé des choses intéressantes. J'ai trouvé déjà ici l'installation de guide, très bien, qui est pas mal. Voilà, ensuite, on a la liste des composants que j'ai trouvée sur un autre site web, qui est là. Voilà. donc je commence à comprendre ce qu'il faut faire mais trouver un tutoriel suffisamment clair p\"), ('150007', \"aire mais trouver un tutoriel suffisamment clair pour pouvoir le faire en français je l'ai un peu cherché mais je l'ai pas trouvé en français, on est bien d'accord en anglais j'en ai trouvé quand même et notamment en anglais il y en a un qui est pas mal, qui est celui-ci donc vous avez l'adresse qui est là j'ai un peu de combats bien en faisant, on cherche de prendre mon clé et dans ce tutoriel-ci ce qui est intéressant, du 16 février 2023 donc il n'est pas trop ancien non plus. Il y a différent\"), ('150008', \"l n'est pas trop ancien non plus. Il y a différentes étapes que je vais essayer de reproduire, et ensuite je vais essayer d'aller plus loin, justement en créant une étude qui soit spécifique. Un peu plus sophistiqué que le simple succession de commandes simples. Très bien, c'est cette idée. Mais bon, je conseille quand même de regarder ce tutoriel-ci, parce qu'il parle de l'installation dans NIME, ensuite de la connexion entre NIME et R, qui sert de moteur de calcul pour certaines étapes, et aus\"), ('150009', \" de moteur de calcul pour certaines étapes, et aussi la question des packages, comment on installe des packages. Parce que c'est un aspect qui sera très important. Je vais revenir dessus en détail tout à l'heure, parce que ça me paraît vraiment important. Allons-y. Alors première étape, du coup, comment j'installe ce package-là spécifique qui permet d'utiliser R dans Naim. Donc j'ai lancé Naim ici, et en réalité Naim fonctionne également avec le mécanisme des packages. Donc je vais tout simpleme\"), ('150010', \"mécanisme des packages. Donc je vais tout simplement « Help, install new software », je vais aller sur le dépôt qui m'intéresse, moi je prends tous les sites comme dépôt. et là j'ai la possibilité de choisir les extensions que je veux mettre en place donc intégration par exemple R donc il est en train de chercher et il va me dire, on a la possibilité ici, vous avez vu donc j'ai récupéré tout simplement ici R statistics, alors j'ai pris le standard j'ai pas pris le Windows binaries, j'ai pris le \"), ('150011', \"d j'ai pas pris le Windows binaries, j'ai pris le standard donc statistics, si je mets statistics normalement ça doit arriver également alors il ne propose pas ici parce que je l'ai déjà installé c'est pour ça qu'il n'est pas proposé mais on peut l'installer du coup ça c'est fait, ça c'est la première étape donc j'ai récupéré et vous avez ici dans script dans scripting gr donc c'est bien celui-là, on est bien d'accord c'est bien celui-là que j'ai installé donc là dans la copie d'écran que j'ai m\"), ('150012', \" installé donc là dans la copie d'écran que j'ai monté tout à l'heure et elle n'apparaissait pas parce que je l'ai déjà installée. Mais si elle n'est pas installée, cette proposition-là sera visible et vous avez la possibilité de l'intégrer. Du coup, j'ai des nouveaux composants ici qui apparaissent, qui permettent d'intégrer du code R dans votre workflow. Ça, c'est un premier élément. Deuxième élément maintenant, une fois que j'ai installé le package, il faut que je dise au package, le moteur R\"), ('150013', \"ckage, il faut que je dise au package, le moteur R, il faut le chercher à tel endroit. C'est ce qu'on fait ici. donc c'est ce que je fais ici, donc je vais dans Préférences, et dans Name, on a R. Voilà, et tout simplement, j'ai mis le chemin d'installation de R sur ma machine. Donc, c'est normalement C, Programme File, R, et la version de R que vous avez. Moi, j'ai la version 4.4.1. Alors, voilà, donc une fois que la connexion est faite, c'est bon. en revanche, là où c'est très important si vous\"), ('150014', \"n. en revanche, là où c'est très important si vous avez besoin de paquets spécifiques il faut l'installer en amont dans R pour pouvoir les utiliser dans Naim dans la connexion Naim vers R pour pouvoir en profiter il faut que dans R vous installez les paquets alors c'est ce que j'ai fait moi-même donc pour bien montrer comment j'ai fait donc là j'ai lancé RStudio mais c'est ça qui m'intéresse c'est la console qui m'intéresse et je vais voir la liste des packages qui sont installés. Je vais voir l\"), ('150015', \"te des packages qui sont installés. Je vais voir la liste des packages qui sont installés. Très bien. Voilà. Alors, si je fais print de V, il y a plein de choses. Donc, pour avoir la liste des packages, je récupère uniquement la colonne package. C'est ce que je fais ici. Voilà. Et voilà la liste des packages déjà installée. Alors là, dans mon tuto, dans ce tuto-ci, qu'est-ce que je vais faire? Je vais faire une analyse factorielle. Voilà, avec le paquet du PSYS. Alors, pourquoi PSYS? Parce qu'il\"), ('150016', \" paquet du PSYS. Alors, pourquoi PSYS? Parce qu'il y a deux choses qui m'intéressent. un, il fait des rotations de facteurs, ce que ne font pas les autres packages. J'aurais pu utiliser par exemple FactoMiner ou bien d'autres ADE4 et tout ça, mais dans ces packages-là, je n'ai pas les rotations de facteurs, donc ça pour moi, ça ne me convient pas. Et le deuxième aspect qui a fait que j'utilise PSYCH, c'est qu'il y a la fonction PREDICT, qui permet de projeter des individus supplémentaires. Dans \"), ('150017', \"t de projeter des individus supplémentaires. Dans les autres packages, notamment francophones, les individus supplémentaires sont spécifiés de manière à part et qui sont appelés dans le train. Il ne faut pas faire un prédict, si vous voulez. Et moi, je voulais absolument faire un prédict sur un échantillon supplémentaire. Du coup, voilà. C'est pour ça que j'ai pris Psyche. Donc Psyche, par exemple, il est là. Il est où Psyche, là? Ce n'est pas en alphabétique, donc Psyche, il doit être par là. L\"), ('150018', \" alphabétique, donc Psyche, il doit être par là. Là, voilà, voilà. Il est là. Alors, il y a un deuxième facteur également que j'ai utilisé, c'est le package FDM2ID. Alors, FDM2ID, qui permet de faire des camins. F 2M des id, il est par là. F 2M des id, voilà, voilà, voilà. Très bien, qui permet de faire des camins. Là également, les camins existent déjà dans le package standard de R, dans stats, mais là, pour le coup, je voulais utiliser ce package-là parce que, de nouveau, il y a la fonction pr\"), ('150019', \"ge-là parce que, de nouveau, il y a la fonction predict, qui permet de projeter des individus supplémentaires, de connaître le cluster d'appartenance des individus supplémentaires et l'assure d'un camin. Tout simplement pour ça, en fait. Peut-être que dans les camins standards, il y en avait, mais bon, je n'ai pas vu, je n'ai pas vu. Encore une fois, les packages, c'est à nous de vérifier s'ils tiennent la route ou pas. C'est ce que j'ai fait moi-même. Je ne vais pas utiliser un package qui ne t\"), ('150020', \"-même. Je ne vais pas utiliser un package qui ne tient pas la route. Déjà, ça je l'avais parlé dans mon cours de programmation R avec les étudiants. Un, on vérifiait si le package a été publié dans le journal de R, ça veut dire qu'il a été validé par des scientifiques, et si ce n'est pas le cas, si c'est le cas, c'est déjà rassurant. Et si ce n'est pas le cas, c'est à nous, parce qu'on a une certaine expertise, d'aller récupérer le package, d'aller récupérer le code source du package, et de véri\"), ('150021', \"er récupérer le code source du package, et de vérifier ce qui se passe. C'est ce que j'ai fait, tout simplement. Donc, ces paquets-là, par exemple, ils sont là. Donc là, j'utilisais Coreplot, puisque ça, tout le monde connaît. Sinon, j'utilisais Psyche, qui permet de faire des analyses factorielles avec l'analyse en composantes principales, notamment avec la fonction predict, que je n'ai pas trouvé par ailleurs, à ma connaissance, peut-être qu'il y en a d'autres. Et pareil, F2MID, qui permet éga\"), ('150022', \" y en a d'autres. Et pareil, F2MID, qui permet également de faire un predict, à partir d'un k-mins, ce qui permet de rattacher des individus supplémentaires et de savoir à quel cluster je dois le rattacher. Bien, une fois que c'est bon ça, du coup, je peux commencer à travailler, mais avant de commencer à travailler, je parle...  de l'analyse que je vais mettre en place et les données que je vais mettre en place. Alors, les données sont là. Et les données qui sont là, je vais l'ouvrir rapidement\"), ('150023', \"s données qui sont là, je vais l'ouvrir rapidement. Alors, c'est une base que j'ai utilisée par ailleurs ici. J'avais fait une vidéo sur les auto-encodeurs. Voilà, par là. Et c'est cette base, c'est la même base que je vais utiliser. Alors, dans la base qu'il y a, c'est des véhicules, très classiques, qui sont décrits par un certain nombre de variables, toute quantitative ici. Et on a ici les individus illustratifs, qui sont par là. Donc les individus supplémentaires que je vais projeter dans l'\"), ('150024', \"vidus supplémentaires que je vais projeter dans l'espace factoriel et également que je vais rattacher au cluster que je n'aurai pas la suite. Donc c'est bien un tandem clustering que je vais mettre en place. D'abord, je projette les individus dans un espace factoriel et ensuite, dans l'espace factoriel, je fais un clustering, une classification automatique, si je parle français, je fais une classification automatique en créant des groupes. des groupes dans lesquels les individus sont très simila\"), ('150025', \"oupes dans lesquels les individus sont très similaires, et en essayant de faire des groupes très dissimilaires en temps. C'est l'idée. Allons-y, alors. Une fois que tout ça s'est dit, allons-y, je peux me lancer. Alors, comme la dernière fois, dans l'extension Python pour R, je vais créer un groupe de workflow, voilà. Et ces extensions, j'appelle ça. Voilà. et dans Essay Extension, je vais créer un workflow qui est extension R avec un exemple de clustering. Sur Factor. Boum! Très bien. Je l'ouvr\"), ('150026', \"clustering. Sur Factor. Boum! Très bien. Je l'ouvre, c'est bien. Alors, les étapes sont très classiques. Déjà, il faut que j'importe les données. C'est ce que je vais faire maintenant. Je vais importer les données. Les données que j'ai montrées tout à l'heure. C'est bien ce fichier-là avec la première feuille d'Aber. D'abord, les individus actifs qui sont là. Allons-y. Donc là, je prends l'outil Excel Reader. Voilà. Et je spécifie la base que je vais utiliser. Donc, allons-y. Rose, je vais sur l\"), ('150027', \"vais utiliser. Donc, allons-y. Rose, je vais sur le bureau, dans démo. Et je prends les individus actifs. Alors, il y a uniquement que c'est variable. Ce qui m'intéresse, bien sûr, la colonne modèle, là, c'est un identifiant. Donc, je ne vais pas l'utiliser dans mes calculs. Mais je vais l'utiliser pour faire les graphiques. On est d'accord là-dessus. J'exécute, je vérifie que tout est nickel, mais normalement, tout va bien. On a bien la base qui est là. Tout est impeccable. Avec les 28 individu\"), ('150028', \" est là. Tout est impeccable. Avec les 28 individus. Alors, première étape, c'est quoi? Déjà, j'aimerais faire un corrélation plot pour voir les données, mais surtout pour voir comment je peux mettre en place cela. Alors, je vais dans les composants R, et dans les composants R, toujours dans le mécanisme de Naim, vous avez les objets en entrée, type d'objet d'entrée, avec les ports d'entrée, et les objets en sortie, avec les ports de sortie. Donc, il faut absolument que j'arrive à gérer ça. Donc\"), ('150029', \", il faut absolument que j'arrive à gérer ça. Donc, je prends ici, j'ai un peu regardé quand même, et je prends RViewTable. Qu'est-ce qui se passe dans RViewTable? Ce qui se passe, c'est que là, en entrée, il prend une table, c'est le flèche noir donc ce raccordement va être possible et en sortie c'est un rectangle vert c'est un carré vert pour le savoir je lis tout simplement la documentation c'est ce que je dis également aux mes étudiants 90% de mon temps c'est lire la doc soit lire les articl\"), ('150030', \"e mon temps c'est lire la doc soit lire les articles scientifiques qui sont sous-jacentes aux méthodes que j'essaie de voir si elles sont intéressantes ou pas et ensuite lire la documentation des packages qui les implémentent parce que souvent entre la théorie d'écrire dans les articles scientifiques et le paquet de lui-même, parfois, quand ce n'est pas les mêmes auteurs, parfois, il y a des simplifications, ou des raccourcis, ou je ne sais pas quoi. Même quand c'est les mêmes auteurs, d'ailleur\"), ('150031', \"uoi. Même quand c'est les mêmes auteurs, d'ailleurs, des fois, il y a des raccourcis. Donc, c'est à nous absolument de vérifier ça. Alors, je regarde ici. Très bien. Donc, la documentation, je ne vais pas la lire devant vous. Tout simplement, ce que j'apprends, c'est que le rectangle vert, là, veut dire qu'on a une sortie image. Très bien. Allons-y. Je fais la connexion. Une fois que je fais la connexion, je regarde le code que je vais pouvoir entrer. Déjà, un premier élément important, c'est qu\"), ('150032', \"trer. Déjà, un premier élément important, c'est que je me rends compte qu'il y a une variable prédéfinie, nameIn, qui désigne les informations qui arrivent en entrée. Ce qui arrive avec la flèche noire en entrée est stocké dans une variable prédéfinie qui a un nom. nameIn. Et le nameIn, je regarde dans le workspace, à droite ici, le nameIn est un dataframe. Donc, ce que je reçois en entrée, c'est un dataframe issu du Excel Reader, et avec un dataframe avec les colonnes suivantes. Moi, je vais fa\"), ('150033', \"frame avec les colonnes suivantes. Moi, je vais faire un correlation plot. Et dans le correlation plot, j'utilise que les variables puissance jusqu'à poids. Voilà, un modèle, je ne vais pas l'utiliser dans la corrélation, on est bien d'accord là-dessus. Très bien. Donc, je vais récupérer NaimIn en data frame. Je vais exclure la colonne modèle et je vais calculer la corrélation que je vais afficher avec corrélation plot. Sachant que dans R, j'ai installé déjà le package core plot. C'est très impo\"), ('150034', \"nstallé déjà le package core plot. C'est très important. Regardez ici dans R, il y a déjà le package core plot que j'ai installé. Il est où là? Il est là. D'accord. Donc, ça, c'est un travail à faire en amont. Il faut installer R déjà. Ensuite, raccorder NIME sur R. Et dans R, il faut installer les paquets dont vous avez besoin. Sinon, vous ne pouvez pas les utiliser. Vous êtes d'accord là-dessus. Alors, dans Save ici, je peux fermer ça. Du coup, je récupère le code. Bon, j'ai préparé mon code à\"), ('150035', \" je récupère le code. Bon, j'ai préparé mon code à l'amont parce que sinon, je ne m'en sors pas. Et je le mets ici. Je ne vais pas le taper devant vous. J'ai mis des commentaires. Tout ce code-là sera mis en ligne, bien sûr. On est bien d'accord là-dessus ou non? C'est pas ça que je voulais faire. Tout ce code-là, j'ai mis en ligne. Donc, qu'est-ce qui se passe? Je récupère le dataFarm à l'entrée, qui est là, et je le mets dans une variable df. Ensuite, dans df, j'exclus la colonne modèle. C'est\"), ('150036', \"nsuite, dans df, j'exclus la colonne modèle. C'est ce que je fais ici. Ensuite, à partir de df, où il y a puissance à poids, je calcule la corrélation. Je charge la libre-recore plot, qui est déjà installée dans le R que j'ai raccordé à Naim. Et j'affiche la corrélation plot. Voilà, le graphique, si vous voulez. Et dans la sortie standard, j'affiche la matrice de corrélation. Allons-y, alors. OK, exécute. Voilà, j'exécute en OpenView. Du coup, là, il fait appel à R. Il fait appel à R, voilà, et \"), ('150037', \", il fait appel à R. Il fait appel à R, voilà, et il récupère le graphique. Voilà, et on a, voilà, la conciliation plot. Bon, les variables sont toutes, c'est des voitures des années, enfin, des certaines années. Donc, à l'époque, donc toutes ces variables, ce qui paraît tout à fait logique, c'est que toutes ces variables-là sont corrélées, la puissance est corrélée avec la cylindrée, avec la vitesse, la longueur, le poids, il n'y a que la hauteur, il y a un élément différent. Alors, ça c'est un\"), ('150038', \"r, il y a un élément différent. Alors, ça c'est un premier élément, donc ça c'est la sortie standard, mais on a aussi le standard, enfin, on avait le view là, le view c'était le graphique, et la sortie standard c'est ce qui est issu du print. donc allons-y, voilà, on a bien ce qui est issu du print d'accord ici, c'est ce que j'ai affiché en sortie voilà alors, j'avais dit que en réalité, la sortie qui est là, c'est une image c'est pas moi qui le dis, j'avais dit c'est R qui le dit vue image alor\"), ('150039', \"dis, j'avais dit c'est R qui le dit vue image alors il y a autre chose, c'est ce que j'ai mis ici voilà, ça c'est l'issue du print le standard d'autre pouce, c'est l'issue du print s'il y a des messages d'erreur, il récupère également alors si c'est une image donc normalement je dois pouvoir afficher je suis allé voir les outils qui permettent de manipuler des images voilà et par exemple j'ai image view image view est par là bon je vais mettre comme ça parce que je ne le trouverai pas image view\"), ('150040', \"mme ça parce que je ne le trouverai pas image view voilà qui est là et effectivement regardez c'est cohérent c'est compatible, pourquoi c'est compatible? parce que là j'ai un rectangle vert et là, en sortie, et là, j'ai un rectangle vert en entrée. Je peux faire la connexion. Vérifions alors si je peux visualiser. Effectivement, je peux visualiser. Alors, également, je peux également le sauvegarder dans un fichier PNG. Dans un fichier, alors, c'est image-bretter ici. Donc là également, c'est com\"), ('150041', \"st image-bretter ici. Donc là également, c'est compatible. Pourquoi? Parce que, regardez, la sortie, c'est un rectangle vert. L'entrée, c'est un rectangle vert. Je peux faire la connexion. Et de nouveau ici, je configure et je dis, je vais sauvegarder sur le bureau, dans démo, et je mets mon corplot..png, il y a différents formats,.png, et j'enregistre. Voilà, s'il existe, il l'écrase, et j'exécute. Vérifions s'il est là, il est là. Vous avez vu, on est le 30 octobre 2024, il est 10h45. Si je do\"), ('150042', \" on est le 30 octobre 2024, il est 10h45. Si je double-clic, je peux l'afficher. Il est là. Il est bien d'accord là-dessus. C'est pas mal, c'est vraiment pas mal. Et le mécanisme de Naim est assez intéressant pour ça. Alors bon, on peut renommer les sommets. Je ne le fais pas habituellement, mais comme on a fait le commentaire, je le fais. Voilà, on peut renommer les sommets.  individu actif, par exemple. Voilà. Il y en a qui aiment boire ça, pourquoi pas, mais bon, ce n'est pas l'essentiel. Voi\"), ('150043', \"rquoi pas, mais bon, ce n'est pas l'essentiel. Voilà. Une fois que c'est fait, du coup, qu'est-ce qui se passe? J'aimerais faire maintenant mon analyse en deux temps, mon tandem clustering. Donc, je vais faire d'abord la projection factorielle dans le plan, et dans cette projection factorielle dans le plan, via une ACP, il y a tout un tas de choses que j'aimerais voir. J'aimerais voir la matrice des corrélations, j'aimerais voir la projection des individus, ainsi de suite. Voilà. En utilisant le\"), ('150044', \" individus, ainsi de suite. Voilà. En utilisant les fonctionnalités de R. Là, pour le coup, même s'il y a une ACP dans AM, il n'y a pas notamment une ACP avec une rotation varimax. Vous l'avez ici, PCA. Voilà. Très bien. Vous avez la documentation qui explique. Voilà. Donc, regardez. Ainsi de suite. Voilà. Très bien. PCA compute. Voilà. Compute. Voilà. Très bien. Voilà. Donc, moi, il y a le apply en revanche. Ça, c'est pas mal. Mais bon, moi j'ai besoin de faire une ACP avec une rotation de vari\"), ('150045', \" besoin de faire une ACP avec une rotation de varie max. C'est toujours une surprise, je le fais lundi prochain, je suis avec les étudiants, et je vais parler de l'ACP. La plupart ont fait l'ACP déjà, mais ils ont une vision un peu scolaire de l'ACP. Et moi je leur dis, non, l'ACP peut le voir de différentes manières, et notamment dans l'ACP, on marche rarement dans les ouvrages francophones, c'est les rotations de facteurs qui sont essentielles pour l'interprétation. Souvent en ACP, vous avez u\"), ('150046', \"pour l'interprétation. Souvent en ACP, vous avez un premier facteur qui regroupe tout, et du coup, vous avez une sorte de tautologie, si vous voulez, une pléonasme totale, effet taille, on appelle ça, où toutes les variables sont corrélées avec le premier facteur. Et il faut discerner ça. Et il y a différentes approches, notamment les ACP sur les corrélations partielles, que je verrai également, mais il y a aussi des rotations qui permettent de rendre l'interprétation plus lisible du rôle des fa\"), ('150047', \"endre l'interprétation plus lisible du rôle des facteurs et des positionnements relatifs entre les différentes variables. Je ne vais pas refaire à le cours ici, mais bon c'est pour ça que j'ai voulu utiliser le package psychique alors pour pouvoir utiliser ça donc je reviens de nouveau dans air et bien je vais dans scripting dans air et je vois qu'il ya un air learner voilà donc de nouveau je lis la doc et ce que je vois c'est que deux et il ya deux éléments essentiels en entrée il prend des don\"), ('150048', \"eux éléments essentiels en entrée il prend des données ça va être les données d'apprentissage ici et en sortie on obtient un workspace voilà un espace mémoire si vous voulez dans lequel il ya le modèle et les autres informations ah bah c'est exactement ce que je veux allons-y alors je mets le rleur nerf très bien et je fais la connexion regardons ce qui se passe donc là figure et on a une série d'informations intéressantes déjà regardez bien voilà on a en entrée ici les data frames d'entrée ce q\"), ('150049', \"à on a en entrée ici les data frames d'entrée ce qui est les données qui sont là on est bien d'accord un data frame d'entrée qu'est ce que je veux faire moi parce que je veux faire c'est que je veux faire une acp voilà en utilisant le paquet spsi c'est ce que je suis ici alors je récupère le code 1 alors je supprime ça et je mets ici qu'est ce qui se passe donc non je fais sauvegarde c'est c'est le contrôle S, j'ai le pouce, le pouce et l'index, le rivet sur contrôle S. Qu'est-ce qui se passe? E\"), ('150050', \"le rivet sur contrôle S. Qu'est-ce qui se passe? En l'entrée, j'ai le data frame qui est issu de la table. Voilà, Excel Reader, qui est issu d'Excel Reader. Très bien. Voilà. Donc, ce data frame-là contient un certain nombre de variables dans le modèle dont je n'ai pas besoin. Elle est calcul. Donc, voilà, je récupère les variables actives et j'exclus le modèle. Ensuite, je charge Psy, ce qu'il faut installer dans R d'abord, quand on fait la connexion. Et ensuite, j'appelle la CP de Psy. Donc là\"), ('150051', \"exion. Et ensuite, j'appelle la CP de Psy. Donc là, je mets le nom du paquet, 2.2. la fonction. Ce n'est pas obligatoire. Mais mes étudiants me disent, on peut faire ça marche, ça marche, ça marche. C'est le fameux ça marche. Et moi, je leur dis oui, mais si on veut rendre son code lisible, ça paraît plus intéressant de mettre tout le temps le nom du paquet et le nom de la fonction. Parce que là, c'est un code très simple, mais quand le code devient très sophistiqué, très complexe, l'association\"), ('150052', \"ent très sophistiqué, très complexe, l'association entre package et fonctions, souvent, des fois, on ne s'y trouve pas, et moi, ça me paraît intéressant. Donc, moi, je m'astreins à le faire tout le temps, mais après, on n'est pas obligé de le faire. Donc, je prends les données actives ici, voilà, très bien, je veux deux facteurs, je veux calculer la projection des individus dans l'espace factoriel, et je veux une rotation varie max pour que l'association entre les variables et les facteurs soit \"), ('150053', \"ociation entre les variables et les facteurs soit plus tranchée. Là, il y a deux choses qu'on ne voit pas, ici, donc, je choisis en amont le nombre de facteurs, et je fais une rotation. La rotation, là, on le voit rarement dans les paquets d'enfants francophones, en tous les cas. Et de nouveau, dans la sortie standard, j'affiche les résultats de l'ACP. Je fais OK. Très bien. Une fois que c'est fait ça, j'exécute. Et il me donne la sortie standard, le print de mon ACP qui est issu de Psyche. Donc\"), ('150054', \", le print de mon ACP qui est issu de Psyche. Donc ici, on a les différentes informations. Très bien. Ensuite, on a d'autres informations. Et on voit que les deux premiers facteurs regroupent 79% de l'information disponible. OK. Voilà. Bon, il y a d'autres informations qui sont propres à psych. Ce n'est pas l'enjeu ici. Ce n'est pas l'enjeu ici. Et l'autre enjeu, peut-être, c'est la corrélation des variables avec les facteurs qui sont là. OK. Ça, c'est un premier élément. Donc, ça y est, j'ai fa\"), ('150055', \" c'est un premier élément. Donc, ça y est, j'ai fait mon ACP. Et en sortie, là, on a un rectangle, cette fois-ci, qui est gris. Qu'est-ce qu'il y a dans le rectangle gris? Il me dit, c'est un workup. workspace qui contient le modèle. Notamment. Mais il y a aussi autre chose, en fait. En tous les cas, c'est un espace mémoire de R dans lequel on a les objets qu'on a créés. Dont le modèle. Le modèle de la CP. Mais il y a aussi d'autres informations, peut-être. On va essayer de voir ça. Donc, c'est \"), ('150056', \" peut-être. On va essayer de voir ça. Donc, c'est ce que je fais ici. Et notamment, je vais récupérer, je veux afficher les boulies des valeurs propres. En utilisant du code R. Donc, c'est ce que je fais ici. Pour ça, j'ai besoin d'un outil qui est un RView. Regardons RView. Qu'est-ce qu'il dit? Le RView, workspace cette fois-ci. On a vu, là, c'est un RView table, qui prend en entrée une table. Là, c'est un RView qui prend en entrée un espace mémoire de R. Je vais le placer pour voir. Ah oui, ef\"), ('150057', \"oire de R. Je vais le placer pour voir. Ah oui, effectivement, de nouveau ici, on a des ports qui sont compatibles. Port de sortie, là, un rectangle gris. port d'entrée qui est un rectangle gris. Donc, ils sont compatibles. Très bien. Alors là, ici, je vais mettre ACP. Donc là, j'aimerais afficher, par exemple, les bollies des valeurs propres. Bully, script plot, si vous voulez. B, V, P. Allons-y. Oui, non, j'ai rien. Pourquoi j'ai rien? Parce que je n'ai pas fait la connexion. Je fais d'abord l\"), ('150058', \"e je n'ai pas fait la connexion. Je fais d'abord la connexion. Voilà, je mets par là parce que ça commence à vite. Voilà. Bon. clairement, Naïm, il faut avoir un grand écran avec une bonne définition d'écran parce que vous remplissez, alors il y a un zoom on peut mettre un zoom si vous voulez mais c'est vite fait de tout remplir donc il faut bien organiser son travail alors cette fois-ci, j'ai fait la connexion j'ouvre encore une fois et voilà, regardez en information j'ai l'espace mémoire issu \"), ('150059', \"egardez en information j'ai l'espace mémoire issu du premier calcul sous R avec l'ACP du coup, j'ai les données voilà, ça c'est les données initiales ça les donne avec les variables actives. Et notamment, j'ai l'objet fcp. Du coup, je vais avec l'objet fcp, je peux avoir les valeurs propres. Et donc, je veux faire le graphique des valeurs propres en utilisant R. C'est ce que je fais ici. Là, tout simplement. Boum. Vous avez vu, hein? Voilà. Ébouler des valeurs propres en utilisant le plot et en \"), ('150060', \"er des valeurs propres en utilisant le plot et en utilisant la propriété values de mon objet fcp. Donc, j'ai fait un attribut d'abord dans R pour vérifier et j'ai vu qu'il y avait value, j'ai vérifié, c'est des valeurs propres. Donc une fois que c'est bon là, très bien, je peux faire un calcul. Et on a bien ici le script plot. Alors bien sûr également, vous voyez qu'en sortie c'est un rectangle vert, donc c'est une image, j'aurais pu faire une image view ou une image d'exportation. Par exemple s\"), ('150061', \"age view ou une image d'exportation. Par exemple si je veux créer un rapport, des choses comme ça. Mais bon, je ne le fais pas ici. Donc ce qui est intéressant, regardez bien, c'est qu'en entrée, j'ai un rectangle le gris, et ça veut dire que j'ai l'espace mémoire de la procédure R qui est là, de la session R qui est là. C'est comme ça que j'ai récupéré les données que j'avais créées, les data frames que j'avais créés, en plus du data frame d'entrée ici, et surtout j'ai récupéré l'objet ACP, qui\"), ('150062', \"rée ici, et surtout j'ai récupéré l'objet ACP, qui contient notamment les valeurs propres. Vous avez bien vu, qui est là ici. Très bien. C'est génial, c'est génial. On peut dire tout ce qu'on veut en fait. Alors notamment, si je veux faire par exemple le cercle des corrélations. De nouveau, je fais un AirView. Donc là, je vais faire cercle des corrélations. Et je refais la connexion. Je mets ça un peu ici, parce que sinon c'est vite... Voilà. Alors, bon, je vais regarder avec... Si il n'y a pas \"), ('150063', \"rs, bon, je vais regarder avec... Si il n'y a pas le cercle des corrélations, attends, je te le fais le programme. Donc j'ai préparé en amont tout simplement. Voilà. Très bien. Donc, regardons vite le code. En entrée toujours, là, c'est un objet grisé, c'est un rectangle carré, c'est un rectangle gris. Donc, en sortie ici, c'est l'espace mémoire de la session R où j'ai fait la CP qui vient d'entrer ici. Et donc, de nouveau, j'ai les différentes informations qui sont dans l'espace mémoire de R. E\"), ('150064', \"nformations qui sont dans l'espace mémoire de R. En plus, les données en entrée initiale. Donc, il y a le data frame intermédiaire que j'ai créé, ensuite le data frame avec les variables actives.  et l'ACP que j'ai créé avec Psi. Donc, je récupère les deux colonnes de corrélation et ensuite, je dessine le cercle des corrélations. Voilà, je mets le code ici, vite fait, là. Encore une fois, ce n'est pas l'enjeu. L'enjeu, c'est de faire les sessions R sous Name. Mais bon, donc là, j'ai fixé les val\"), ('150065', \" R sous Name. Mais bon, donc là, j'ai fixé les valeurs limites. Moi, c'est un plus un, c'est des corrélations. On est d'accord, c'est un, c'est deux, c'est les vecteurs des corrélations. J'ai mis un titre. Je me suis assuré que le graphique soit bien carré et non pas rectangulaire. Il faut absolument respecter les ratios. Et ensuite, j'ai mis les axes et j'ai fait un texte pour avoir les noms des variables. Alors, j'ai rajouté un petit jiter, parce que sinon, les variables se superposaient. Du c\"), ('150066', \"ce que sinon, les variables se superposaient. Du coup, ça rendait le graphique très peu lisible. Donc, j'ai rajouté un petit jiter, simplement. Et là, j'ai récupéré les noms des variables. Des variables actives, donc modèle n'entre pas en ligne de compte ici. Oui, d'accord. Voilà, et j'ai rajouté des flèches pour faire joli, genre je sais faire des flèches sous air. Allons-y! Je lance ça, et voilà, on va avoir le fameux cercle des corrélations. C'est pas mal, hein? Je suis très content de moi. A\"), ('150067', \"'est pas mal, hein? Je suis très content de moi. Alors, bien sûr, on peut faire ça avec d'autres paquets, si vous voulez l'utiliser. Admettons que vous n'avez pas besoin de faire une rotation varie max. Là, je vous conseille d'utiliser FactoManner, ça me paraît plus approprié pour le coup. Ou bien DE4 aussi, qui est pas mal également. Voilà, très bien. donc c'est à vous de choisir l'outil le plus adapté par rapport à ce que vous voulez faire ça, ça a toujours été mon discours donc moi je voulais\"), ('150068', \"ça a toujours été mon discours donc moi je voulais faire une ACP avec une rotation varimax j'ai brisé le psy mais si je n'avais pas besoin de faire cette rotation varimax j'aurais pu prendre d'autres packages notamment ceux qui font référence en France FactMiner, ADE4 et d'autres également, pour ne pas faire de jaloux CA est pas mal également enfin il y en a quand même pas mal des packages voilà alors, dernier élément parce que je vais faire un cluster 1 là-dessus, je veux faire une projection d\"), ('150069', \"luster 1 là-dessus, je veux faire une projection des individus dans l'espace factoriel. De nouveau, je fais un RView. Et je vais utiliser les scores calculés via la CP. Je fais cette connexion-là. Voilà. Donc là, les individus ont vu du... Et je mets le code. Encore une fois, je l'ai préparé. Très bien, donc je récupère les coordonnées factuelles des individus de l'objet ACP qui est arrivé en entrée ici, voilà, mon ACP qui est là, qui était dans le workspace, dans l'espace mémoire de R, ensuite \"), ('150070', \"le workspace, dans l'espace mémoire de R, ensuite je fais le plot, et dans le plot, j'utilise, c'est très important cette fois-ci, la colonne modèle, qui est là, qui est dans le data frame D, qui est dans le name in aussi, j'aurais pu l'utiliser, mais il est dans D, que j'avais créé, qui est une copie de Naiming, qui est dans l'espace mémoire. Donc, dans l'espace mémoire de R, on a les objets calculés, on a aussi les data frames, tous les objets qu'on a créés, en fait. Boum! Et je lance. Et voil\"), ('150071', \"qu'on a créés, en fait. Boum! Et je lance. Et voilà. Bon, pas besoin d'être prix Nobel pour comprendre, là, on a plutôt les grosses voitures, la Chrysler 300, mon Dieu, très bien, là, on a plutôt les petites voitures, Twingo, Yaris, très bien. Ensuite, il y a un deuxième élément, ça c'est le premier facteur, petite voiture grandes voitures, et sur le deuxième facteur, les voitures plutôt hautes, voilà, on a Mercedes classe A, les petits monospaces plutôt hauts, si vous voulez, et là, on a les vo\"), ('150072', \"s plutôt hauts, si vous voulez, et là, on a les voitures plutôt basses, Mazda et X8, Mazda et X8, mon dieu, ou bien la Mégane CC, juste comme ça. Voilà, très bien, bon, je ne vais pas faire des analyses ici. Mais ce qui est intéressant, c'est que du coup, on a bien les coordonnées factorielles des individus dans l'espace factoriel. Alors du coup, du coup, j'ai fait un clustering en utilisant ces deux dimensions-là. Normalement, si je fais un clustering, là, On va avoir, si je fais quatre groupes\"), ('150073', \"tering, là, On va avoir, si je fais quatre groupes, on aurait ce groupe-là, des grosses voitures, des petites voitures, des voitures intermédiaires, familiales, et des voitures petits monospaces, haut, ici. Celui-là, il est plutôt inclassable, mais bon. Camis ne sait pas gérer les données atypiques, contrairement à d'autres méthodes de clustering. Mais bon, on va voir ce que ça donne. Alors, qu'est-ce qui se passe? Je veux lancer une session R, mais en entrée, maintenant, ce n'est plus une table\"), ('150074', \"ais en entrée, maintenant, ce n'est plus une table. C'est un workspace qui contient l'objet ACP. Vous êtes d'accord là-dessus? Et en sortie, je veux avoir un espace mémoire R, encore une fois, pour pouvoir faire les graphiques ainsi de suite. Alors, j'ai regardé un peu. J'ai passé le temps à lire la doc. Vous n'imaginez pas le temps que j'ai passé à lire la doc. Donc ça, j'ai passé vraiment en revue ce qu'il y a fait là. J'ai lu en détail ici. Très bien. Une fois que j'ai lu en détail ici, j'ai \"), ('150075', \"ès bien. Une fois que j'ai lu en détail ici, j'ai vu qu'il y a une outil R2R. Le R2R, qu'est-ce qui se passe? Il prend en entrée un espace mémoire R et il renvoie en sortie un autre espace mémoire R dans lequel vous pouvez mettre des objets additionnels. Alléluia! C'est exactement ce que je veux. Donc ici, en entrée, je veux un espace mémoire qui contient notamment de l'objet ACP et en sortie, je veux avoir un espace mémoire qui contient l'objet ACP toujours et un objet supplémentaire, c'est l'o\"), ('150076', \"ACP toujours et un objet supplémentaire, c'est l'objet Kamins. ce qui me permettra de faire le déploiement sur les individus supplémentaires. Allons-y. Tout ça, j'ai lu là, on est d'accord. Allons-y. Une fois que j'ai fait ça, alors... Je vais mettre ça un peu là, parce que ça commence à prendre beaucoup de place, tout ça, mon Dieu. Je vais mettre ça par là. Voilà, voilà. R to R. Très bien. J'ouvre ici. Qu'est-ce qui se passe? Ce qui se passe, c'est que du coup, j'ai bien en entrée les éléments \"), ('150077', \"est que du coup, j'ai bien en entrée les éléments qui étaient dans l'espace mémoire de R ici, de la session R ici. Qu'est-ce que je vais faire? Moi, je vais faire les camins en utilisant les scores de l'ACP, les coordonnées facturées des individus, en utilisant le paquet qui permet de faire le prédicte par la suite. C'est pour ça que je n'utilise pas la fonction standard camins. Alors, tout ça également, j'ai récupéré là. Et je mets le code. Donc, je prends la librairie que j'ai déjà installée s\"), ('150078', \", je prends la librairie que j'ai déjà installée sous Air, sinon ça ne va pas marcher, on est bien d'accord là-dessus. Une fois que j'ai fait ça, je lance les camins sur les coordonnées factorielles et je veux quatre groupes. C'est ce que je mets ici. Ensuite, j'affiche les effectifs par groupe dans la sortie standard. Allons-y. Très bien. il fait ses calculs, là, ça va arriver rapidement, et voilà. Donc, il a bien fait les quatre groupes, dans le premier groupe, il y a dix individus, dans le de\"), ('150079', \"e premier groupe, il y a dix individus, dans le deuxième, il y a 8, dans le troisième, il y a 5, dans le quatrième, il y a 5. Une fois que c'est bon, ça, alors, très bien. Donc, normalement, à la sortie de l'objet qui est là, j'ai, un, l'objet ACP, avec les coordonnées factorielles, et de l'objet K-means avec le cluster d'appartenance des individus. Parce que c'est bien ce qu'on a eu là, regardez bien. C'est la propriété cluster de l'objet K-means qui m'a permis de faire le tableau ici. On est b\"), ('150080', \"s qui m'a permis de faire le tableau ici. On est bien d'accord là-dessus. Du coup, alors, je peux faire un graphique où je reprends la représentation factorielle des individus en les coloriant selon le groupe d'appartenance. Donc là, j'ai mis Camins. Boum! Allons-y alors, je vais faire de nouveau un AirView Workspace. Voilà. Je le renomme Roge avec Cluster. Voilà, et je mets ici. Alors, regardons. on voit bien ce qui se passe c'est quand même pas mal on a bien l'objet ACP et on a maintenant un n\"), ('150081', \" mal on a bien l'objet ACP et on a maintenant un nouvel objet c'est l'objet KM que j'ai créé précédemment vous avez vu là ici j'ai créé l'objet mon KM et donc il est dans l'espace de R maintenant l'objet mon KM que j'ai créé ici et c'est bien ce qu'on voit ici là donc il y a l'objet ACP qui a été créé précédemment et il y a un objet supplémentaire sur l'objet KM. Du coup, je peux faire la projection des individus en illustrant les points, en coloriant les points, selon le groupe d'appartenance. \"), ('150082', \"riant les points, selon le groupe d'appartenance. C'est ce que je fais ici. De nouveau, regardez bien. Voilà, regardez bien. Là, avec l'objet ACP, je récupère les coordonnées factorielles. Ensuite, ici, avec l'objet Cluster, avec l'objet MonKM, qui est mon clustering, en fait, issu du K-Means, j'ai la propriété cluster, qui me donne le cluster d'appartenance des individus. Du coup, je peux bien faire le plot, et en mettant ici le texte avec le modèle, et illustré par les couleurs. Voilà, très bi\"), ('150083', \"dèle, et illustré par les couleurs. Voilà, très bien. Zoom, on aura le graphique maintenant. Alors, je fais un petit commentaire rapide là. Moi, quand je fais une projection factorielle, je m'astreins toujours à mettre les mêmes étendues en abscisse et en ordonnée. Pourquoi? Parce que sinon, les proximités sont trompeuses. C'est pour ça qu'on vous dit tout le temps, ça je le dis aux étudiants, les étudiants me disent, il faut mettre les pourcentages ici. Les pourcentages de variation, en fait, d\"), ('150084', \"ges ici. Les pourcentages de variation, en fait, d'étendue, si vous voulez. Donc la variance restituée. mais ça veut dire que il faut que quand vous regardez le graphique il faut tenir compte du fait que là par exemple c'est 60% et que là je n'ai que 20%  Là, en fait, c'est plus étendu que ici. L'abstice est plus étendu. C'est impossible. C'est impossible. Je veux bien qu'on regarde un graphique et qu'en même temps, dans la tête, on s'arrange pour dire « Mais là, c'est plus étendu que là, en fai\"), ('150085', \"r dire « Mais là, c'est plus étendu que là, en fait. » C'est ingérable. Donc, moi, ce que j'ai dit aux étudiants, c'est que quand vous faites une ACP, vous avez intérêt, en réalité, à mettre les mêmes étendues en abscisse en ordonnée et en mettant un graphique avec un graphique carré. comme ça, visuellement, vous avez exactement une idée des proximités. Sans avoir à tenir compte du fait que l'étendue est plus élevée en abscisse qu'en ordonnée parce que la vaillance restituée est plus élevée en a\"), ('150086', \"ce que la vaillance restituée est plus élevée en abscisse qu'en ordonnée. Et voilà. Donc on voit bien ici, on voit bien que la modus est prédémise. Et que là, on a vraiment la perception, on a une perception de la réalité des choses, ici. Sans avoir besoin des pourcentages là et là. ça c'est vraiment un élément important je sais bien quand on vous dit tout le temps les pourcentages, oui c'est vrai mais ça force un exercice supplémentaire qui n'est vraiment pas très accessible, c'est pour ça que \"), ('150087', \"t vraiment pas très accessible, c'est pour ça que moi je préfère le faire différemment c'est ce que je montre dans le code ici voilà, dans mon code qui est là, regardez bien je dis bien que l'étendue en X et en Y sont les mêmes donc il faut vérifier en amont simplement et je mets aspect égal à 1 pour que le ratio abscisse ordonnée soit bien le même, pour avoir un graphique qui soit carré. Et à partir de là, les proximités dans le graphique sont les vraies proximités restituées par le plan factor\"), ('150088', \"es vraies proximités restituées par le plan factoriel, sans avoir à s'embêter sur les pourcentages de variation restitués par les axes. Ok. Bon, je reviens ici sur mon autre affaire. On voit bien ici. On voit bien. Oui. grosses voitures en bleu, petites voitures en vert voilà les voitures familiales ici ils ne savaient pas où le mettre donc il l'a mis avec eux les voitures plutôt intermédiaires et les petits monospaces sont en haut ici, alors Velsati ce n'est pas un petit monospace mais elle est\"), ('150089', \"sati ce n'est pas un petit monospace mais elle est autre quand même ceci étant donc c'est pour ça qu'elle est là, on est bien d'accord là-dessus bien une fois que j'ai ça alors j'aimerais positionner les individus donc supplémentaires et dans l'espace factoriel, et les rattacher à leur groupe d'appartenance. Voilà ici. Rappelez-vous, dans mon fichier de données, il y a des individus supplémentaires, c'est les voitures Peugeot. Je vous montre ici. Du coup, la 407, on la rattache à qui? La 307 CC,\"), ('150090', \" Du coup, la 407, on la rattache à qui? La 307 CC, on la rattache à quel groupe? La 1007. C'est quoi la 1007? À quel groupe de voitures on pourrait le rattacher? Ou la 607 d'ailleurs. Là également, on connaît un peu les voitures, on va voir si les données confirment ce qu'on pense des voitures. Donc déjà, première étape, il faut que je charge les données supplémentaires. On est d'accord? Les individus illustratifs. C'est ce que je vais faire maintenant. Donc, de nouveau, je remonte et en I.O. Je\"), ('150091', \"tenant. Donc, de nouveau, je remonte et en I.O. Je mets Excel Reader et je vais chercher le deuxième jeu de données. On configure, je vais là, sur le bureau, et je prends des mots, et voilà. Et cette fois-ci, je prends les individus illustratifs, les quatre voitures Peugeot. Voilà. Qu'est-ce qui se passe? J'ai besoin de deux étapes. La première étape, c'est que, un, je dois faire une prédiction, entre guillemets prédiction, sur les individus supplémentaires. D'une part, une projection dans l'esp\"), ('150092', \"lémentaires. D'une part, une projection dans l'espace factoriel, et d'autre part, un rattachement à un cluster. Voilà, donc la prédiction entre guillemets ici, c'est deux étapes. Une projection des individus supplémentaires dans le repère factoriel, et ensuite, dans ce repère-là, c'est rattaché en calculant la distance au barisson, tout simplement, des individus supplémentaires. Alors, dans R, il y a un outil intéressant, c'est l'outil prédictor. Donc, l'outil prédictor, on va voir ce qu'il fait\"), ('150093', \" Donc, l'outil prédictor, on va voir ce qu'il fait. il prend en entrée deux éléments. Il prend en entrée un data table, un jeu de données avec la flèche ici, un data input. On voit les flèches qui sont compatibles. Ça déjà, je peux le faire. Et le deuxième élément, c'est qu'il prend en entrée un workspace de R, un espace mémoire de R qui contient des objets. Là également, il y a un rectangle gris en entrée. Il paraît compatible avec la sortie grise ici de R2R, qui combine l'ACP et le clustering.\"), ('150094', \"se ici de R2R, qui combine l'ACP et le clustering. Je le mets ici. Non, mais franchement, Naim, c'est vraiment pas mal. Naim, c'est vraiment pas mal. J'ai fait beaucoup de projets dans ma vie, mais s'il y a un projet auquel j'aurais aimé participer, c'est bien Naim, parce que je trouve vraiment intéressant la conception qu'on est faite. C'est génial. Alors bon, il faut que je charge les données, sinon on ne les verra pas. Une fois que je l'ai là, je peux ici coder. Et donc, le codage, c'est quoi\"), ('150095', \" je peux ici coder. Et donc, le codage, c'est quoi? c'est qu'on va prendre dans l'entrée ces données-là, on va faire un prédict sur l'ACP d'abord, pour protéger des individus, et ensuite faire un prédict sur les K-Mines pour attacher au groupe. C'est ce que je fais ici. On va voir. Alors, regardons ce qu'on a en entrée. On a l'objet ACP, l'objet K-Mines, et on a le name-in, qui est le data frame qui vient des individus supplémentaires cette fois-ci. C'est ça, hein? Donc le name in, qui est là, e\"), ('150096', \"ci. C'est ça, hein? Donc le name in, qui est là, en réalité, ça représente les données qui sont entrées ici, cette fois-ci. Et non pas les données des individus actifs. Et dans l'espace mémoire, il y a les individus actifs que j'ai copiés dans un autre data frame pour qu'il n'y ait pas un conflit de noms, et les objets ACP et Kamiens. Allons-y alors, on va pouvoir le faire. Je mets le code et j'explique. Voilà, donc je récupère les individus supplémentaires que je mets dans un data-prime que j'a\"), ('150097', \"lémentaires que je mets dans un data-prime que j'appelle Dbis. Ensuite, dans Dbis, il y a toutes les colonnes, y compris modèle. J'évacue la colonne modèle qui ne sert pas au calcul. Ensuite, je fais un prédicte. C'est pour ça que j'ai utilisé Psyche. Avec Psyche, je fais un prédicte. Voilà, pour la projection des individus dans l'espace factoriel. Voilà. Et ensuite, je fais également un prédicte. à partir des coordonnées factorielles en utilisant les camises. Je rattache chaque individu supplém\"), ('150098', \"t les camises. Je rattache chaque individu supplémentaire son cluster d'appartenance en calculant les distances au bariceintre. J'aurais pu le programmer ici, mais comme il y a un package F2MID qui le fait très bien, prenons plutôt ce package-là. On est d'accord là-dessus. Du coup, maintenant, j'ai un data frame dans lequel je vais rajouter les coordonnées des individus, les modèles des véhicules, 607, P607, P307CC, P1007, ainsi de suite, P407, Peugeot, ainsi de suite, et je rattache également l\"), ('150099', \"eugeot, ainsi de suite, et je rattache également le question d'appartenance. Alors, on peut faire afficher dans la sortie AllFooter, voilà, print. Je pourrais, mais je ne vais pas le faire. Voilà. Non, ça nous regarde. Bien, une fois que c'est fait ça, voilà, exécutons. donc il charge bien les données en entrée ici et il fait les deux étapes que j'ai mis ici bon j'ai rien mis, j'ai pas mis de print, c'est pour ça qu'il n'y a rien ici on est d'accord là dessus en revanche en sortie là maintenant \"), ('150100', \"ord là dessus en revanche en sortie là maintenant j'ai une table avec le data frame que j'ai créé là name root donc name root cette fois-ci c'est une variable prédéfinie qui symbolise la flèche de sortie qui dit c'est une information sous forme de table donc nameMoot est une variable standard comme nameIn ici et nameMoot ça représente cette flèche là dataOutput donc ça y est dans nameMoot maintenant j'ai mis cette information là on peut le visualiser regardez ici, dataOutput et voilà bien les do\"), ('150101', \"iser regardez ici, dataOutput et voilà bien les données qui ont sorti on a quoi? on a les coordonnées factorielles des 4 voitures supplémentaires les noms des modèles correspondants et les clusters d'appartenance. Donc je vois que la 407 a été rattachée au cluster 1, la 307 CC, la Peugeot 1007 a été rattachée au cluster 4 et la 607 au cluster 3. Qu'est-ce qu'il me faut alors? Ce qu'il me faudrait maintenant, c'est faire un graphique où je peux faire un graphique déjà ici. À la sortie de la data \"), ('150102', \"ire un graphique déjà ici. À la sortie de la data table, je peux faire un graphique pour projeter les individus supplémentaires. Mais faire un repère où on voit que les individus supplémentaires ce n'est pas très parlant. Ce qui est intéressant, c'est mettre les individus actifs, un peu en grisé, et mettre au-dessus les individus supplémentaires pour voir les rattachements associés. Ce serait pas mal, ça. Or, qu'est-ce qui se passe? C'est que là, j'ai un data frame avec uniquement les individus \"), ('150103', \" j'ai un data frame avec uniquement les individus supplémentaires. Ah ben du coup, c'est compliqué, là. Comment je peux faire, alors? Alors, une des possibilités, j'ai essayé de faire ça, mais ce n'est pas une bonne idée. une des possibilités, c'était de dire je récupère ici les données actives et je vais le concatener dans un data frame avec les données illustratives. Comme ça, je peux faire le graphique à côté et rajouter une colonne qui permet de savoir qui est actif, qui est illustratif. Qua\"), ('150104', \" de savoir qui est actif, qui est illustratif. Quand j'ai commencé à le faire, c'est facile à faire, mais quand j'ai commencé à le faire, je me suis rendu compte que ça allait être très difficile à expliquer. Et non, non, il faut absolument que ce soit lisible. Du coup, je suis parti sur une autre solution.  Et ça me permet de montrer un autre outil. Qu'est-ce qui se passe? Là, à la sortie, j'ai des individus supplémentaires, sous forme de table. Et là, à la sortie, ici, j'ai un workspace avec l\"), ('150105', \" Et là, à la sortie, ici, j'ai un workspace avec les individus actifs et les coordonnées factorielles via l'objet ACP et les groupes d'appartenance via l'objet mon KM. Donc, du coup, ce qui serait bien, c'est que je connecte ces deux-là pour pouvoir l'exploiter. Il y a un outil sous ça, c'est addTableToL. je vais rajouter dans r voilà dans l'espace mémoire de r là donc là c'est voilà sup voilà là je vais connecter la table des individus supplémentaires en sortie ici avec l'espace mémoire dans le\"), ('150106', \"taires en sortie ici avec l'espace mémoire dans lequel on a les objets voilà acp et l'objet camins des données actives Et comme ça, on va pouvoir faire ce fameux graphique qui permet de visualiser ces différentes informations-là. Allons-y. Qu'est-ce que je mets ici alors? Donc, en entrée ici, regardez, on a l'espace mémoire de R issu de là, et la nouvelle table que j'ai créée ici, qui correspond aux individus supplémentaires avec leurs coordonnées factorielles et les clusters d'appartenance. Don\"), ('150107', \"s factorielles et les clusters d'appartenance. Donc là, qu'est-ce que je vais faire? Je vais combiner tout ça. alors là on a les objets de l'espace mémoire et là le name in c'est les individus supplémentaires avec leurs coordonnées factorielles du coup voilà regardons ce qui se passe je récupère name in que je mets dans un data frame spécifique voilà et j'affiche ce qu'il y a dans la mémoire maintenant bien vérifier que tout ça est bien visible donc allons-y Qu'est-ce qu'il y a dans mon LS? Il y\"), ('150108', \"onc allons-y Qu'est-ce qu'il y a dans mon LS? Il y a les différents objets. On a les données actives, on a l'objet KM, l'objet ACP et l'objet KM, et surtout j'ai le data frame avec la table supplémentaire qui sont maintenant dans le même espace. donc je vais pouvoir faire le graphique combinant les informations de l'ACP et de K-Means sur les individus actifs et la projection des individus supplémentaires l'idée tout simplement ici c'était de ramener les données qui sont issues de ce calcul là la\"), ('150109', \"ner les données qui sont issues de ce calcul là la table qui est là, dans le même espace mémoire que là où j'avais fait mes calculs donc maintenant je peux mettre un AirView pour faire mon graphique dans mon graphique qu'est-ce qui se passe je vais élargir ça un peu et je récupère le code voilà tac, tac, tac du coup dans le code qu'est-ce qui se passe tout simplement je vais agrandir on voit bien ce que je suis en train de faire avec l'objet ACP du premier espace mémoire Voilà. Je récupère les c\"), ('150110', \"du premier espace mémoire Voilà. Je récupère les coordonnées factorielles. Ensuite, je les projette dans l'espace factoriel. On les colle. Voilà. Enfin, je prépare ici, en fait. C'est plus ça, en fait. Voilà. Et je mets une couleur un peu différente. J'aime bien les couleurs différentes. Ici, plutôt pastel, pour le coup. Voilà, moins flashy. Voilà. Avec leur couleur d'appartenance. Donc, ça, c'est les individus actifs. Voilà. Et pour les individus illustratifs, avec le deuxième data frame qui es\"), ('150111', \"s illustratifs, avec le deuxième data frame qui est dans l'espace mémoire des individus illustratifs. Je le mets cette fois-ci, et j'affiche le modèle, et je mets une couleur plus forte pour qu'on les voit en surimpression. C'est ça l'idée. Cette étape-là est essentielle. Sinon, je ne pouvais pas connecter ces deux informations-là, les données actives et les données illustratives. Il faut dire que je puisse les connecter. C'est ce que j'ai fait ici. Il y a d'autres manières de faire. Je suis abs\"), ('150112', \"ci. Il y a d'autres manières de faire. Je suis absolument d'accord sur le fait qu'il y a d'autres manières de faire. Mais bon, moi j'essaie de faire comme ceci pour pouvoir utiliser ce nouveau composant-là. C'est ça l'idée en fait. Voilà, faire une forme de jointure si vous voulez. Voilà, et du coup maintenant je peux faire projection avec AND SUP. Allons-y, c'est ce que je fais ici, exécute un OpenViews. Très bien. Et on a bien, voilà. Et on voit bien les rattachements. Voilà, donc on avait les\"), ('150113', \"t bien les rattachements. Voilà, donc on avait les différents groupes, la Peugeot 1007 a été rattachée au groupe des petits monospace. Oui, effectivement, la Peugeot 1007 est un petit monospace. Voilà. Ensuite, la 607 a été rattachée au groupe des grosses voitures. Berlin, oui, elle a la même classe que la Madesse classe E, que l'Audi A8. Donc, sa place est tout à fait légitime, si vous voulez. Ensuite, on a les voitures familiales intermédiaires. La Peugeot 407 vient proche de la Vectra. Ce n'e\"), ('150114', \". La Peugeot 407 vient proche de la Vectra. Ce n'est absolument pas étonnant. Alors, 307cc, oui, on peut se poser des questions, mais là, on voit la mégane CC, là. Donc, la proximité 307cc, ça paraît tout à fait légitime, effectivement. Donc, on voit bien le rattachement des individus supplémentaires dans l'espace factoriel avec les individus actifs et les clusters d'appartenance. Et voilà. Alors, qu'est-ce qui se passe? Donc, je reviens en amont ici, je reviens à cette idée-là. On peut égalemen\"), ('150115', \" ici, je reviens à cette idée-là. On peut également, donc ici, j'avais fait une vidéo où je montre qu'on peut intégrer du code Python dans NIME. Pour qu'il n'y ait pas de jaloux, parce que moi, je suis totally agnostique sur les outils. Je ne fais pas de réel, j'ai des outils du tout. Moi, tout simplement, je cherche l'outil le plus adapté par rapport à ce que je veux faire. J'ai souhaité faire une autre vidéo où je montre que c'est possible également de le faire avec R. Très bien. Et on a des o\"), ('150116', \"ement de le faire avec R. Très bien. Et on a des outils, on a une combinaison d'outils qui est assez intéressante. Et pour montrer ça, justement, j'ai montré, j'ai fait ce petit processus-là, qui paraît un peu complexe, mais en réalité, qui est extrêmement lisible. C'est ça l'intérêt de NIME. C'est que les opérations qu'on fait, que nous réalisons, sont très lisibles. Il suffit de bien organiser simplement ces composants et comprendre le rôle des ports dans les connexions, les signes qui sont tr\"), ('150117', \" ports dans les connexions, les signes qui sont très intéressants en termes de qu'est-ce que je dois mettre en connexion avec les différents composants. Bien, alors comme d'habitude, je mettrai en ligne, une fois que j'aurai mis en ligne la vidéo, je mettrai en ligne également le code R qui est là, que j'avais préparé ici, et également l'archive du workflow Sune. Voilà, excellent travail à tous.\"), ('160001', \"Bien, c'est parti. Alors, dans cette vidéo, je vais parler de l'extension Python pour NIME. Si je schématise un peu l'idée, c'est la possibilité d'intégrer du code NIME dans des workflows Python. Du code NIME qui est impacté sur un composant. Et du coup, votre composant fait partie intégrante du workflow sous NIME. Alors, essayons de situer les choses pour que tout le monde puisse bien profiter de cette vidéo.\\n\\nNIME est un logiciel de data science. J'ai mis le site web là, voilà, ici. Mais il es\"), ('160002', \"e. J'ai mis le site web là, voilà, ici. Mais il est plus que ça, en fait. Il a des fonctionnalités ETL, enfin, on peut faire des pipelines de gestion de données. Il est capable de faire du traitement d'images. Il est capable également de traiter des données textuelles. Enfin, c'est un super outil. C'est un super outil que j'apprécie énormément. Voilà.\\n\\nAlors justement, on peut trouver facilement sur mon site des vidéos, j'en ai fait toute une série sur NIME. Ces dernières années, j'utilise cet o\"), ('160003', \"ie sur NIME. Ces dernières années, j'utilise cet outil-là. Ça, c'est ces dernières années, mais quand je faisais encore des tutoriels rédigés à l'époque, sur ce site-là cette fois-ci, quand on fait une recherche par mots-clés, CTRLIFE NIME, il y en a pas mal. Et si je reviens sur les plus anciens, les plus anciens datent de 2008. Je comparais là, l'idée c'est de comparer les performances de différents logiciels que j'étudie beaucoup à l'époque. Il y avait Tanagra bien sûr, dans lequel j'étais tr\"), ('160004', \"l y avait Tanagra bien sûr, dans lequel j'étais très impliqué, oui un petit peu, voilà, mais il y avait aussi NIME, il y avait Orange Machine Learning, Orange Machine Learning, il y avait Air à l'époque, voilà, il y avait RapidMiner, Weka, ainsi de suite. J'ai même écrit des documents plus anciens je pense, là septembre 2008, il me semble que là je parle déjà de NIME justement. Il me semble en 2008, je dois parler de NIME. Là, je présente Tanagra, je présentais Tanagra à l'époque, mais je compar\"), ('160005', \", je présentais Tanagra à l'époque, mais je comparais justement avec NIME. Les autres, Orange, Machine Learning et NIME. C'est un outil qui m'intéresse beaucoup et que j'utilise moi-même d'ailleurs dans une partie de enseignements notamment en master 2, master 2 data science, master 2 6, en début d'année, le niveau des étudiants en codage que ce soit sous Python ou sous R est encore un peu hétérogène et il faut quand même que j'avance par ailleurs, je ne peux pas attendre, voilà et justement pou\"), ('160006', \"s, je ne peux pas attendre, voilà et justement pour amener tout le monde sur le même registre, les remises à niveau, les méthodes de machine learning, que ce soit le supervisé ou le non supervisé, je le fais sous 9, voilà les deux premières semaines. À partir de la troisième semaine, ils ont un excellent niveau que ce soit sous Python ou sous R, la question ne se pose plus généralement je travaille avec ces outils-là parce que je vois un autre avantage c'est qu'il faut que les étudiants sachent \"), ('160007', \"antage c'est qu'il faut que les étudiants sachent coder un bon codeur ne fait pas un bon data scientist mais on ne peut pas être un bon data scientist si on ne sait pas coder, très bien coder cette nuance est très importante je reviens à faire je suis allé sur ce site-là l'idée c'est de rajouter des composants dans lesquels on va intégrer du code Python dans nos workflows c'est ça l'idée j'ai regardé la documentation en ligne j'ai trouvé celui-là celui-là qui explique un peu ce qui se passe je l\"), ('160008', \" celui-là qui explique un peu ce qui se passe je l'ai lu attentivement ensuite il faut des exemples bien sûr parce que sinon on ne comprend pas c'est très joli de lire mais bon, s'il n'y a pas d'exemple on ne peut pas et j'ai trouvé un exemple qui est celui-ci. Voilà. J'ai trouvé un exemple qui est celui-ci, mais ils ne sont pas d'ISER les gars. Ils te montent le workflow, ce qu'on vient obtenir au final, mais quand même, il y a des choses qu'on a un peu du mal à saisir. On a un peu du mal à sai\"), ('160009', \"a un peu du mal à saisir. On a un peu du mal à saisir, donc du coup, je me suis dit, tiens, je vais refaire la crame, en la détaillant pas à pas, et l'idée, c'est que comme ça, ce sera reproductible. Que chacun puisse comprendre ce qui se passe à chacune des étapes. C'est ça le plus important. Le faire une fois, c'est bien, mais trouver, pouvoir le schématiser pour que la démarche soit reproductible, c'est un niveau différent. Mais bon, quand même, rendons à César ce qui a pertinent à César. Je \"), ('160010', \"e, rendons à César ce qui a pertinent à César. Je l'aurais eu beaucoup de mal si je n'avais pas trouvé cet exemple-là. Essayer de comprendre, récupérer, on peut le récupérer. Je vais par là, on doit le récupérer par là. et ensuite l'étudier en détail. Après, décomposer, ça c'est le travail que j'ai dû faire, pour comprendre exactement. Bien, alors allons-y. Qu'est-ce qu'il me reste à dire? Ah, ce qu'il me reste à dire également, c'est comment ça se passe dans NIME pour pouvoir avoir ces fonction\"), ('160011', \"se passe dans NIME pour pouvoir avoir ces fonctionnalités Python. Donc là, je vais réduire ça, et j'ouvre NIME. Voilà, donc j'ai ouvert NIME, là, pour l'instant, il n'y a rien. Très bien. Et déjà, il y a une première étape, c'est qu'il faut importer le package qui permet d'avoir ces fonctionnalités NIME. Ça, c'est un premier point important. Donc, ces packages-là, vous pouvez les importer tout simplement. Help ici, Install New Software, voilà. Et vous sélectionnez le site. Voilà, donc je vais ré\"), ('160012', \" vous sélectionnez le site. Voilà, donc je vais récupérer tous les sites, moi. Et ensuite, vous cherchez ici des extensions NIME. Donc, il va scanner, voilà. C'est exactement les mêmes mécanismes qu'il y a sous, avec les packages, avec Conda Forge. Moi, j'utilise beaucoup Conda sous Python, ou bien également pour le cran sous R, c'est le même mécanisme ici. C'est vraiment cette idée-là, c'est cette idée-là que j'aurais dû avoir quand je travaille sur Tanagra, parce que ça permet justement de pou\"), ('160013', \" sur Tanagra, parce que ça permet justement de pouvoir rajouter facilement ces outils. Mais bon, c'était une autre histoire. Et voilà, une fois que c'est fait là, très bien, dans les composants, vous avez les composants, donc la liste des composants, je l'ai fait déjà, donc je ne vais pas le faire devant vous, moi j'ai fait en amont, ça prend un peu de temps, il faut avoir une bonne connexion. Voilà, on a des composants Python maintenant. Voilà. Alors, ce qui nous intéresse le plus, c'est Python\"), ('160014', \"Alors, ce qui nous intéresse le plus, c'est PythonScript. C'est celui-là. Alors, j'avais vu qu'il y a aussi des composants qui sont legacy. Alors, déjà, qu'est-ce que c'est que legacy? Donc, je suis allé sur le site de NIME pour comprendre ce que c'est que ça. Ce qui m'intéresse, c'est celui-là, encore une fois. Ce qui est intéressant, je clique sur le composant et vous avez vu, vous avez l'explication de ce qu'il fait. On va aller en détail là-dessus. Je vais revenir en détail là-dessus. Mais b\"), ('160015', \"essus. Je vais revenir en détail là-dessus. Mais bon, en lisant ici, et surtout en regardant les inputs, soit objets, soit tables, et les outputs, j'ai compris énormément de choses. Alors, qu'est-ce que c'est que ces composants Python Legacy? En fait, c'est les composants qui sont dépréciés tout simplement. Alors, pas exactement. Je suis allé chercher sur Internet, simplement, et l'idée c'est justement, pourquoi Legacy? Ça veut dire quoi Legacy? Donc je suis allé voir, très bien, et il explique \"), ('160016', \"Donc je suis allé voir, très bien, et il explique là, en fait. Voilà. L'idée de legacy, tout simplement, c'est ce qu'il explique là, c'est qu'ils seront supprimés à terme. Voilà. Donc surtout, il ne faut surtout pas les utiliser, en fait. C'est ça l'idée, hein. C'est ça l'idée. Très bien, voilà. Donc tout ce qui est legacy, en bas, là, on oublie. On oublie parce qu'ils vont être dépressés, tout simplement. Ils vont être dépressés dans le temps. Ça ne sert à rien de s'énerver là-dessus. Voilà, do\"), ('160017', \"a ne sert à rien de s'énerver là-dessus. Voilà, donc moi je reviens ici. OK, donc c'est bien celui-là que je vais utiliser. OK. Ensuite, la question c'est, quelle version de Python on utilise? Parce que c'est bien beau tout ça. Mais, donc je vais mettre un script Python, mais quelle version de Python je vais utiliser? Et puis, quel package je vais utiliser également? Ça, c'est une vraie question ça. Parce qu'il faut que j'ai un contrôle là-dessus. Alors ça, vous le trouvez ici, file, voilà, réfé\"), ('160018', \". Alors ça, vous le trouvez ici, file, voilà, référence. C'est bien ça. Et dans File Préférences, vous avez NIME et là, vous avez Python. Alors, il y a différentes versions. Il y a la version qui est intégrée Bundled. Je ne sais pas comment dire ça. Bundled, moi, je dis ça. Voilà. C'est la version qui est impactée avec NIME. Voilà. Donc, il dépend de la version de NIME aussi, du coup. Voilà. Mais au moins, pour le coup, on est sûr que ça marche, parce que c'est intégré dans NIME. OK. quelle vers\"), ('160019', \"parce que c'est intégré dans NIME. OK. quelle version est notamment des librairies, ça c'est à voir. Mais ils disent qu'il y a les principaux packages déjà, importants, une sélection de packages. C'est ce qui est dit ici. Vous avez vu, là, ils le disent très bien. Voilà, installation, distanction, voilà. Très bien. Moi, ce qui m'intéresse justement, c'est un, Pandar, et deux, je vais faire du machine learning, on va voir l'apprentissage supervisé, l'analyse predictive, le classement, voilà. Et q\"), ('160020', \", l'analyse predictive, le classement, voilà. Et quelle version de Sikit Learn, du coup, si je veux utiliser Sikit Learn? Très bien. Ça, c'est une première partie, et c'est ce qui se passe ici. Et je me rends compte qu'on peut profiter d'autres environnements, notamment les environnements Panda. Ah! Ça, c'est excellent, ça. Donc, j'ai la possibilité de créer un environnement sur Panda, qu'on a créant, créat, et tout ça, de mettre les packages qui m'intéressent, y compris des packages qui ne sont\"), ('160021', \" m'intéressent, y compris des packages qui ne sont pas des packages usuels, par exemple Yellow Brick, ou je ne sais pas, tous les packages qui me paraissent intéressants, et de pouvoir en profiter ici. Regardez, là, ce sont les packages que j'utilise pour mes TD. J'ai fait un sur la réglation logistique, j'ai fait un sur la qualité des données, un sur les MLops avec MLflow, avec PyTorch également, ils sont disponibles ici. Donc je pourrais les utiliser. Donc ça, c'est une possibilité réellement \"), ('160022', \"iliser. Donc ça, c'est une possibilité réellement intéressante. Mais bon, il faut savoir, il faut déjà, un, installer un Aconda, et deux, savoir manipuler qu'on a pour créer l'environnement avec les outils qui vont. Mais bon, un data scientist qui ne sait pas créer des environnements sous Python, il faut quand même qu'il se pose des questions. Là, pour l'instant, je vais simplifier et je reste bien sur la version du Python qui est intégrée, qui est impactée dans NIME. Bien. Alors, qu'est-ce qui \"), ('160023', \"st impactée dans NIME. Bien. Alors, qu'est-ce qui reste à voir? Je peux commencer la démo. Je regarde un peu ce que je devais dire ici. Je vais faire un schéma classique d'apprentissage supervisé. Je vais essayer de reproduire cette idée-là. Cette idée-là. J'ai un échantillon d'apprentissage. Je vais modéliser là-dessus. je récupère un échantillon test, j'applique le modèle sur l'échantillon test et je mesure les performances. Voilà l'idée, en intégrant les Python Script. Mais en essayant de com\"), ('160024', \"tégrant les Python Script. Mais en essayant de comprendre un peu pourquoi là j'ai une petite flèche noire, pourquoi là j'ai un petit carré bleu, en entrée, en sortie, qu'est-ce que je dois faire avec ça? C'est ça la principale difficulté. Alors, pour travailler, je vais utiliser une base extrêmement connue, c'est la base des breast cancer. Vous connaissez tout le monde, hein? Et j'ai fait une préparation en amont. Alors, comment j'ai fait ma préparation en amont? Hop là, voilà, je les scindées e\"), ('160025', \"aration en amont? Hop là, voilà, je les scindées en deux parties. Donc, j'ai les données en apprentissage ici. Il y en a 399, parce qu'elle est en tête ici. Donc, c'est la partie apprentissage. L'idée, c'est de prédire les cellules bénignes ou malignantes, les tumeurs, donc, voilà, soit cancéreuses, soit non-cancéreuses, à partir des caractéristiques des cellules. Ça, c'est la donnée d'apprentissage. et sur le test, j'ai exactement la même structure, mais c'est d'autres individus, bien sûr. Il y\"), ('160026', \"ure, mais c'est d'autres individus, bien sûr. Il y en a 300 cette fois-ci. Voilà. Donc, c'est des données très classiques que j'utilise énormément dans ma partie tutoriel. Il faut absolument qu'on comprenne à peu près ce qu'on manipule. Mais bon, il ne faut pas non plus que ce soit un frein. Si le problème est trop complexe, les gens perdent le temps là-dessus et ils oublient l'essentiel. L'essentiel, ici, aujourd'hui, c'est de savoir créer un script Python et de l'intégrer dans un workflow de N\"), ('160027', \"ript Python et de l'intégrer dans un workflow de NIME. Très bien. Donc, une fois qu'on a compris les données, on va pouvoir y aller réellement. Alors, je vais créer un groupe de workflow, donc new workflow group, et je fais essai Python, extension Python. Là. Voilà. Ça y est, j'ai mon groupe de workflow ici, et je vais rajouter un workflow. Apprentissage supervisé ou classement. Classement en français est l'équivalent de classification en anglais. Ça, j'ai dit tout le temps. Ou classification su\"), ('160028', \". Ça, j'ai dit tout le temps. Ou classification supervisé, je préfère. Voilà. Mais bon. Ouais. Voilà. Classement, donc classification supervisé. Bien. Il m'a créé mon workflow. Donc, première étape, qu'est-ce qu'il faut? Il faut que je charge des données. On est bien d'accord là-dessus. C'est toujours la même étape. je crée le modèle, voilà, donc il faut que j'abcharge les données, ensuite je modélise, ainsi de suite. Très bien. Sauf que cette fois-ci, je m'utilise avec Python. Allons-y, donc, j\"), ('160029', \"is-ci, je m'utilise avec Python. Allons-y, donc, j'importe les données, et je lis le fichier Excel. Jusque-là, ça va, je réduis ça un peu, parce que ça va prendre un peu de place mon affaire, là. Très bien. Voilà, et j'importe les données. Donc, j'importe les données ici, je le cherche, sur le bureau, il est dans démo, et je charge ici, et je récupère la partie apprentissage. Vous êtes d'accord là-dessus. J'exécute, une fois que c'est exécuté, je peux visualiser les données, bien les données qui\"), ('160030', \" peux visualiser les données, bien les données qui sont là, vous êtes d'accord là-dessus. Voilà le fichier de données, et il fait un typage automatique, donc il reconnaît que là, c'est une chaîne de caractère, et là c'est des colonnes d'entier. Et ça y est, maintenant, je vais intégrer le Python script qui est censé faire l'apprentissage supervisé avec du code Python. Oui, d'accord. Allons-y, du coup, je récupère PythonScript. Voilà. Et hop là. Alors, déjà, premier élément, donc ça y est, j'ai m\"), ('160031', \"lors, déjà, premier élément, donc ça y est, j'ai mis PythonScript, et je vois deux éléments en entrée. Enfin, deux éléments. Un, un élément en entrée qui est une table, censé être une table, et là, un élément en sortie qui est censé être une table. Quand je vais sur Python Script, vous avez les explications ici, qui vous disent exactement ce qui se passe, par là. Donc là, il explique ce que faire, je l'ai lu en détail. C'est ce que je dis tout le temps à mes étudiants. Une grosse partie de mon t\"), ('160032', \" temps à mes étudiants. Une grosse partie de mon travail, c'est de lire la documentation, ou lire les articles scientifiques d'ailleurs. Je passe énormément de temps à faire ça, pour essayer de voir si c'est intéressant ou pas pour nous, est-ce que je l'intègre dans mes enseignements ou pas, qu'est-ce que je peux en faire, quelle valeur ajoutée ça va amener à mes étudiants. C'est ça la vraie question. Donc là, pour le coup, ici, il fallait que je comprenne bien ce qui se passe. Et il fallait exa\"), ('160033', \" comprenne bien ce qui se passe. Et il fallait exactement que je comprenne ce qu'il y a ici. Donc, il dit, par défaut, il y a un input table. Voilà. Et par défaut, il y a un output table. Alors, OK. Très bien. Du coup, là, c'est des données également. C'est les données que l'on a visualisées tout à l'heure. Je fais la connexion. Voilà. Alors, quand même, regardons le code maintenant. Qu'est-ce qu'il y a dans le code? Je vais dans Configure. Donc, je vais sur Python Script. Voilà. Et je vais dans\"), ('160034', \" je vais sur Python Script. Voilà. Et je vais dans Configure. Je vais dans Configure et on a le code Python. Qu'est-ce qu'il dit? Alors, il y a un package qu'il installe par défaut avec une fonction. Alors, name, input, output. Pas besoin d'être pris Nobel. Voilà, c'est bien name scripting. C'est dans le module name scripting. Voilà, scripting de NIME plutôt. Et dedans, il y a une fonction kneo. Très bien, il y a un objet. Alors, ce kneo-là permet justement de gérer les éléments en entrée et sor\"), ('160035', \"t justement de gérer les éléments en entrée et sortie. Et ce qu'il fait ici, c'est un simple transfert des données d'entrée. Donc, il peut y avoir plusieurs. Et le premier est numéroté 0. Il récupère les données d'entrée et il l'envoie dans le port de sortie. Vous avez vu la petite flèche noire en sortie. Donc, cette partie-là, c'est la partie qui arrive là. Voilà, les données d'entrée, la flèche noire. et ces données de rentée-là sont transférées simplement vers la sortie. C'est la partie qui e\"), ('160036', \"s simplement vers la sortie. C'est la partie qui est là. OK. Donc, si je lance là, bon, il a dû lancer, mais il ne s'est rien passé parce qu'on ne voit rien, en fait. Du coup, qu'est-ce que je vais faire? Je vais essayer de voir si on a bien les mêmes données. Donc, je vais rajouter un visualiseur de données, InteractiveTable. Voilà. Ah oui, qui va être legacy aussi bientôt. Bon, ce n'est pas grave. Voilà. et je le mets ici. Si j'en crois au code qu'on a vu tout à l'heure, au script Python qu'on\"), ('160037', \" qu'on a vu tout à l'heure, au script Python qu'on a vu, il prend les données d'entrée, il l'envoie en sortie et voilà. Il ne fait rien d'autre. Allons-y alors. Exocute un OpenViews. Effectivement, vous avez bien vu. Il a bien transféré les données. Quand j'ai exécuté, il a bien récupéré les données d'entrée et il l'a envoyé en sortie. Et j'ai bien les données dans l'interactive table ici. Vous avez bien vu. donc ça déjà c'est nickel bien maintenant l'idée c'est qu'on va essayer de compléter not\"), ('160038', \"ant l'idée c'est qu'on va essayer de compléter notre code alors déjà moi une question que je me pose j'ai utilisé la version qui est impactée de de Python ok mais quelle version de CKITLEARN dont je dispose du coup voilà je dispose de quelle version de CKITLEARN avec la version impactée de Python avec small set of package, comme il le dit. Allons-y, alors. Du coup, le code qui est là, je vais essayer de le compléter. Alors, c'est là que j'ai récupéré un peu. J'ai préparé le code en amont. Voilà.\"), ('160039', \"péré un peu. J'ai préparé le code en amont. Voilà. Voilà. Donc, vous avez vu, j'ai rajouté dans le code, là. Je suis allé ici, je suis allé un peu vite, peut-être. Donc, revenons. Je suis allé là. Voilà. Je suis allé dans Configure. Très bien. Et là, j'ai complété le code. Donc les données sont transférées toujours, ça je n'y touche pas. Et j'ai complété le code où j'ai importé la librairie CKIT Learn et j'ai affiché la version de CKIT Learn. Run all, je vais lancer le script là. Boum! La versio\"), ('160040', ' all, je vais lancer le script là. Boum! La version par défaut de CKIT Learn avec le Python qui est impacté et 1.3.2. Là on est en octobre 2024, on est plutôt en la version 1.5 et quelques. là, voilà. Mais bon, en tout cas, on a bien cette version-là. Donc, si ça ne nous satisfait pas, si on veut une version plus récente de C-Hitler, là, pour le coup, il faudra créer un environnement et un porteur sous Konda, dans la Konda, du coup, sous Konda, avec Konda, et ensuite, vous importez cet environne'), ('160041', \"vec Konda, et ensuite, vous importez cet environnement-là, ici, comme j'ai montré tout à l'heure. Mais bon, là, pour demander un exemple, moi, ça me suffit, un peu. Bien. Alors, une fois que c'est fait ça, alors, qu'est-ce qui se passe? Ce qui serait pas mal, maintenant, c'est que je suis complètement code alors qu'est ce que je veux faire ce que je veux faire voilà c'est récupérer les données en entrée ensuite il faut que j'isole la variable cible classe et les autres variables rédictives et en\"), ('160042', \"le classe et les autres variables rédictives et ensuite lancer la régression logistique là ce que je veux faire ok alors déjà le premier élément c'est récupérer les données d'entrée et les transformer en panda c'est ce que je vais faire ici donc ça je vais supprimer ça et maintenant on va récupérer les données d'entrée pour les mettre dans un data frame panda alors ça le code là il est là voilà regardez là ce que je fais c'est que tout simplement je récupère le input table donc la flèche d'entré\"), ('160043', \" je récupère le input table donc la flèche d'entrée ici et je la caste  en panda. Ensuite, je l'ai mis dans une variable data et j'affiche les colonnes. Je vais lancer pour voir. Et voilà, il dit, il a bien récupéré les éléments, mais il me dit, attention, tu n'as rien mis dans l'output. Parce que je l'ai effacé l'output. On est bien d'accord là-dessus. Ce n'est pas très malin, ça. On est bien d'accord. J'ai effacé l'output. Du coup, il m'a dit, voilà la version, il l'affiche là. Ensuite, j'ai r\"), ('160044', \"voilà la version, il l'affiche là. Ensuite, j'ai récupéré les données, je l'ai mis en panda, en data ici. Et ensuite, voilà, j'ai affiché les colonnes, je l'ai mis ici. Et il m'a dit, attention, dans ton script, là, tu as bien récupéré les entrées, mais tu n'as rien mis en sortie. Et ce n'est pas bon, ça. Bon, on va le remettre. OK. Donc là, pour l'instant, on va remettre. voilà hop là là au lieu de une poudre c'est out tout est bien je relance un running là très bien j'ai vu un là on a la versi\"), ('160045', \"n running là très bien j'ai vu un là on a la version très bien et là je supprimer peut-être voilà je relance voilà et on a bien les index ici et là cette fois ci comme j'ai bien envoyé des éléments en sortie il râle plus il ya dit oui c'est cohérent comme il ya une flèche en sortie là j'aurais dû envoyer quelque chose et je n'avais pas mis cette sortie là m'a dit oui ok et bien donc je sais Je sais que maintenant, je peux récupérer les données puisque je récupère les colonnes ici. C'était ça l'e\"), ('160046', \"isque je récupère les colonnes ici. C'était ça l'enjeu. OK. Qu'est-ce qui se passe alors? Maintenant, je vais essayer de lancer la réaction logistique. C'est ce que je vais faire maintenant. Alors, je récupère. Voilà. J'ai mis ici parce que je ne voulais pas... Je récupère ça. Voilà. Alors, il manque l'importation. Alors, je vais expliquer bien sûr. pas à part, sinon on ne comprend pas ce que je manipule. Regardez, là, je vais agrandir un tout petit peu pour qu'on voie mieux. Regardez, là, je ré\"), ('160047', \"tit peu pour qu'on voie mieux. Regardez, là, je récupère les données en entrée. Je les mets en DataFrame Panda. J'affiche les colonnes, c'est ce qu'il a fait ici. Ensuite, je récupère dans deux structures distinctes, la variable cible et les explicatives. La variable cible, c'est la colonne classe, la dernière à droite. Et ensuite, en explicative, c'est les autres colonnes. C'est ce que je fais ici, là et là. Ensuite, là, je vais importer la réaction logistique de CKET Learn, qui est la version \"), ('160048', \"tion logistique de CKET Learn, qui est la version 1.3.2 ici, et j'instancie la réaction logistique. Voilà. Ensuite, je lance l'entraînement ici du modèle. OK. Alors, j'ai des coefficients maintenant. Ce serait pas mal que je les affiche. Donc, il me manque quelque chose. Je mets la librairie Panda. Et une fois que j'ai mis la librairie Panda, je récupère les coefficients de la régression logistique de LR je récupère le LR ici et je l'affiche dans la console print coef à la sortie je renvoie les \"), ('160049', \" la console print coef à la sortie je renvoie les données telles quelles pour l'instant voyons ça en run all et on a bien les coefficients vous avez bien vu je vais effacer je relance, c'est quand même mieux d'avoir tout propre on a la version de Python c'est ce que fait le print ici, la version de CKIT Learn plutôt. Ensuite, j'affiche les colonnes, très bien, et ensuite j'affiche les coefficients que j'ai mis sous forme de Derafin Pranda pour que ce soit plus lisible. Donc là, c'est les colonne\"), ('160050', \"e ce soit plus lisible. Donc là, c'est les colonnes, c'est ça, et ensuite, l'affichage des coefficients, c'est ça. Bien, alors qu'est-ce que j'aimerais avoir? Ce que j'aimerais avoir, donc là tout va bien, ça fonctionne, mais là je fonctionne ici en standalone, en fait, dans le Python script. Ce qui ne serait pas mal, c'est que dans les données en sortie, je vais rajouter la colonne des prédictions en resubstitution sur l'échange d'apprentissage. Juste pour voir si ça marche. Et surtout pour voi\"), ('160051', \" Juste pour voir si ça marche. Et surtout pour voir comment manipuler cette partie-là pour ajouter les informations qui m'intéressent. C'est ça l'idée. Allons-y. L'idée est très simple. Tout simplement, je vais le mettre ici. Regardez, je récupère les données d'entrée. Data, c'était les données d'entrée que j'avais typées en panda. Dans la variable data. Je récupère les données d'entrée et je les copie. Je fais une copie que je mets dans OutData. Ensuite, dans OutData, je rajoute une colonne que\"), ('160052', \" Ensuite, dans OutData, je rajoute une colonne que j'appelle prédiction. et cette prédiction là je la remplis, je la complète avec la prédiction sur l'échantillon d'apprentissage lui-même. On verra sur l'échantillon test après, parce que je connais bien les éléments, mais pourquoi il fait la prédiction sur l'échantillon d'apprentissage? Non, là ce qu'on essaie de voir c'est comment rajouter des informations dans l'autre table, c'est ça qu'on veut, très bien, donc c'est ce que je fais ici. Donc, \"), ('160053', \", très bien, donc c'est ce que je fais ici. Donc, du coup, ce qui se passe, c'est qu'une fois que j'ai les données initiales plus la colonne prédiction, il faut que je l'envoie dans la sortie ici. Vous êtes d'accord là-dessus? Donc là, c'est plus... C'est le hot data que je peux mettre là. Hot data. Sauf que ça ne va pas marcher. Pourquoi ça ne marche pas? Parce que le hot data, c'est un panda. Et là, c'est un objet table qui est à lui. Je ne sais pas le type. Il faudrait regarder vous-même. Trè\"), ('160054', \"s pas le type. Il faudrait regarder vous-même. Très bien. Donc, je sais très bien que ça ne va pas être compatible en réalité. Donc, il faudra caster également ces données-là en table de NIME. Alors ça, tout simplement, c'est ça. Bon, pas besoin d'être pris Nobel pour comprendre ce qu'on est en train de faire. Regardez, je prends les données que je veux envoyer en sortie, qui est un DataFrame Panda, et je vais le typer en table 9. Et cet élément que je viens de créer là, cet objet que je viens d\"), ('160055', \"que je viens de créer là, cet objet que je viens de créer, je l'envoie en Notputable 0, le premier élément qui est là. Alors, si je lance, ça va marcher, bien sûr. Mais on ne voit rien, en fait. Ce qui est intéressant, c'est que je vais fermer ça maintenant, et je vais lancer de nouveau le workflow. Donc ici, il a pris les données d'entrée, il a fait tout ce que je viens d'expliquer tout à l'heure, il a créé le modèle, il a créé un nouveau data frame avec les données d'entrée, et il a rajouté la\"), ('160056', \"rame avec les données d'entrée, et il a rajouté la prédiction, et il l'a envoyé en sortie. Voyons si ça marche. Exocute en OpenViews. Voilà, très bien. Et on a bien ça. On a bien jusqu'à classe les données d'entrée, avec les 399 observations, les mêmes que tout à l'heure. Très bien. Ça, c'est un premier élément. et il a rajouté la colonne de prédiction. Ça y est, je sais manipuler l'épitonscript. Je sais exactement ce que je vais faire avec et je suis capable de le faire. C'est ça l'intérêt. Trè\"), ('160057', \" suis capable de le faire. C'est ça l'intérêt. Très bien, nickel. Alors, si on essaie d'aller plus loin, si on veut exporter maintenant les coefficients sous forme de data frame panda, donc il faudrait rajouter un deuxième port pour qu'on ait les coefficients également qui soient exportés pour que, je ne sais pas, faire n'importe quoi avec, je ne sais pas. Mais moi, ce que je veux surtout savoir, c'est comment on rajoute un port supplémentaire en sortie. C'est ça que je veux savoir. Comment on r\"), ('160058', \" sortie. C'est ça que je veux savoir. Comment on rajoute un port supplémentaire en sortie et comment on va le compléter, le « populate », comme on dit en anglais. Alors, ça en réalité, c'est un mécanisme que j'ai mis un peu de temps à trouver, mais j'ai fini par trouver en lisant la doc tout simplement. C'est qu'on peut rajouter et enlever les ports. Pour l'instant, il n'y a que... Je vais mettre un peu plus bas pour qu'on voie mieux les choses. On peut rajouter et enlever des ports. Pour l'inst\"), ('160059', \"On peut rajouter et enlever des ports. Pour l'instant, il n'y a que deux ports. Un port en entrée, c'est celui-là qu'on voit, et un port en sortie, qu'on voit bien ici. Alors moi, je veux quoi? Je veux rajouter une nouvelle table en sortie qui correspond aux coefficients du modèle. Dans les coefficients que j'avais fait print, ces coefficients print que j'ai mis sous forme de DataFrame Panda, je veux qu'ils soient en sortie. Donc, dans les ports, là, je vais ajouter un port de sortie. Et il l'a \"), ('160060', \" là, je vais ajouter un port de sortie. Et il l'a rajouté ici. C'est génial. En vrai, franchement, les gars qui ont fait NIME, c'est quand même des gens qui réfléchissent et qui réfléchissent de manière bien. C'est vraiment un super outil. Très bien. Qu'est-ce qu'il me reste à faire? Ce qu'il me reste à faire, du coup, c'est modifier mon code pour que je rajoute un élément dans le deuxième port de sortie. Allons-y dans le script. Alors, ça c'est le script initial. Du coup, il faut que je rajoute\"), ('160061', \"le script initial. Du coup, il faut que je rajoute un élément dans le deuxième port de sortie. Et là, les ports sont indexés en fait. Là, celui d'en haut, c'est le port 0. Et celui d'en bas, du coup, c'est le port numéro 1. Alors, pour envoyer les éléments, je mets le code ici. Regardez, les coefficients que j'avais fait print là. Vous avez vu, les coefficients que j'avais fait print là. J'ai mis ce form de data frame panda. Je vais l'envoyer dans le port numéro 1 en sortie de table. Voilà. Form\"), ('160062', \"s le port numéro 1 en sortie de table. Voilà. Form panda de coef, qui sont les coefs qui sont là.  Voilà, je l'envoie dans le port numéro 1 des tables. Le numéro 0 est dans le premier. Bien, faites OK alors. Du coup, je vais le visualiser. De nouveau, je mets un interactive table. Très bien. Voilà. Et je fais cette connexion-là. Encore une fois, regardons bien. Le 0, c'est celui-là. Voilà, c'est les données plus la prédiction, en resubstitution. Et le 1, le deuxième en bas, ça va être les coeffi\"), ('160063', \"Et le 1, le deuxième en bas, ça va être les coefficients que j'avais mis sous forme de tata frame panda. Regardons si c'est vrai. Donc, je sauve. Contrôle S, j'ai la main greffée sur le contrôle S. Et du coup, le S est quasiment effacé même. Voilà, j'exécute. Et on a bien les coefficients cette fois-ci. Là, sur le port numéro 1, on a bien les coefficients. Et sur le port numéro 2, on a bien... Bien vu, très bien. Hop là. On a bien les deux éléments. Celui-là, c'est celui-là. Celui en haut à droi\"), ('160064', \"ts. Celui-là, c'est celui-là. Celui en haut à droite ici, c'est cette table-là. Et celui du bas là, c'est celui-là. C'est génial. C'est génial. Bien. Alors, une fois que j'en suis là, je me dis, bon, OK, je vais essayer de continuer. L'idée, c'est quoi? L'idée, justement, c'est que cette fois-ci, je vais exporter le modèle pour pouvoir faire la prédiction. Je reviens à mon exemple de référence. C'est ce qui se passe. Ici, maintenant, on comprend bien. On a les ports d'entrée ici, ports d'entrée,\"), ('160065', \"bien. On a les ports d'entrée ici, ports d'entrée, port de sortie. Et là, le port de sortie, cette fois-ci, ce n'est pas une table. Parce que ce n'est pas une flèche. Donc, c'est un autre type, en fait. Ça va être un objet. Ensuite, une fois que c'est exporté là, ça va être l'objet modèle. Le classifieur de type City Learn. Ça, c'est fait. Une fois que c'est ça, là, je vais l'envoyer, celui qui va faire le prédict sur l'échantillon test. Donc, j'ai un autre script qui prend les données de l'écha\"), ('160066', \"ai un autre script qui prend les données de l'échantillon test. Moi, l'échantillon test, il est dans une autre base. Voilà. qui prend le modèle en entrée et qui va faire le predict pour l'envoyer ici. Si c'est ça, moi je peux le faire sans problème, je te le fais sans souci. Du coup, qu'est-ce qui se passe pour nous? Ça veut dire que dans le script ici, on a sorti les données avec les prédictions en resubstitution, on a sorti une deuxième table, c'est les coefficients, il faut que j'exporte l'ob\"), ('160067', \"c'est les coefficients, il faut que j'exporte l'objet modèle. Il faut rajouter un port du coup. C'est ce que je fais. « addPort » et cette fois-ci, ce n'est pas une image, c'est un objet piclé. Très bien, zoom. Et on a le petit câble qui apparaît. Qu'est-ce qu'il me reste à faire? Il me reste à compléter ce port-là. Allons-y. Du coup, on vient dans la configuration. Et là, c'est le premier port qui est objet. On est d'accord. Du coup, je vais compléter mon code. Voilà. Et ce que j'envoie en sort\"), ('160068', \"pléter mon code. Voilà. Et ce que j'envoie en sortie, c'est le classifieur lui-même. Regardez, le classifieur, j'avais l'instancier là. Ensuite, j'avais fait un fit. Il a les bons coefficients et tout ça. Il est en mémoire maintenant. Cet objet en mémoire, le LR, je vais l'envoyer dans les outputs. Et cette fois-ci, ce n'est pas un output table, c'est un output object. et c'est le premier output object, c'est le premier bleu ici il y a deux noires, deux flèches noires table 0 c'est la première f\"), ('160069', \"s, deux flèches noires table 0 c'est la première flèche noire table 1 c'est la deuxième flèche noire et object 0 c'est le premier carré bleu ici donc si je vais exporter plusieurs objets, c'est possible je rajoute un deuxième port et c'est output object 1 que je vais compléter cette fois-ci bien, très bien Voilà, c'est prêt. Donc, je peux exécuter. Et ça marche sans problème. Et il exporte bien l'objet ici. On va essayer de voir son. Donc, l'étape suivante, c'est quoi alors du coup? L'étape suiv\"), ('160070', \"e suivante, c'est quoi alors du coup? L'étape suivante, c'est on va importer les données test et on va récupérer en entrée les données test et le modèle qui est là. Et on va faire la prédiction. Alors, de nouveau, du coup, je vais charger les données Excel, Excel Reader. Très bien. Dans Excel de Reader, je vais récupérer de nouveau Brest, qui est dans une démo. Et cette fois-ci, je récupère la partie test. Vous avez vu? Ok. Une fois que c'est fait, donc, dans la partie test, il y a 300 observati\"), ('160071', \"t, donc, dans la partie test, il y a 300 observations. J'ai écrit quelque part, mais bon, on le voit, de toute manière, les numéros de ligne. Donc, j'exécute, voilà, et si je visualise, on a bien les 300 observations de l'échantillon test. Tout va bien. Et de nouveau, je récupère un Python script. Par défaut, il prend les données en entrée. Vous êtes d'accord là-dessus? Très bien. Donc, si je connecte là, il récupère les données en entrée, qu'il envoie en sortie. Donc, si j'exécute, si je mets u\"), ('160072', \"envoie en sortie. Donc, si j'exécute, si je mets un interactif table là, et que je lance, le code là, il est par défaut, là on est d'accord. le code par défaut, c'est qu'il récupère les données en entrée, input table 0, le port numéro 0, il n'y en a qu'un seul, donc c'est forcément numéro 0, et il l'envoie dans le port numéro 0 de table en sortie, la flèche noire, voilà. Donc si je regarde ça, et que je lance là, execute on open views, on a bien les 300 observations, vous avez bien vu, c'est 299\"), ('160073', \"les 300 observations, vous avez bien vu, c'est 299 parce que ça commence à 0 le numéro de ligne, donc on a bien les 300 observations. Donc mon idée à moi, c'est quoi? c'est qu'on va récupérer le modèle qui est là et l'appliquer sur l'échantillon test. Et de sortir ici les données avec les données initiales de test initial et la colonne de prédiction sur l'échantillon test. Qu'est-ce qu'il faut alors? Il faudrait maintenant que je récupère, que j'ajoute un port en entrée, un input object. Vous av\"), ('160074', \"ajoute un port en entrée, un input object. Vous avez vu, dans le Python script ici, j'avais rajouté un output object picled. Et là, cette fois-ci, je vais ajouter un input object picled. Hop là! Et du coup, on va faire la connexion entre l'objet exporté qui soit importé ici. Qu'est-ce qu'il reste à faire? Il reste à écrire le code. Allons-y, c'est ce que je vais faire. Alors, je récupère le code, très bien, que j'ai préparé en amont toujours. Voilà. Et je vais le mettre ici. Voilà. Alors, là, to\"), ('160075', \"là. Et je vais le mettre ici. Voilà. Alors, là, toujours, c'est le module pour importer les données, pour manipuler les scripts avec NIME. OK, très bien. Ensuite, je vais mettre bien la data test. Voilà. je récupère les données d'entrée, le port noir ici, je le mets sous Panda, et je le mets dans la variable data test. Ensuite également, je récupère le modèle qui est le premier objet. Il n'y a qu'un seul port d'objet en entrée ici, c'est le bleu qui est là, il n'y en a qu'un seul port, et du cou\"), ('160076', \"qui est là, il n'y en a qu'un seul port, et du coup c'est bien le port numéro 0. Très bien. Ensuite, dans ces données d'entrée, dans les données data test, je vais récupérer uniquement les variables qui ont été utilisées dans le modèle. Donc, modèle features name in, c'est la liste des variables qui sont utilisées dans la variation logistique. Très bien. Alors là, comme j'ai changé le nom, je mets data test. Vous voyez d'accord là-dessus, sinon ça ne va pas marcher. Une fois que j'ai récupéré le\"), ('160077', \"a ne va pas marcher. Une fois que j'ai récupéré les X, qu'est-ce qui se passe? J'applique la prédiction sur les données test cette fois-ci, et j'ai une colonne prédiction. Qu'est-ce qu'il me reste à faire? Il me reste à rajouter, du coup, cette nouvelle colonne dans les données test. Donc, je duplique DataTest. Et je mets dans OutData. Et dans le OutData, je rajoute la colonne Prédiction. Qu'est-ce qu'il me reste à faire maintenant? Dans le port de sortie, il faut que je réduise un peu. Dans le \"), ('160078', \"de sortie, il faut que je réduise un peu. Dans le port de sortie, il n'y en a qu'un seul. Il y en a numéro 0. Dans le port de sortie, je rajoute le code Panda. Je rajoute en le translippant en Panda. C'est bien vu. Donc, le root data, là, où j'ai les données test plus la prédiction que j'ai appliquée sur les X en test, je le mets en panda et je l'envoie en sortie. Alors, si je le run, là, pour vérifier, vérifions ça, il a marché. Il a dit, vous avez bien les variables en mémoire, tout est nickel\"), ('160079', \"vez bien les variables en mémoire, tout est nickel, avec les différents éléments. Donc, on sait que ça marche. OK. OK, du coup, je vais, si ça marche, testons, je vais lancer. Et on va voir s'il a bien rajouté la colonne de prédiction dans mes données test. Donc j'exoccuite en OpenViews, voilà. Et on a bien ici dans mes données, vous avez bien vu, on est bien sur l'échantillon test, il y a 300 observations. On a les X plus la classe, puisque j'avais dupliqué les données pour les mettre en sortie\"), ('160080', \"ais dupliqué les données pour les mettre en sortie, et on a la colonne prédiction. Qu'est-ce qu'il me reste à faire du coup? Il me reste à confronter ces deux colonnes-là, c'est ça. Il me reste à confronter ces deux colonnes-là, la classe observer et la classe predict. sur l'échantillon test pour que la matérialisation de confusion ait calculé l'accuratie, ainsi de suite. Allons-y, c'est ce que je vais faire maintenant. Il y a un outil pour ça, c'est Scorer. Voilà, Scorer, et je le mets ici. Alo\"), ('160081', \"'est Scorer. Voilà, Scorer, et je le mets ici. Alors, je vais le configurer. Qu'est-ce que je pose? Je pose la classe observée et la prédiction. Voilà.  Là, pédagogiquement, c'est quand même super NIME. Parce que quand je fais mon cours moi-même, je passe mon temps à faire des schémas au tableau pour montrer, parce que c'est un flux en fait, c'est un workflow, c'est un pipeline en réalité qu'on manipule. Et dans le pipeline, il y a différentes étapes. Et il faut absolument qu'on les comprenne. E\"), ('160082', \"apes. Et il faut absolument qu'on les comprenne. Et justement, les schémas que je fais au tableau pour montrer chacune des étapes du workflow, je peux les reproduire exactement sous NIME. Et visuellement, les étudiants comprennent exactement ce qui se passe. Donc on a bien les éléments ici. Une fois que c'est fait ça, voilà, exécute en OpenViews, et j'ai bien la matrice de confusion, voilà. Et voilà, j'ai le nombre d'éléments mal classés, c'est 15, c'est les éléments qui sont sur l'antidiagonal,\"), ('160083', \"5, c'est les éléments qui sont sur l'antidiagonal, là, et j'ai l'accuratie, et j'ai l'accuratie, ou son complément, le taux d'erreur. Ici, c'est anecdotique, de toute manière, on sait que les fichiers Brest-Cancers, c'est très facile à apprendre. Et on a bien 5% d'erreur. Voilà. Alors, qu'est-ce qu'il me à dire ce qui me reste à dire ce que j'avais fait un pense bête mais je l'ai oublié l'attendez il faut que je remette ça qu'est ce qui me reste à dire pense bête parce que j'avais mis en pense b\"), ('160084', \"à dire pense bête parce que j'avais mis en pense bête mais je l'ai effacé par défaut figurez vous voilà hop là il est là alors qu'est ce qui me reste à dire si vous voulez faire si vous voulez mettre du code complexe ici voilà rien ne vous empêche d'utiliser les pipelines au lieu d'avoir une action très simple je crée un modèle voilà et je fais la réaction logistique mais admettons que je vais faire de l'encodage un one mode encoding parce que j'ai des variables explicatives qualitatives et ensu\"), ('160085', \"ai des variables explicatives qualitatives et ensuite admettons que je vais combiner ça avec une standardisation ou bien je fais une projection factorielle et je vais lancer la réaction sur les composantes voilà à ce moment là on a tout intérêt à utiliser des pipelines voilà donc le code qui est là vous le complétez en utilisant des pipelines et là vous vous avez une possibilité. Alors, les pipelines, j'en parle beaucoup moi-même. Regardez ici, c'est à la place de NIME, je mets pipeline. Je vais\"), ('160086', \"'est à la place de NIME, je mets pipeline. Je vais mettre là, pipeline. Voilà. Donc, je montre, ce n'est pas comme si j'en ai fait pas mal. Donc, vous regardez le principe des pipelines. Là, c'est relativement simple. Voilà. Et ensuite, on peut avoir des choses plus complexes, plus élaborées, plus sophistiquées par ici. Très bien. Donc, vous avez la possibilité ici. Si vous avez une trame plus complexe que je crée un modèle et j'exporte le modèle, c'est de dire j'ai des opérations complexes et à\"), ('160087', \", c'est de dire j'ai des opérations complexes et à ce moment-là, j'ai mis les successions d'opérations dans un pipeline. Et vous avez cette possibilité-là. L'autre élément important également, c'est qu'on est sous Python. Du coup, qu'est-ce qui m'empêche de faire un programme plus sophistiqué également? c'est-à-dire mettre des branchements conditionnels pour faire des actions sélectives selon ce que je vais faire, ou encore également, mettre en place également, je ne sais pas, des boucles. Rien \"), ('160088', \"lace également, je ne sais pas, des boucles. Rien ne vous empêche de faire ça pour avoir des calculs très complexes, et ensuite seulement exporter des résultats. De la même manière, si vous avez des pipelines de données, cette fois-ci des workflows de données, rien ne vous empêche de mettre un script Python, et dans le script Python, de rajouter les opérations sophistiquées que vous voulez mettre en place, de transformation de variables. Du coup, si je vois bien l'idée, vous avez votre pipeline \"), ('160089', \" si je vois bien l'idée, vous avez votre pipeline ici avec les fonctionnalités qui sont super. Là, vous avez les manipulations sur les colonnes, vous avez les manipulations sur les lignes. Donc, vous mettez ici les éléments standards de votre workflow de manipulation standard des données. Et si à un stade, vous avez besoin de programmer des manipulations plus complexes en Python, vous rajoutez votre composant dans un Python script. et dans le Python script, vous codez de manière plus complexe le\"), ('160090', \"hon script, vous codez de manière plus complexe les opérations que vous rajoutez dans Workflow. Et en termes de manipulation de données, les possibilités sont infinies. Elles sont du même ordre que quand je programme tout, from scratch, tout Python. Sauf que là, il y a déjà tout un tas de manipulations qui sont déjà pré-programmées. Donc, tout ce qui est pré-programmé, je laisse faire NIME, il le fera très bien de toute manière. Et quand j'ai des choses vraiment propres à moi, très complexes, là\"), ('160091', \" choses vraiment propres à moi, très complexes, là je mets un script python pour enrichir les différentes transformations. Et après, je mets tout ça dans le workflow. C'est génial ici. De toute manière, moi je pense, il n'y a pas un outil qui fait tout. Ça n'existe pas. Donc, une première question qui se pose à nous en tant que data scientist, c'est choisir l'outil le plus adapté par rapport aux problèmes qu'on veut traiter. Ça, c'est toujours été mon credo. Et le deuxième élément qui me trouve \"), ('160092', \"é mon credo. Et le deuxième élément qui me trouve très intéressant, c'est de savoir combiner les outils quand il y a un des outils qui ne suffit pas. Un des outils individuellement n'arrive pas à tout faire, ou un autre, vous prenez les outils et vous les combinez. Et savoir combiner les outils, c'est un vrai savoir-faire. Et là, pour le coup, la possibilité de pouvoir mettre du script Python dans un workflow NIME, ça me paraît quelque chose de pas mal du tout. Je me demande s'ils vont pas faire\"), ('160093', \"as mal du tout. Je me demande s'ils vont pas faire des TD là-dessus. Il faut que je réfléchisse un peu. Voilà, excellent travail à tous.\"), ('170001', \"### Introduction\\n\\nBien, c'est parti. Dans cette vidéo, je vais parler des auto-encodeurs et de leur implémentation avec PyTorch. Très rapidement, je ne vais pas faire le cours ici, le cours est en ligne. Mais ce que c'est qu'un auto-encodeur, c'est un réseau de neurones avec plusieurs couches, avec des couches intermédiaires. Et l'idée, c'est de créer un réseau qui permet de reconstituer le mieux possible des données initiales. Donc dans la couche d'entrée et de sortie, on a la même chose. Ce n'\"), ('170002', \"e d'entrée et de sortie, on a la même chose. Ce n'est pas tellement la reconstitution en elle-même qui est intéressante, c'est le fait qu'on a des couches intermédiaires, et que ces couches intermédiaires-là donnent une représentation des données résumées, si vous voulez. C'est une forme de compression, c'est une forme de représentation factorielle.\\n\\n### Pourquoi PyTorch ?\\n\\nPourquoi j'en parle ici, et pourquoi surtout je parle avec PyTorch, parce que, vous avez remarqué, j'avais fait des séries \"), ('170003', \" que, vous avez remarqué, j'avais fait des séries de vidéos avec PyTorch. Pourquoi j'ai fait ça? Parce qu'on a un cours de deep learning à l'université, très bien, et quand je fais les bilans avec les étudiants, chaque année je fais un bilan avec les étudiants, ils m'avaient dit, ah oui, mais PyTorch, quand même, on a des difficultés, on a mis du temps à rentrer dedans et tout ça. Alors, moi, en début d'année, c'est mon rôle, ça c'est mon rôle pour le coup en tant que responsable, c'est de faire\"), ('170004', \"ur le coup en tant que responsable, c'est de faire des remises à niveau pour que les étudiants soient préparés à affronter, à suivre le mieux possible dans les meilleures conditions les différents enseignements. Donc, j'ai des ateliers, j'appelle ça des ateliers où j'ai fait des remises à niveau sur différents thèmes et j'ai prévu du coup depuis cette année de mettre en place des remises à niveau sur le deep learning et notamment sur l'utilisation de PyTorch. Donc les vidéos là sur PyTorch là c'\"), ('170005', \"n de PyTorch. Donc les vidéos là sur PyTorch là c'est pas du au hasard on est bien d'accord donc ça y est en début d'année on est là on est le 9 octobre là très bien donc l'atelier sur le diplôme de PyTorch ça va arriver incessamment sous peu et en regardant un peu là je me suis rendu compte que j'avais oublié j'avais fait quand même pas mal sur PyTorch mais j'ai oublié j'en ai oublié un c'est sur les auto encodeurs or ça fait partie des choses que l'on va explorer qu'on explore chaque année. Ma\"), ('170006', \"ue l'on va explorer qu'on explore chaque année. Mais chaque année jusqu'à présent j'utilisais Keras, parce que c'est le plus intéressant, plus simple à utiliser. Mais comme il y a une demande autour de Python de la part de mes étudiants, ben zou je te le fais moi. Je le fais sans problème la vidéo.\\n\\n### Objectifs des Vidéos\\n\\nBien, donc une fois que c'est dit ça, j'ai un deuxième élément. Donc les vidéos c'est pour mes étudiants, mais c'est pour tout le monde également, il faut qu'ils servent à t\"), ('170007', \"out le monde également, il faut qu'ils servent à tout le monde, c'est ça l'idée. C'est pour ça que je fais toujours un petit préambule un peu long, mais il faut absolument que tout le monde puisse se situer pour savoir comment il peut profiter au mieux de la vidéo. Et justement, il y a un élément très important, j'ai fait ça cet été, c'est que j'ai recensé toutes mes vidéos, donc celle de 2004, celle de 2023, j'étais polyx, celle de 2022, j'ai recensé toutes mes vidéos, et pour chacune des vidéo\"), ('170008', \"censé toutes mes vidéos, et pour chacune des vidéos, j'ai mis la référence, le descriptif, et j'ai mis en ligne également tout le matériel associé. Quand c'est sous Python, il y a quand même pas mal de Python là-dedans, de notebook Python, j'ai mis en ligne du coup, là on le voit, regardez en bas à gauche là, voilà, en bas ici, je ne peux pas montrer avec le doigt ici, avec la souris là, en bas ici, là, vous avez vu? Voilà, en bas à gauche donc j'ai mis en ligne un fichier archive où dans l'arch\"), ('170009', \"'ai mis en ligne un fichier archive où dans l'archive je mets et le notebook je mets les données utilisées et tous les éléments qui permettent de fonctionner correctement, donc notamment quand je travaille sur Python, je mets en ligne le fichier IAML qui permet d'installer l'environnement à l'identique de ce que moi j'ai utilisé pour mon tutoriel. Donc ce travail-là est très important, j'en ai pris du temps, mais ça me paraissait indispensable. Comme ça, quand vous récupérez une vidéo, vous la t\"), ('170010', \"omme ça, quand vous récupérez une vidéo, vous la trouvez là, vous la trouvez intéressante, très bien, vous allez sur ce site-là, c'est le site des tutoriels, vous avez l'adresse qui est là, et sur ce site-là, vous retrouvez la vidéo avec le lien sur YouTube, et vous retrouvez également tout le matériel pédagogique qui permet de reproduire exactement ce que je fais dans le tutoriel. Les programmes, les données et les éléments complémentaires, comme les environnements, les fichiers IAML. C'est vra\"), ('170011', \"e les environnements, les fichiers IAML. C'est vraiment important parce qu'on voit tous beaucoup de tutoriels sur Internet, on voit tous de vidéos, mais quand on ne nous donne pas les éléments pour pouvoir les reproduire, des fois on se lance, on réécrit tout, mais ça ne marche pas parce qu'il manque des choses, parce qu'on ne sait pas, peut-être qu'il y a des choses qu'on a loupées. Il me paraît indispensable que tout le matériel qui permette de reproduire déjà à l'identique, ce serait pas mal \"), ('170012', \" reproduire déjà à l'identique, ce serait pas mal le tutoriel, doit être mis en ligne et je le fais je l'avais fait pour les tutoriels rédigés depuis très longtemps, depuis 2004 pour les tutoriels vidéo je me suis laissé emporter et à un moment donné je me dis stop, il faut absolument que je refasse ce travail là pour que ce soit réellement productif et justement, deuxième point important pour ce qui est des auto-encodeurs j'avais déjà travaillé dessus donc j'avais fait le support de cours là qu\"), ('170013', \"dessus donc j'avais fait le support de cours là que j'ai montré tout à l'heure et j'avais fait un tutoriel que j'utilise pour mes enseignements j'avais fait un tutoriel sur l'implémentation des auto-encodeurs avec Keras cette fois-ci c'est un tutoriel rédigé qui est là que j'avais écrit en novembre j'ai dit quand ça? novembre 2019, ça fait longtemps 5 ans très bien, voilà du coup, qu'est-ce que je vais faire? je vais essayer de reproduire la même idée de refaire la même trame, mais en utilisant \"), ('170014', \" idée de refaire la même trame, mais en utilisant PyTorch, on ne doit pas être dépendant de l'outil, c'est ça si je comprends ce que c'est que les auto-encodeurs et que je suis capable de le mettre en oeuvre avec Keras, il n'y a aucune raison que je n'arrive pas à le mettre en oeuvre avec avec PyTorch ou avec n'importe quel outil qui implémente les auto-encodeurs. C'est ça, le vrai fond de l'histoire, il est là. On doit maîtriser les outils, mais on ne doit pas être dépendant d'un outil. Nuance,\"), ('170015', \" on ne doit pas être dépendant d'un outil. Nuance, nuance.\\n\\n### Implémentation avec PyTorch\\n\\nMaintenant que j'ai fait mon long discours, on va pouvoir y aller réellement, parce que c'est ça qui importe. Là, j'ai mis le matériel pédagogique. Donc là, justement, c'est le notebook que je vais montrer tout à l'heure. Là, c'est le fichier IAML avec justement l'environnement, les paquets qui sont dans l'environnement. Donc, je vous montre très rapidement. Là, on voit bien ici. Donc, voilà les différen\"), ('170016', \"nt. Là, on voit bien ici. Donc, voilà les différents éléments que j'ai utilisés moi-même. Je le mettrai en ligne, du coup, maintenant que vous avez compris. Et là, il y a le fichier de données. Alors, le fichier de données, très rapidement, il y a différents éléments. Très bien. Il y a ici les individus et les variables actives, donc là c'est une étiquette, c'est un label, on a une série de véhicules, tout le monde sait que j'adore les voitures, très bien, c'est plus un secret, et là on a les va\"), ('170017', \"très bien, c'est plus un secret, et là on a les variables actives qu'on va utiliser justement pour essayer de trouver la représentation intermédiaire. On peut le voir comme une forme d'ACP, encore une fois, surtout dans sa forme très simple ici, j'ai qu'une couche cachée avec une fonction de transfert, là j'ai utilisé le transgénal hyperbolique, très bien, on peut le voir effectivement comme ça. Dans le tutoriel rédigé, j'ai utilisé une fonction de transfert sigmoïde cette fois-ci, et j'ai montr\"), ('170018', \"de transfert sigmoïde cette fois-ci, et j'ai montré des éléments d'interprétation, comme les services de corrélation, des choses comme ça. Dans cette vidéo aussi, en revanche, je vais utiliser le tangent hyperbolique pour changer, mais normalement, ça ne change pas le principe général. Très bien, nous avons la description, on a des variables qui ne sont pas dans les mêmes unités, on est d'accord, la puissance est en chevaux, le poids est en kilogrammes, la hauteur, ainsi de suite, ainsi de suite\"), ('170019', \"rammes, la hauteur, ainsi de suite, ainsi de suite, centimètres cubes, ainsi de suite. Là, les variables actives et individus actifs. Ensuite, là, on a les individus supplémentaires qu'on va essayer de projeter dans l'espace factoriel, dans l'espace intermédiaire. On va voir si c'est possible. Et là, j'ai recoupé les Peugeot cette fois-ci. Donc, Peugeot 407, voiture familiale, 307cc, plus coupé cabriolet avec le toit qui s'ouvre. Très bien. Mi7 plus monospace et 67 plus berlin. Voilà. Donc, les \"), ('170020', \"lus monospace et 67 plus berlin. Voilà. Donc, les individus supplémentaires qu'on va essayer de représenter dans l'espace factoriel, entre guillemets encore en cours factoriel, et l'autre élément important, c'est les variables illustratives qui permettent de renforcer l'interprétation des représentations factorielles. C'est parce que je mets dans le plan que je peux le faire. Si on est à plusieurs dimensions, c'est un peu plus complexe et puis l'intérêt est nettement moindre. Mais bon, là, comme\"), ('170021', \"intérêt est nettement moindre. Mais bon, là, comme je me suis mis dans le plan, pour faire le pont entre ce qu'on va voir aujourd'hui, les autocodeurs, et ce qu'on pourrait faire par ailleurs avec les méthodes factorielles, voilà, j'utilise les variables illustratives qui permettent d'interpréter les positions relatives entre les véhicules qu'on aura dans l'espace. Mais bon, c'est le principe de l'embedding qui est important. Dans les auto-encodeurs, c'est le principe de l'embedding qu'il faut b\"), ('170022', \"urs, c'est le principe de l'embedding qu'il faut bien essayer de bien situer pour pouvoir en profiter le mieux possible par la suite. Bien, allons-y alors.\\n\\n### Préparation de l'Environnement\\n\\nJe ferme ça et j'ouvre mon... Je ferme aussi ça parce que je ne veux pas que ça m'embrouille. Voilà, voilà. Alors, voilà. comme d'habitude j'ai rédigé d'abord je vais pas taper le code devant vous et première chose à faire vérifier la version de python vérifier  version de Python. Donc, j'ai créé l'environ\"), ('170023', \"fier  version de Python. Donc, j'ai créé l'environnement avec le fichier IAML. J'ai fait Create. Alors, moi, je suis plutôt Konda. Donc, Konda en Create, ainsi de suite, avec le fichier IAML, il a installé l'environnement. Et donc, j'ai choisi le bon noyau ici. Vous avez bien vu, hein? J'ai pris le bon noyau ici. Donc, j'ai lancé VS Code et j'ai sélectionné le noyau, voilà, comme noyau, l'environnement que j'ai créé dans le Anaconda PowerShell. Alors, je vérifie la version. Python, très bien, je\"), ('170024', \"lors, je vérifie la version. Python, très bien, je vérifie la version de PyTorch voilà la version de PyTorch que j'ai utilisée 2.3.0, CPU, je n'ai pas le GPU non, je n'ai pas les moyens alors, la version de Python c'est 3.11.9 première chose à faire, donc j'importe la première faite donnée où il y a les individus et variables actives et j'affiche les 10 premières lignes, c'est ce que je fais ici voilà, donc on a bien la Citroën C5 la Laguna, Citroën C4 Renault Crio, Renault Laguna ainsi de suite\"), ('170025', \"oën C4 Renault Crio, Renault Laguna ainsi de suite, avec les cylindrées, les puissances et tout ça. Bon, on voit bien, là je reconnais tout de suite le modèle. C'est le V6, je pense. Très bien, ainsi de suite, ainsi de suite. Alors, pour avoir les informations sur les variables, je fais info, et on n'a pas de valeur manquante. En TD, on n'a jamais de valeur manquante, je ne vais pas commencer à vous embrouiller là-dessus. sachant qu'on aura aussi une remise de niveau sur un atelier sur la qualit\"), ('170026', \" une remise de niveau sur un atelier sur la qualité des données. C'est très important. Il faut quand même qu'on sache exactement ce qui se passe si on a des données manquantes, quelle est la bonne procédure à mettre en œuvre. Et il n'y a pas de solution universelle. Là également, je le dis tout le temps à mes étudiants, il n'y a pas de solution universelle pour les données manquantes. Pour les données manquantes, il faut tenir compte de la méthode de machine learning commis derrière et puis auss\"), ('170027', \"e de machine learning commis derrière et puis aussi tenir compte du fait que, est-ce qu'on est sur la phase d'apprentissage ou bien on est sur la phase de déploiement. Là, il faut... Ça, c'est un élément très important. C'est pour ça que, quand on est dans l'apprentissage des méthodes elles-mêmes, on va au cas où le plus simple, parce que c'est la compréhension de la méthode qui prime. Après, les problèmes de... Voilà, ça, c'est une autre histoire. Mais en tous les cas, parce qu'il y a plein de \"), ('170028', \"e. Mais en tous les cas, parce qu'il y a plein de gens qui disent « Ouais, on n'apprend pas les données manquantes », mais il n'y a pas de solution universelle. Quand on traite les données manquantes, il faut le mettre en lien avec la méthode de machine learning. Sinon, ça n'a absolument pas d'intérêt. traiter une valeur manquante pour une ACP, c'est la même chose que traiter une valeur manquante quand on est sur une régression logistique ou quand on est avec un arbre de décision ou ainsi de sui\"), ('170029', \"d on est avec un arbre de décision ou ainsi de suite, ainsi de suite. C'est vraiment important ça, il n'y a pas, les recettes de cuisine, il n'y a rien de pire. Allons-y. Dans ce sens-là, bien sûr, tout le monde adore la cuisine, je n'ai rien contre la cuisine. Alors, ensuite, je récupère les dimensions, N c'est le nombre d'observations, P c'est le nombre de variables actives. Il y en a 6. Voilà, 28, 6. OK. Alors, rappel très rapide sur le cours. Comme les variables ne sont pas sur les mêmes uni\"), ('170030', \" Comme les variables ne sont pas sur les mêmes unités, et pourtant, là, dans le réseau, je vais les mettre ensemble, il ne faut pas que les variables qui ont une variance plus élevée tirent les résultats, eux, quand il va calculer le gradient. C'est très important, ça. Donc, du coup, non, non. Dans ce cadre-là, c'est important de standardiser les variables pour les ramen\"), ('180001', \"### Transfert Learning avec PyTorch\\n\\nBien, c'est parti. Dans cette vidéo, je vais parler de transfert learning avec PyTorch. Tout le monde a bien compris que je suis à fond dans PyTorch en ce moment, j'essaie d'explorer les différents aspects pour que je puisse en faire profiter mes étudiants, mais tout le monde en général, puisque ma vidéo est publique, tout le monde peut regarder.\\n\\n#### Qu'est-ce que le transfert learning ?\\n\\nLe transfert learning consiste à mixer deux modèles. Un modèle généri\"), ('180002', 'ng consiste à mixer deux modèles. Un modèle générique pré-entraîné sur de grands corpus, et sur lequel on va greffer un modèle spécifique au problème que nous souhaitons traiter. Dans mon tuto ici, je vais essayer de discerner les chiens et les chats, un problème extrêmement connu en vision par ordinateur avec les réseaux de neurones convolutifs.\\n\\n#### Problèmes en vision par ordinateur\\n\\nOften, when we are in computer vision, we create a model that is specific to our data. However, this poses se'), ('180003', \"at is specific to our data. However, this poses several problems. First, finding the right configuration of our neural network, especially the convolutional neural network, is a real challenge. How do I parameterize my different convolutional layers, my dense layer, and so on? That's a real problem. The other problem is, do I have enough data? Generally, we don't have enough data to create effective models, especially when it comes to labeled images, which can be expensive. So, either we have ge\"), ('180004', \"ges, which can be expensive. So, either we have generic images, or images specific to our problem, and labeling them can be complicated. Therefore, the first approach, which is extremely popular, is to create the model from scratch. I've made videos on that. In this video, I took Pokémon, Bidrill against Cubone, and I created the model from scratch. That's a first approach. There's a second approach, which is to use pre-trained models on large labeled datasets. Among the most famous of these dat\"), ('180005', \"beled datasets. Among the most famous of these datasets is ImageNet, and several people have created models that are usable in all circumstances. These models are not specific to your problem. In this video, I tried to classify a Fiat Barquetta from the 90s and a Jaguar, a cat, if you want. The idea is that often, when we create specific models, the problem of data preparation and feature construction arises. And that's exactly the role of convolutional layers here. But finding the right value f\"), ('180006', \"utional layers here. But finding the right value for the parameters is a real challenge, especially when we lack data. In any case, when we want to treat specific problems. So, the idea is to use pre-trained models to perform this preparatory phase. That's the idea.\\n\\n#### Importance de la phase préparatoire\\n\\nThis preparatory phase of data is fundamental for the quality of classifiers in machine learning. That's extremely known. You want to predict heart disease, you have height, weight, you need\"), ('180007', \"t heart disease, you have height, weight, you need to think about using BMI, the body mass index, if you want. And that, it won't find it alone, so you need to have professional knowledge for that. So, this phase of feature selection and feature construction is extremely important. And as for feature selection, we can automate it, but for feature construction, unless we use methods like Ayo, but then we don't understand what we are manipulating, often we don't know very well what to do. And the \"), ('180008', \"often we don't know very well what to do. And the idea here is to base ourselves on pre-trained models to have this feature construction. And in the new representation space, that's where I will define my model. I do this in NLP, in text mining, so in text data processing. I do this with my students, we use pre-trained models to try to have representations in a space of chosen dimension, of chosen size, with the famous embeddings. That's exactly what I did in a video, and I did it with my studen\"), ('180009', 'what I did in a video, and I did it with my students, in the summer, we use pre-trained models, and once we have loaded them, sometimes they are quite heavy, it can cost gigabytes sometimes, once we have these pre-trained models, we dive our data, our corpus, into this space, and we get a vectorized representation of our corpus, a numerical vector, and in this vectorized representation, we can apply machine learning methods, either predictive methods, or clustering methods, or other methods, and'), ('180010', 'hods, or clustering methods, or other methods, and so on. So, it\\'s the same in text processing, and it\\'s also the same in computer vision, which is what we\\'re going to try to see today.\\n\\n#### Application du transfert learning\\n\\nBecause I had discussed this with my students, they told me, \"Oh, we don\\'t see very well.\" So, I\\'m like everyone else, I go on the internet and I make queries, I make Google queries, and I found a tutorial on Python. So, I looked at the PyTorch tutorial, I looked at it in '), ('180011', 'looked at the PyTorch tutorial, I looked at it in detail, and I reproduced it. I do this every time I want to study a method that is implemented with a library, what happens? I look at the tutorial and I reproduce what is already online. Then, I need to be able to apply it to other domains, other data, but also other variants, to know and verify that I have understood it well. So, when I read this tutorial, I read it in detail, and I saw that in fact, it uses ResNet. You will see, the address is'), ('180012', \"fact, it uses ResNet. You will see, the address is there, on the PyTorch site. It uses ResNet. It's a bit tricky, but I stuck to it to understand it, but okay. So, you have seen, it uses ResNet here. It uses ResNet. Okay, I said, yes, but me, okay, ResNet, I have done it. Okay, I will use another pre-trained model, and in this video, I used ResNet and I also used VGZ19. So, I said, but can't I use VGZ19 for this? So, I looked at it, I made Google queries, and I realized that it's always like tha\"), ('180013', ' queries, and I realized that it\\'s always like that in reality, there are many people who talk about transfer learning and many, in fact, reproduce what is on the Python site. So, there too, it\\'s ResNet. Very well. Okay, we talk about transfer learning. Very well. And there too, there is a site that is not bad, with a tutorial that is not bad, but still, in transfer learning, it\\'s ResNet that is used. It\\'s written, you will see it, the guy says, \"Yes, I show you, but if you want the code, you ha'), ('180014', '\"Yes, I show you, but if you want the code, you have to write to me,\" but we know where he got it from, so it doesn\\'t suit me at all. I want my students to do treatments on specific images with parameters and models that I have chosen, which are not those that are found online. I absolutely want them to be able to transpose the idea. So, I looked again. And finally, I found one that is not bad, which is here, where in fact, it talks about VGG19. That allowed me to understand already what the str'), ('180015', 'That allowed me to understand already what the structure of VGG19 is. And among other things, I understood that VGG, in fact, is in two parts. One part that it calls features, which is the convolution part. And a part that is classifier, which is the dense part. So, we will take VGG19. So, the features part, we will not touch it. Here, the classifier part, we will only modify the connection between the penultimate layer and the output layer. So, the connection that was between the penultimate an'), ('180016', \"the connection that was between the penultimate and the output layer, we will remove it, and we will graft our part that allows us to do a binary classification, cats and dogs. That's what happens. I took the time to put colors. You have seen, it took me time. So, I will use this one of VGG19. We will not touch it. Then, the first layers of VGG19, we will not touch them. The only layer we will modify is the connection between the penultimate layer and the output layer, which this time, we will m\"), ('180017', 'r and the output layer, which this time, we will make it binary because we want to treat a binary problem. This schema is very important for understanding. All the features part, we will not touch it. The convolution, pooling part. Then, on the dense part, all the parts until the penultimate layer, we will not touch them either. The weights will remain as they are. The only thing we will modify is the connection between the penultimate layer and the output layer, where we will learn on our image'), ('180018', \"the output layer, where we will learn on our images, this time, the synaptic weights of this connection, which I put in red here. It's only the red part that we will modify. So, we start from an intermediate representation towards a binary output. That's good. We can see it as a form of preparation, of feature construction, in fact, as a preparation of variables, where we project the images into a 4096-dimensional space. That's good, we will know where this number comes from after. And then, we \"), ('180019', \" where this number comes from after. And then, we will graft a supplementary layer. We could have done a logistic regression, in fact. A logistic regression on a space defined by the layer and by the penultimate layer that is at 4096 dimensions. We have vectors for each image of 4096 values. That's what happens. Okay, I hope everyone has understood. Anyway, in the TD, we will come back to this, but that's really the idea, we will only graft a sub-part on something that already exists. The base o\"), ('180020', \"-part on something that already exists. The base of images that we will use is cats and dogs, I found it here, I loaded it here simply. So, I went there, I put the data on Kaggle, I had to register, but I didn't want to register if you want, so I searched a bit, I found it here, and I did a download simply. So, there are thousands of images here, but with my machine, we won't be able to process thousands of images with this tutorial here, so I created a small folder where I put my different elem\"), ('180021', \"eated a small folder where I put my different elements, they are there. So, what happens? In my folder, as usual, there is my tutorial, my notebook, that doesn't change, Gilles de these images that I put in two parts, very net, in the training part, I put the images of cats and dogs, two folders. The cats are there. There are 100 in total. Yes, because my machine is not powerful. And the dogs, there are 100 in total. That's the training part. Then, in the test part, the same, I put the cats, oth\"), ('180022', \"n, in the test part, the same, I put the cats, other cats, not the same as in the training part, we are in agreement. And other dogs, 100 as well. So, I have 200 observations for training. Half-half, half cats, half dogs. And 200 observations for testing. Otherwise, there are thousands if you want, the more data there is, the better the model's performance will be, that's for sure, but the longer the calculation time will be, and we won't be looking at each other like dogs and faïence, that's th\"), ('180023', \"ing at each other like dogs and faïence, that's the case, waiting for the calculations to pass, we are well in agreement on that, I love this expression, I don't understand everything it means, anyway, let's go. So, I prepared as usual my small notebook there, and we can do the treatments step by step. So, first, I load the libraries. I created an environment, as usual. I created an environment where I put, on the one hand, PyTorch, and I added Torchvision. That's the two main libraries. Then, I\"), ('180024', \"orchvision. That's the two main libraries. Then, I added others like Matplotlib, Pandas, and all that, for the different manipulations. But mainly, it was necessary to have PyTorch and Torchvision. So, how and all that, the versions, that I explained in the videos. I am at my fourth video on PyTorch, so it's good, it's good, it's enough to look. I advise you to look in parallel this video, where there, I start from scratch. And in this video, first, I will build the model from scratch, with perf\"), ('180025', \"st, I will build the model from scratch, with performances that will be pathetic, if you want, and I will explain why it's pathetic. And then, we will try to see what happens if we exploit the transfer learning, will we improve the performances by doing a minimum of additional treatment? That's the idea. So, I load the libraries as usual. Very well, so that's good. It's a bit short, but it's good. Then, I specify the default path. It's the demo folder that is there. Very well, demo that is there\"), ('180026', \"older that is there. Very well, demo that is there. Then, I tell him the transformation functions. So, I do transformations, there are several here. One, I resize, I redefine the definition, 224, 224, that's to be compatible in reality with VGG by the way, but that's an issue, the more it's small, the less it takes time to process, of course, but here I wanted to be compatible with VGG by the way. Then, I put it in tensor to make Python manipulate it, and then I normalize. Why? Because I saw tha\"), ('180027', \"e it, and then I normalize. Why? Because I saw that it did not converge at all, and in fact, the base values are between 0 and 1. And that's what the documentation says, I read the documentation, they say that for better learning, for better convergence, it's interesting to do a normalization, centered around 0.5, and then divided by that as standard deviation. And in reality, the consequence of that, you can verify it yourself, is that the values that initially vary between 0 and 1 will vary be\"), ('180028', \"s that initially vary between 0 and 1 will vary between -1 and +1. So, ReLU will work better. Let's go. So, once that's done, I will load the training data. So, there, it passed the training folder. That's the idea. It went into the image train folder, and it passed the training folder with the cats and dogs. That's what happens, if you have seen, in the default folder, I go into the image and I go into the train. Very well. And we have the classes here, there are two, cat and dog. And if I calc\"), ('180029', \"es here, there are two, cat and dog. And if I calculate the frequencies, there are 100 that are cats, and 100 that are dogs. Very well, so far, it's good. Then, to be able to do the training from scratch, I create an iterator that allows us to go through the training base. I put a batch size equal to 1 always to process the images one by one and to have a visibility on what we are manipulating. When we want to process large bases, I tell you, it's necessary to increase the batch size, but then t\"), ('180030', \"s necessary to increase the batch size, but then the calculations are more complex. That's also something to keep in mind. Very well. I do a shuffle to not be influenced by the order of the data in the training base, simply. Otherwise, it will take all the cats first, then all the dogs, then it will take all the cats again, then all the dogs, it's just impossible. It will oscillate. So, for that, it will shuffle. Then, to see if my iterator works, I create a Python iterator, I take the next one \"), ('180031', \", I create a Python iterator, I take the next one and I look at the image title. It's good, it's a tensor and I display it. Don't forget that in the tensor, it puts the label in the first dimension, and then it puts the inverse, in fact, it puts the channels in the first. So, that's why we need to transpose here, to put it in the right order. Look well at this video, it explains it in detail. So, I won't come back on that. Look well in this video, why it's necessary to transpose the different di\"), ('180032', \", why it's necessary to transpose the different dimensions of your tensor to be compatible with Matplotlib. So, that's it. Once that's good, I display the image. And we have a dog. Very well. And its label is 1. And the label 1 corresponds to Dog. Very well. So, now, I will do the training from scratch. That is to say, I will do this video. So, I allow myself to go fast. You are well in agreement on that. Very well. It's exactly the same idea as here. Except that instead of Pokémon, we will use \"), ('180033', 'here. Except that instead of Pokémon, we will use cats and dogs. Let\\'s go, so I stop saying \"so\". I have defined the pooling layer, the convolution and pooling layer, already there, how to do it, it\\'s always a question that remains, you will see with your computer vision course, then to calculate here the number of neurons in the flatten layer, there is an online calculator, I talked about it in this video, so I won\\'t talk about it today, and then in the dense layer, I put several layers. The de'), ('180034', \"n in the dense layer, I put several layers. The definition of the different layers remains a real problem. That's a real problem. It's necessary to have good knowledge. That's why you have a computer vision course. Then, that's the initialization of the tools. And in the forward part, I define the sequence. Again, I will go very fast here. Why? Because all that is explained in detail in this part. We are in agreement on that. Once that's done, I instantiate my convolutional neural network. Once \"), ('180035', \"instantiate my convolutional neural network. Once it's instantiated, I need to parameterize the training. I need to fix the training parameters. There are two elements. One, the loss function. And two, it's the optimization algorithm. There, I use the cross-entropy. That's what allows me not to have to normalize here in softmax. Otherwise, it would be necessary to normalize here. But here, we take the raw values that are in the form of logit. In any case, they are degrees of belonging, but not n\"), ('180036', \"any case, they are degrees of belonging, but not normalized, so the sum does not make 1 and we can have negative values. That's why I use this loss function here that allows me to authorize that here this output here. Then, here, the optimization algorithm, I use simply a gradient descent, a variant of stochastic gradient descent, which is an optimized quality, better than the classical stochastic gradient descent, if you want. I pass it here the parameters of my network. Once that's done, I wil\"), ('180037', \" parameters of my network. Once that's done, I will launch the training. It will take a little time, but that's also explained in detail. I fix the number of passes on the entire base. Then, I create here a vector that allows us to store the different costs. And then, I iterate on each image and I retrieve the image. I reset the gradients, then I do a forward on the image to have the output of the network, and I compare, on the one hand, the output of the network, and the true label of belonging\"), ('180038', \"ut of the network, and the true label of belonging, which is the index here. Once I have calculated the loss, I backpropagate the error on the synaptic weights, with a backward here. That's the calculation of the gradients, and then, it updates the synaptic weights, that's the role of the step. And the loss, I add it, because I want to add all the iterations, the losses of the iterations on the training base. Once that's passed, this loss, I store it. Then, I launch it. It will take a little tim\"), ('180039', \"e it. Then, I launch it. It will take a little time. But it's good. It's in the process of iterating. But still, look well in detail here. All that is explained. And like in this video, I will put the different aspects. So, you have all the elements. It's in the process of descending. I will interrupt the video and I will resume just after, and that's it, it has finished the training and we have the loss function's convergence. Okay, now, what is the value of this descent? It's almost good, it's\"), ('180040', \" the value of this descent? It's almost good, it's descending towards 0, so we say it's not bad, that's good. Let's try to see what it gives on the test base. The test base, remember, it's there, and it's other cats and other dogs, you are in agreement, very well, so, again, here, I load the test base. Then, I create the iterator, very well, with a batch equal to 1, it's not necessary to shuffle here, but anyway, it's not bad if you put it by default, it's not bad. Once that's good, boom, I take\"), ('180041', \"ault, it's not bad. Once that's good, boom, I take each image, I retrieve the image and the label, I apply the network on it, I retrieve the class label, and at the same time, I retrieve the corresponding label. And once I have retrieved the two, I put them in a list. And I display the two lists. That's also going to go quite fast. Very well. Once that's good, I create the confusion matrix. I create the confusion matrix. That's not good. Very well, very well, very well, let's calculate the accur\"), ('180042', \"l, very well, very well, let's calculate the accuracy. And it doesn't do it. 57%. Very well, 57% of accuracy, of course, it doesn't do it. So, what can explain that? The first reason is the configuration of my network. What makes me say that it's good? It's for that that we need the course to understand what we are manipulating. We are absolutely in agreement on that. Very well. Again, this part, features, it has a double aspect. 1. to highlight the relevant information of the image 2. to reduce\"), ('180043', \"the relevant information of the image 2. to reduce the dimension, and that's a lot of trial and error. So, I looked, there's a guy who's not bad, it's Jason Brunley, who does a lot of tutorials, very interesting, I always look at his tutorials, and he treats cats and dogs and shows the different networks he uses, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on\"), ('180044', ' so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on, and so on'), ('190001', \"Bien, c'est parti. Alors, l'image paraît un peu sibylline. En réalité, dans cette vidéo, je vais parler des modèles pré-entraînés. Alors, faisons un petit recul en arrière pour qu'on arrive bien à situer les choses.\\n\\nLa valorisation de la data, elle est ancienne. Les statistiques, ça existe depuis déjà un moment. La pratique statistique existe depuis un moment. En économétrie, on faisait de la modélisation depuis un moment déjà. Tout simplement, ce qui change un peu au fil des années, c'est les \"), ('190002', \"ce qui change un peu au fil des années, c'est les pratiques, les objectifs, les cas d'application et les technologies. Il y a aussi une forme d'abondance de données qui n'existait pas avant. Quand moi-même j'étais étudiant en économie, il y avait toujours des belles théories, puis après, on n'avait pas les données pour faire les modèles, donc on s'arrêtait là. On est dans une ère différente maintenant, on a plein de données pour tout et n'importe quoi, et il faut faire quelque chose là-dessus.\\n\\n\"), ('190003', \" quoi, et il faut faire quelque chose là-dessus.\\n\\nCe qui est assez prégnant, c'est les modèles pré-entraînés sur des gros corpus de données. Notamment pour les données textuelles, les LLM, Large Language Models, sont des modèles pré-entraînés sur des énormes corpus disponibles sur Internet. Il y a plein de documents textuels sur Internet. Il suffit d'aller voir. Je rigole tout le temps quand j'en parle à mes étudiants. Je parle des commentaires sur le site L'Equipe. À la fin de chaque article, o\"), ('190004', \"ur le site L'Equipe. À la fin de chaque article, on pourrait faire une thèse là-dessus. Une très longue thèse. Voilà, donc les documents textuels avec des applications très connues actuellement, comme ChatGPT et ainsi de suite, l'IA générative pour le texte. Il y en a aussi l'IA générative pour d'autres choses, pour des images, pour tout et n'importe quoi. Et d'autres documents également qui sont utilisés, c'est les images étiquetées, étiquetées par des gens à l'autre bout du monde qui sont payé\"), ('190005', \"par des gens à l'autre bout du monde qui sont payés au lance-pierre sans protection sociale. Il y a aussi des choses à dire là-dessus, mais bon, très bien. Alors, ces images-là, du coup, c'est une forme de base de connaissance, c'est une base de données sur laquelle on peut appliquer des énormes modèles prédictifs, souvent des réseaux de neurones, des deep learnings, des réseaux de convolution, enfin des réseaux de neurones convolutifs généralement, qui permettent justement d'avoir une forme de \"), ('190006', \"nt, qui permettent justement d'avoir une forme de connaissance universelle et sur laquelle on peut présenter une image supplémentaire et il va essayer de la classer automatiquement.\\n\\nAlors, justement, pour s'amuser un peu, nous, on va mettre en œuvre les modèles pré-entraînés dans PyTorch. Je n'ai pas d'action dans PyTorch, mais vous avez bien compris qu'en ce moment, j'explore énormément PyTorch. Et je vais lui présenter deux images. Alors, une première image, c'est une Fiat Barqueta. Bon, ce f\"), ('190007', \"première image, c'est une Fiat Barqueta. Bon, ce fut un temps que les moins de 20 ans ne peuvent pas connaître, bien sûr, à un moment donné, Fiat a sorti des voitures fun, voilà, voilà, voilà, mais bon, aussi maintenant, notamment la Fiat 124, mais dans les milieux des années 90, voilà, très bien, ils ont sorti coup sur coup la Fiat Barchetta qu'on a devant nous, et la Fiat Coupé, sur des voitures assez sympas, très très sympas, avec des couleurs pimpantes d'ailleurs. Donc, je vais prendre l'ima\"), ('190008', \" pimpantes d'ailleurs. Donc, je vais prendre l'image, je vais la présenter au modèle pré-entraîné, et on va voir ce qu'il va dire. S'il me dit Fiat Parqueta, je vais tomber par terre. Parce que ça veut dire que ça a été étiqueté en tant que tel. Et en fait, non. Même s'il y en a beaucoup, des étiquettes, elles sont bien limitées quand même. Déjà, s'il reconnaît que c'est un cabriolet, ce serait pas mal. S'il reconnaît que c'est une voiture, ce serait déjà pas mal aussi. Ça, c'est une première im\"), ('190009', \"rait déjà pas mal aussi. Ça, c'est une première image. Ensuite, on va présenter une deuxième image. C'est non pas une Jaguar, mais un jaguar, l'animal. Bon, là, il y a un Allemand, j'espère qu'il va me dire que c'est un animal, un félin, au moins un félin, peut-être jaguar. Bon, s'il me dit que c'est un chat ou un chien, là, ça sera discutable. Mais l'idée, c'est ça, c'est qu'on a créé des modèles qu'on a entraînés sur des grandes bases pré-étiquetées, qui sont étiquetées, on a une forme de conn\"), ('190010', \"etées, qui sont étiquetées, on a une forme de connaissance universelle, et on peut nous présenter nos données pour qu'ils puissent les classer automatiquement. Et ça a beaucoup d'applications, ça, ça a beaucoup d'applications.\\n\\nAlors, je vais fermer mes deux images là et je vais parler de la documentation que j'ai utilisée. Alors, j'ai regardé sur PyTorch, tout simplement. Sur PyTorch, il y a beaucoup de choses très intéressantes. Maintenant que j'ai décidé de regarder de près le deep learning p\"), ('190011', \"j'ai décidé de regarder de près le deep learning pour essayer de faire des ateliers là-dessus, pour préparer les étudiants au cours de computer vision, voilà, je suis allé sur PyTorch et j'ai regardé ce qui était disponible. Donc, j'ai lu en long, en large, la doc de PyTorch. Je connais quand même pas mal de choses sur PyTorch maintenant. Et à un moment donné, il y a un chapitre sur les modèles pré-entraînés. OK, j'ai regardé, il y a beaucoup d'applications possibles. Et parmi les applications, \"), ('190012', \"pplications possibles. Et parmi les applications, il y a la classification, le classement, si vous voulez. Donc, la classification supervisée. Donc, j'ai regardé, il y a différents modèles. Très bien, donc j'ai regardé également, très bien, qu'on peut utiliser. Et ils ont mis un tuto sur ResNet, qui est un des modèles possibles. Alors, ResNet est là. Je suis regardé là, donc, également. Comment l'utiliser? En m'inspirant du tuto qui est là. En explicitant chaque étape, parce que c'était un peu s\"), ('190013', \"plicitant chaque étape, parce que c'était un peu si bilingue, quand même. C'était même un peu obscur, en fait. Mais en détaillant chaque étape, j'ai fini par bien comprendre ce qui se passe. Voilà, ResNet qui est là. Là, il y a un tuto qui en parle pas mal. Je trouve très bien. J'ai mis l'adresse, là. Voilà. Donc, un tuto qui en parle pas mal. De l'architecture, de comment ça a été fait, qu'est-ce qu'il y a derrière, ainsi de suite. Je vous conseille de le lire. Moi, je l'ai lu en tout cas. Une \"), ('190014', \"ille de le lire. Moi, je l'ai lu en tout cas. Une fois que j'ai réussi à reproduire ce tuto-là sur mon image, je me suis dit qu'il faut que j'essaie de trouver un autre modèle pour voir également si c'est possible d'avoir les mêmes choses. J'ai réappliqué. C'est ça qui est intéressant. Déjà, si on reproduit ce qu'on trouve sur Internet, ce n'est pas mal. Mais si on arrive à le mettre en œuvre sur un autre contexte, soit un nouveau modèle, soit d'autres données, ce serait pas mal également. J'ai \"), ('190015', \"autres données, ce serait pas mal également. J'ai essayé également, VGZ19, et VGG19, c'est un autre modèle pré-entraîné qui est disponible en ligne. Et là également, j'ai trouvé une petite documentation qui en parle pas mal. J'ai mis ici M. Bosch, là, voilà, Godens Bosch, qui en parle pas mal de ce qu'il y a derrière, ainsi de suite. Voilà, regardez. Mais généralement, c'est des réseaux de neurones convolutifs, tout simplement. C'est vraiment parce qu'il y a aussi un autre aspect, peut-être qu'o\"), ('190016', \"ce qu'il y a aussi un autre aspect, peut-être qu'on va voir, je suis en train de regarder là-dessus, c'est le transfert learning. C'est-à-dire que je crée un peu, il y a des modèles pré-entraînés et est-ce que je peux le raffiner pour mon traitement à moi? C'est une application qui paraît assez intéressante. Il y a un dernier que j'ai regardé et que j'ai appliqué également, mais je ne vais pas le reproduire ici parce que finalement, c'est exactement la même chose. Je suis allé avoir ici MobileNe\"), ('190017', \"ent la même chose. Je suis allé avoir ici MobileNet qui est plutôt utilisé pour les systèmes embarqués, donc un modèle plus léger en réalité. Là également, j'ai trouvé un article qui en parle, j'ai lu en détail également, mais une fois qu'on a fait sur l'un et qu'on arrive à reproduire sur l'autre, voilà, faire un troisième, c'est la même chose. Donc, c'est pour ça que je ne l'ai pas fait dans ma vidéo. Donc, l'idée, c'est quoi? C'est qu'on va travailler sur des modèles pré-entraînés pour essaye\"), ('190018', \"availler sur des modèles pré-entraînés pour essayer de classer les deux images. La Fiat Barquetta d'un côté, la voiture décapotable, et de l'autre côté, le jaguar, l'animal, le félin. Très bien. On va voir s'il les reconnaît automatiquement. Ensuite, une fois qu'on a reproduit ça sur notre image, on va essayer de voir est-ce que je peux également mettre la démarche en œuvre la démarche avec VGZ19. C'est ça l'idée globale. Alors, très rapidement, mon répertoire de travail est là. Il y a mes deux \"), ('190019', \"mon répertoire de travail est là. Il y a mes deux images, la voiture, l'animal et mon notebook que je vais démarrer maintenant. Alors, mon notebook, il est là. Comme d'habitude, on va essayer de voir déjà les images et puis voir comment on lit les images et qu'est-ce qu'on peut faire avec. C'est ça l'intérêt. Alors là, j'en parle plus parce que ça fait plusieurs vidéos que j'en fais, donc je ne vais plus aller dans le détail. J'ai créé un environnement PyTorch, avec Python 3.11, et ensuite j'ai \"), ('190020', \"nement PyTorch, avec Python 3.11, et ensuite j'ai installé tout simplement les packages PyTorch, les différentes versions de PyTorch d'un côté, enfin la version du mois de mai 2024, d'un côté, et Torch Vision. Donc c'est les deux packages principalement que j'ai installés. Ensuite j'ai également mis ma toplib pour pouvoir faire afficher les images, mais principalement, il y a PyTorch et TorchVision. Je spécifie le répertoire de travail. Une fois que c'est fait ça, je vais charger l'image. Déjà, \"), ('190021', \"que c'est fait ça, je vais charger l'image. Déjà, pour charger l'image, j'utilise ReadImage de TorchVision. Vous avez vu, dans torchvision.io, j'utilise ReadImage. Je m'attends à ce qu'il mette au format qu'attend TorchVision. Il va mettre un tenseur directement. Du coup, il faut essayer de voir ce qui se passe avec ça. Je charge l'image, très bien, et j'affiche son type. Effectivement, on a déjà un tenseur. Il faut le savoir. J'affiche les dimensions. On a ici la hauteur, premier chiffre, et le\"), ('190022', \"sions. On a ici la hauteur, premier chiffre, et le deuxième chiffre, c'est la largeur. Essayons de voir. Je suis allé là, propriété, et dans les détails, vous avez la hauteur qui est là, 2304 et la largeur 3456. Et il y a 3 canaux. 3 x 8, 24. Pour fond de couleur, il y a bien 3 canaux pour les couleurs rouge, vert, bleu. Alors ce qui pose problème, c'est que ce n'est pas un problème en soi. C'est que c'est des intenceurs, ce n'est pas un vecteur numpy, ce n'est pas une matrice numpy. Et il met l\"), ('190023', \"numpy, ce n'est pas une matrice numpy. Et il met les canaux en première position. Parce que PyTorch fonctionne comme ça. Or nous, si on veut l'afficher avec PyPlot, avec Matplotlib, il ne sait pas faire ça. Il a besoin que les canaux soient en dernière position donc c'est à nous de voir ça donc c'est ce qui se passe ici. Donc déjà, je transforme l'image en matrice numpy et je transpose des axes pour que ici la hauteur soit en première position. Ensuite, qu'en deuxième position, on ait la largeur\"), ('190024', \"nsuite, qu'en deuxième position, on ait la largeur et ensuite, en dernière position, c'est les canaux. Rien que ça déjà, c'est source à problème. Souvent, les étudiants manipulent les données, ils ne savent pas très bien quelles sont les structures, mais moi je leur dis tout le temps, faites type, faites type. En faisant type, vous avez déjà le type de données, et ensuite faites dire pour avoir la liste des propriétés méthode. Et ensuite, regardez la documentation. Regardez la documentation. On \"), ('190025', \"z la documentation. Regardez la documentation. On ne peut pas fonctionner aujourd'hui avec les outils qu'on a, avec la richesse des outils qu'on a, sans regarder la documentation. C'est impossible, c'est impossible. Une fois que j'ai mis dans le bon nombre, du coup, voilà, je peux afficher l'image, et on a bien la Fiat Barqueta qui est là. Très bonne voiture, très amusante. Alors du coup, maintenant mon idée à moi, c'est de reproduire ce tuto-là en détaillant chaque étape, parce que là, bon, il \"), ('190026', \"en détaillant chaque étape, parce que là, bon, il faut s'accrocher, sur mon image. Et on va voir ce qu'il va reconnaître en utilisant les classes pré-entraînées. Alors, je ne l'ai pas dit, mais je dis maintenant, souvent, les modèles pré-entraînés sont entraînés sur ImageNet, qui est un database qui est disponible en ligne d'images étiquetées et qui a servi à plusieurs challenges. Donc, nous, les deux outils qu'on va voir, que ce soit ResNet ou VGG, ils ont été entraînés sur ImageNet en réalité.\"), ('190027', \"GG, ils ont été entraînés sur ImageNet en réalité. Et on a du coup 1000 classes. 1000 classes prédéfinies. Donc pour savoir quelles sont ces mille classes prédéfinies là, il faut regarder sur internet. Il faut regarder sur internet, regarder quelles sont les classes prédéfinies d'Imagenet. J'en afficherai quelques-unes, mais je ne vais pas afficher les mille, on ne va pas les détailler les mille. Voilà, mais ça, ça fait partie des éléments qui sont très importants. J'aurais dû en parler dès le d\"), ('190028', \"nt très importants. J'aurais dû en parler dès le départ. Alors, une fois que c'est ça là, donc on fait la liste des modèles disponibles. En fait, ils sont dans la doc ici, les modèles disponibles. On avait là, très bien. Moi, je vais les afficher également. donc j'ai récupéré les modèles d'entourgé vision et je fais un dire des modèles on a les différents modèles à l'ex net on trouve des tutos sur internet il ya res net et nous on va voir deux c'est res net et vgc 19 alors je charge maintenant r\"), ('190029', \"est res net et vgc 19 alors je charge maintenant res net 50 alors qu'est ce qui se passe j'importe les outils et les poids parce qu'en réalité on a des modèles préentraînés qui ont certaines structures et dans les structures on a les coefficients qu'ils appellent poids synaptique donc ces points là il faut les sauvegarder aussi avec le modèle donc c'est très bien d'appeler la structure mais il faut nourrir la structure avec les poids synaptiques avec les coefficients de votre zone de neurones de\"), ('190030', \"avec les coefficients de votre zone de neurones de votre zone de neurones convolutif et donc c'est pas là il faut les charger il faut les chercher donc la première exécution en réalité il va charger en ligne sur paytors hub voilà les points on va les voir tout à l'heure donc j'appelle les outils j'appelle les poids et je prends les meilleurs points en fait voilà c'est default ici c'est les meilleurs pas pour ce n'est pas ce qu'il ya des versions et des versions donc j'instancie la classe resnet \"), ('190031', \"et des versions donc j'instancie la classe resnet et je lui dis de charger les poids par défaut de resnet des meilleurs et ensuite le modèle je le passe en mode évaluation pour pouvoir faire de la prédiction c'est ça l'idée 1 le eval la permet de le tourner de manière à ce qu'on puisse faire des prédictions avec voilà très bien donc là vous avez la structure je vais pas tout détailler également mais vous pouvez regarder un vous allez tout simplement dans texte éditor et vous avez le détail du mo\"), ('190032', \"ent dans texte éditor et vous avez le détail du modèle mais on voit bien que c'est un réseau de neurones convolutif alors j'avais dit que les poils les charges mais là on dirait qu'ils les chargent pas là mais parce que pourquoi j'ai déjà exécuté mon j'ai déjà exécuté mon notebook donc il a déjà chargé une fois il l'a mis dans un cache et donc la prochaine exécution ils ne la chargent plus. Alors, ils sont où? Ces points-là, ils sont pour ma machine à moi, ils sont à cet endroit-là. Vous avez vu\"), ('190033', \"ine à moi, ils sont à cet endroit-là. Vous avez vu, c'est sur ma machine, perso, à la maison. J'ai le dossier utilisateur de Windows, j'utilise Windows parce que l'université utilise Windows, simplement. Ensuite, Rico, c'est moi. Il a créé un dossier cache, voilà, caché, point cache, torche, hub, parce qu'il est allé chercher le hub de PyTorch, de Torch, justement, voilà, checkpoint, et vous avez les poids synaptiques des différents modèles. Donc celui qui l'a chargé, on voit bien la puissance d\"), ('190034', \" celui qui l'a chargé, on voit bien la puissance des modèles ici. Voilà, il y en a un, il fait 100 mégas, c'est ResNet. Il y a VGG19 qui est différent, c'est un modèle différent, un autre zone de neurones convolutifs différent, et les poids synaptiques font 500 mégas cette fois-ci. En considérant qu'ils sont plus puissants puisqu'il est plus complexe. Mais bon, il y a plein d'articles qui en parlent justement sur Internet, je vous en jure à regarder. Alors il y en avait un troisième que j'ai tes\"), ('190035', \"der. Alors il y en avait un troisième que j'ai testé qui a marché, mais je ne vais pas faire la vidéo. c'est que j'avais testé également ici MobileNet qui sont utilisés pour les systèmes embarqués. Et là, il faut qu'ils soient moins complexes, surtout pour qu'ils puissent être installés sur des machines moins puissantes. Et on voit bien qu'effectivement, la taille des poids synaptiques, la taille du fichier des poids synaptiques, c'est des fichiers binaires, il est moins important ici. Il fait 2\"), ('190036', \"rs binaires, il est moins important ici. Il fait 21 mégas. On voit bien que c'est bien défini pour les systèmes embarqués. Mais ça a marché sur mon Windows. Je ne peux plus le faire fonctionner. Voilà. Donc, dans la première étape, j'installe ici le modèle et je vais chercher sur Internet les poids du modèle, c'est-à-dire les coefficients associés aux différentes couches de votre réseau de neurones. Et il les met en cache ici. Donc, la taille des poids, la taille des fichiers des poids, déjà, ça\"), ('190037', \" poids, la taille des fichiers des poids, déjà, ça vous donne l'idée de la performance, peut-être. Mais bon, parfois, on peut faire plus petit et aussi performant. C'est tout à fait possible, ça. Il faut lire les articles. moi je me suis vraiment alors il y avait les liens sur les articles originaux voilà mais j'ai trouvé des tutos qui en parlent de manière un peu plus simple de manière plus plus accessible si vous voulez, donc c'est pour ça que j'ai plutôt mis ces liens là celui là pour Snet et\"), ('190038', \"j'ai plutôt mis ces liens là celui là pour Snet et celui là pour VGG voilà, mais sinon vous avez les liens sur les articles originaux ici voilà sur Axive voilà, vous les avez là mais bon je fasse les paluchés après mais bon Si on veut vraiment aller loin, il faut le faire. On ne peut pas faire que de la pratique. Il faut comprendre ce qu'on manipule. Et là, les aspects mathématiques et théoriques sont très importants. Il faut les deux. Bien. Alors, une fois que c'est fait, du coup, je mets en œu\"), ('190039', \"s, une fois que c'est fait, du coup, je mets en œuvre un outil pour préparer les données pour que je puisse les présenter à ResNet. C'est ça qui est très important. Vous avez vu, là, mon image, ici, il a une dimension de 2304 en hauteur et 3456 en largeur. Si je prends la deuxième image, la deuxième image est de taille différente. Vous avez vu, ici la largeur c'est 1280 et la hauteur c'est 925. Et pourtant je veux présenter ces deux images différentes-là au même réseau. Forcément, ça ne peut pas\"), ('190040', \"entes-là au même réseau. Forcément, ça ne peut pas marcher. Donc, il faut qu'on prépare les données, l'image, on la retaille, on la redéfinisse de manière à ce que le réseau puisse fonctionner dessus. C'est ça l'idée. Donc, il y a une forme d'harmonisation à faire. Et ça également, il faut trouver l'outil qui le fasse pour nous. Donc, ce qu'on regarde ici tout simplement, c'est que ici, à partir des poids qui ont été chargés en ligne, on fait appel à la fonction transform voilà, du coup j'ai une\"), ('190041', \"el à la fonction transform voilà, du coup j'ai une fonction maintenant et cette fonction là je l'applique sur mon image regardez maintenant la taille alors il a rajouté une dimension c'est le rôle de handsqueeze ici il a rajouté une dimension, c'est la dimension du batch en fait la dimension du batch pourquoi? parce qu'on peut prendre pour des questions de performance on peut prendre une série d'images, les empiler et les traiter en bloc voilà, c'est l'idée du batch Très bien. À ce moment-là, qu\"), ('190042', \"'est l'idée du batch Très bien. À ce moment-là, quand on fait une prédiction, on fait une prédiction sur chaque image empilée de votre bloc, pour plus de rapidité. Alors ça, c'est très bien. Il faut une dimension supplémentaire parce qu'on va empiler des images. Moi ici, je n'ai qu'une seule image. Mais néanmoins, il a besoin qu'on ait une bonne structure pour pouvoir appliquer. Donc, il a créé la dimension ici, la quatrième dimension, si vous voulez, avec un seul élément parce que je n'ai qu'un\"), ('190043', \"ulez, avec un seul élément parce que je n'ai qu'une seule image. C'est le hall du Run Squeeze. Donc, j'ai pré-processé l'image et j'ai rajouté une dimension. Alors, dans le pré-processing, qu'est-ce qui se passe? Voilà, il a retaillé l'image, 224, 224. Donc, avec ResNet, si on veut faire la prédiction, il faut que l'image soit de taille 224, 224, sinon le réseau ne peut pas s'appliquer. Or, mon image, à la base, elle était de 23456. donc il a failli qu'il la redimensionne ça c'est vraiment très \"), ('190044', \"lli qu'il la redimensionne ça c'est vraiment très important c'est là souvent qu'il y a des pierres d'achoppement c'est là souvent que les étudiants perdent du temps parce qu'ils semblent avoir tout fait bien bien faire comme il faut mais à un moment donné ça ne marche pas ça ne marche pas parce que les formats ne sont pas corrects tout simplement, c'est pas ce qu'attend l'outil n'oubliez pas les outils ont été programmés par des gens comme vous et moi et donc ils sont des présupposés sur la stru\"), ('190045', \"t moi et donc ils sont des présupposés sur la structure de données qu'on doit leur présenter et ça passe par le pré-processing et donc pour que restnet fonctionne il faut que l'image se détecte 224, 224 c'est ce qu'il a fait ici qu'est-ce qui se passe alors? une fois que c'est bon là mon modèle qui a été instancié  Je vais instancer ici, la modèle res. Voilà, je l'applique sur mon image. Et j'enlève la dimension, voilà, et j'applique un softmax. Alors softmax, pourquoi? Parce que je veux avoir d\"), ('190046', \"Alors softmax, pourquoi? Parce que je veux avoir des probas d'appartenance à la sortie. Ils vous calculent un degré d'appartenance, mais qui n'est pas une proba. Donc on peut avoir des valeurs positives et négatives. Et la somme ne fait pas 1. Et moi, c'est plus lisible, c'est plus compréhensible pour nous d'avoir des probas d'appartenance en réalité. Donc c'est pour ça qu'on fait la prédiction sur notre image, ensuite j'enlève la dimension 1, et ensuite je fais un softmax, et j'ai un vecteur de\"), ('190047', \" ensuite je fais un softmax, et j'ai un vecteur de prédiction. Vous savez bien, on a un vecteur de prédiction. Un vecteur de prédiction de taille 1000. Alors pourquoi 1000? Parce que les images pré-étiquetées dans Images Net, il y a 1000 catégories. Vous avez vu, il était là le truc. Dans ImageNet, les images sont de 1000 catégories et donc il a donné les degrés d'appartenance aux 1000 catégories. ImageNet est une base de référence. Je vous enjoins vraiment à regarder la documentation d'ImageNet\"), ('190048', \"ns vraiment à regarder la documentation d'ImageNet. ImageNet, c'est la base source qui sert beaucoup à des modèles pré-entonés en classement d'images en computer vision. Une fois que c'est fait ça, du coup, regardons les différentes catégories. Je ne vais pas afficher les 1000, j'en affiche que les 10 premières. Voilà les différentes catégories qui sont disponibles sur Imagenet. Encore une fois, il y en a 1000 en tout, là j'en affiche que les 10. Il y a Stingray, ça rappelle les voitures aussi, \"), ('190049', \" Il y a Stingray, ça rappelle les voitures aussi, une corvette Stingray, Electricray, Tiger Shark, ainsi de suite. Qu'est-ce qui se passe alors? De ce vecteur-là, des probes à d'appartenance, je vais identifier la position du maximum. Je récupère la valeur maximum et je vais identifier la position du maximum pour savoir quelle est la classe correspondante. C'est ce qui se passe ici. Donc là, j'identifie la classe qui correspond au maximum dans le vecteur. Donc la classe la plus probable, si vous\"), ('190050', \" vecteur. Donc la classe la plus probable, si vous voulez, puisqu'on a fait une normalisation softmax. Et ensuite, je récupère le score correspondant, donc la valeur numérique qui est dans le vecteur correspondant. Ensuite, en me basant sur l'ensemble des catégories disponibles, je récupère son numéro de classe. J'applique le numéro de classe pour avoir la catégorie correspondante. Et j'affiche le nom de catégorie, ici correspondant à l'identifiant de la classe, et le score qui était obtenu dans\"), ('190051', \"nt de la classe, et le score qui était obtenu dans le vecteur. Et il me dit qu'à 45%, c'est inconvertible. Oui, parce que dans les images, images nettes, j'imagine qu'il n'a pas reconnu toutes les voitures possibles. Ce serait une possibilité d'ailleurs, si vous voulez raffiner votre modèle, d'identifier les voitures qui sont les cabriolets des années 90. Il y a quoi? Il y a la Z3, BMW Z3, il y a la Porsche Boxster, il y a fin 90, la Mazda MX-5. enfin, je ne vais pas faire la lésion. J'aurais pu\"), ('190052', \"enfin, je ne vais pas faire la lésion. J'aurais pu faire aussi une vidéo sur les voitures, mais non, on va rester dans le domaine qu'on maîtrise bien, bien que je m'y connaisse un peu en voiture. Ça, c'était pour ResNet. Et donc, il a bien reconnu que c'est un cabriolet convertible, c'est cabriolet. Regardons comment fonctionne VGG 19. De la même manière, je récupère le modèle que je vais utiliser, voilà, la structure, et je récupère les poids correspondants. les points par défaut pour ceux qui \"), ('190053', \"rrespondants. les points par défaut pour ceux qui sont à la meilleure performance. Voilà. Donc là, c'est instantiation. Instantiation. Et je passe un mot d'évaluation pour pouvoir faire des prédictions. Ensuite, l'autre élément important, j'utilise l'outil pour pouvoir faire les transformations de manière à ce qu'on présente les images dans les bonnes dimensions pour VGG19 cette fois-ci. Voilà. Et je fais la transformation sur l'image. Donc j'applique ça, et on se rend compte que pour VGZ19, il \"), ('190054', \"lique ça, et on se rend compte que pour VGZ19, il faut également que les images se détaillent 224, 224. Bien, alors rappelez-vous que la première fois que j'ai lancé cette partie du code-là, quand j'ai instancié, qu'est-ce qui se passe? Il est allé chercher sur Internet les poids synaptiques, les coefficients du réseau de neurones. Et dans un fichier qui est là. On voit bien ici, dans un fichier qui est là. Et donc, pour VGG19, l'ensemble des points tient dans un fichier qui fait à peu près 500 \"), ('190055', \"nts tient dans un fichier qui fait à peu près 500 mégas. Voilà ce qu'il fallait comprendre. Mais une fois seulement, donc au premier lancement, une fois qu'il l'a mis en cache sur votre machine, donc en cache localement sur votre machine, quand vous instanciez, vous avez vu, il n'a eu aucun chargement, ça marchait directement. Ensuite, je fais la prédiction sur l'image qui a été mise en mode batch, voilà, j'enlève le batch après, et je fais softmax pour voir les probas d'appartenance, on a les p\"), ('190056', \"ax pour voir les probas d'appartenance, on a les probas ici d'appartenance, de nouveau on a 1000 parce que la base source, la base étiquetée c'était image nette et qu'il y a 1000 catégories sur image nette, et ensuite je fais la prédiction, et cette fois-ci il me dit que c'est un cabriolet, mais avec plus de certitude cette fois-ci, la proba d'appartenance est de 96%, alors que tout à l'heure la certitude était moindre, 45%, La propre appartenance était de 45%. Là, il donne la même conclusion, l\"), ('190057', \"e était de 45%. Là, il donne la même conclusion, la bonne en l'occurrence, mais avec plus de certitude. Très bien, je me suis dit, ça marche. Maintenant, on va voir ce qui se passe si je propose l'image 2, qui est le Jaguar, et non pas la Jaguar, ce n'est pas une voiture, on est d'accord, c'est l'animal cette fois-ci. On va voir ce qui se passe. Alors, je reprends ici mon tuto, j'effage toutes les sorties, et à la place d'image 1, je mets image 2. Voilà. Qu'est-ce qu'il me reste à faire? J'exécu\"), ('190058', \". Voilà. Qu'est-ce qu'il me reste à faire? J'exécute tout de nouveau. Boum, j'exécute, voilà, comme tout est déjà en mémoire, donc ça va assez rapidement. Vous avez vu que la dimension de l'image a changé. Ça, c'est la hauteur, ça, c'est la largeur. Ce n'est pas comme la Fiat Barqueta tout à l'heure. Il y a toujours trois canaux, parce que c'est une image en couleur, rouge, vert, bleu. Très bien, voilà l'animal en question, enfin l'image en tous les cas, lui il ne sait pas que c'est l'image, j'a\"), ('190059', \"les cas, lui il ne sait pas que c'est l'image, j'ai mis image 2 je ne l'ai pas appelé Jaguar mon image, je l'ai appelé image 2, on est bien d'accord et la voiture je l'ai appelé image 1 on est d'accord, donc dans le nom du fichier on n'a pas les éléments pour comprendre, sinon de toute manière il n'en tient pas compte de toute manière. Voilà, donc de nouveau il a là, et on va voir la prédiction la prédiction, il a dit que c'est un jaguar avec une certitude de 52%. Voilà ce que fait Resnet. Voilà\"), ('190060', \" certitude de 52%. Voilà ce que fait Resnet. Voilà ce qu'a fait Resnet. Il me dit que c'est un jaguar, l'animal, en l'occurrence, avec 52% de certitude. Avec une prouvé d'appartenance, c'est 52%. Ça, c'était Resnet. Rappelez-vous, ça, c'était Resnet. Maintenant, regardons ce qu'a fait VGG19. Voilà, VGG19. J'ai fait exactement la même chose. Et il me dit également que c'est l'animal jaguar, mais avec une certitude de 90%, avec une tempête d'appartenance, avec plus de certitude cette fois-ci. Eh b\"), ('190061', \"enance, avec plus de certitude cette fois-ci. Eh bien, ça fait beaucoup d'applications, on pourra le voir en TD également ces éléments-là, et il y a une suite qui paraît assez logique, on va essayer d'explorer, je pense, rapidement, c'est le transfert learning. C'est-à-dire que, voilà, j'ai un modèle pré-entraîné, je veux le raffiner sur mon propre jeu de données. Ça permet de profiter du travail qui a été fait en amont sur des bases étiquetées par ailleurs. Et ensuite, on va essayer de le rendr\"), ('190062', 'ar ailleurs. Et ensuite, on va essayer de le rendre plus spécifique par rapport à nos jeux de données. Mais ça prend comme point de départ les modèles pré-entraînés, justement. Voilà, excellent travail à tous.'), ('200001', \"Bien, c'est parti. Alors dans cette vidéo, je vais reparler des réseaux de neurones convolutifs pour le classement d'images, mais cette fois-ci, je vais utiliser le tandem de bibliothèques TensorFlow-Keras pour Python. Essayons de resituer les choses pour qu'on comprenne bien ma démarche.\\n\\nJ'avais fait une vidéo, pas plus tard qu'hier, sur les réseaux de convolution pour le classement d'image et j'ai utilisé PyTorch. Pourquoi? Parce que je veux que mes étudiants développent une compétence élargi\"), ('200002', \"ue mes étudiants développent une compétence élargie sur l'utilisation de PyTorch. On peut faire beaucoup de choses avec PyTorch. Ils m'ont dit qu'ils avaient rencontré des difficultés donc je me suis dit bon on va essayer d'aller plus loin là-dessus pour qu'ils développent une réelle expertise là-dessus.\\n\\nAlors, petit plurling, on fait à l'université moi-même ça fait partie de mes enseignements mais moi je suis plutôt enclin à utiliser TensorFlow Keras dans un autre contexte que le classement d'\"), ('200003', \" Keras dans un autre contexte que le classement d'image, que la computer vision. Alors du coup, je me dis, ouh là là, les étudiants vont croire que si on veut faire du classement d'image, des réseaux neurones convolutifs, il faut utiliser PyTorch, mais non, on peut très bien le faire également avec TensorFlow Keras. C'est le but de cette vidéo-ci. L'idée est la suivante, pour le coup. Je vais reprendre exactement le même tuto, mais à la place de PyTorch, je vais substituer TensorFlow Keras, tout\"), ('200004', \"PyTorch, je vais substituer TensorFlow Keras, tout simplement. Et on va voir que ça marche pareil. C'est comme Pepsi et Coca. C'est une pub des années 90, c'est exactement pareil. Bon, ce n'est pas exactement pareil, mais la trame est la même. Et finalement, on obtient des résultats qui sont du même acabit. C'est ça le plus important.\\n\\nAlors très rapidement, TensorFlow, Keras, je présente vite fait, mais bon, voilà, TensorFlow, c'est une bibliothèque qui est extrêmement connue, qui est en ligne \"), ('200005', \"èque qui est extrêmement connue, qui est en ligne ici, qui fait partie des bibliothèques de référence de Deep Learning et associée à TensorFlow, en back-end si vous voulez, il y a Keras en fait c'est TensorFlow qui est en back-end de Keras en réalité donc Keras, on peut le voir comme une surcouche qui permet d'exploiter TensorFlow. Alors j'ai fait remarquer que Keras également peut se mettre en front-end d'autres bibliothèques comme PyTorch j'ai jamais essayé, mais ça se fait, c'est ce qui est d\"), ('200006', \"jamais essayé, mais ça se fait, c'est ce qui est dit là tous les cas. Vous êtes d'accord là-dessus? Moi, j'utilise Keras, généralement, surtout en front-end de TensorFlow. Ça rend les instructions plus faciles à comprendre et plus faciles à mettre en œuvre. Ça, ça se passe. Voilà.\\n\\nAlors, j'avais dit que j'utilise beaucoup TensorFlow Keras. Dans d'autres contextes, oui, j'ai fait d'ailleurs beaucoup de tutos là-dessus. Plusieurs tutos, nombreux tutos en été. Il y en a un, là, justement, où je mo\"), ('200007', \"utos en été. Il y en a un, là, justement, où je montre comment on met en œuvre comment on implémente les perceptrons simples et multicouches en utilisant TensorFlow Keras, il y a cette vidéo aussi. J'avais fait également un tuto il y a quelques années sur les réseaux de neurones convolutifs en utilisant Naim, mais en back-end, il n'y avait qu'un TensorFlow Terras, en réalité. Voilà ce qui se passe. Mais bon, comme j'ai utilisé Naim, les étudiants n'entendaient que Naim. Ils ne comprenaient pas q\"), ('200008', \" n'entendaient que Naim. Ils ne comprenaient pas qu'en dessous, il y avait Keras TensorFlow. eux ils disent ils utilisent NIME donc c'est autre chose en réalité j'utilise NIME mais ce n'est que surcouche en réalité en dessous dans ce tuto ci ce qui est utilisé réellement voilà les commandes qui sont lancées c'est des commandes qui sont envoyées à Keras TensorFlow mais bon c'était 2019 et j'ai augmenté significativement mes connaissances sur le deep learning justement grâce à un ouvrage que j'ai \"), ('200009', \"ep learning justement grâce à un ouvrage que j'ai trouvé très intéressant, qui est l'ouvrage de Stéphane Tufféry. J'ai trouvé suffisamment intéressant pour que j'en fasse une fiche de lecture. Je me suis bien donné une dizaine de pages. Donc, je conseille la lecture de cet ouvrage-là, parce qu'il vous permet de voir réellement ce qu'il faut comprendre, ce qui est intéressant dans l'apprentissage profond. Ça, c'était pour donner le cadre général. Mais réellement, ce qui est très intéressant, c'es\"), ('200010', \"Mais réellement, ce qui est très intéressant, c'est de bien comprendre que ces outils-là sont des outils très intéressants de deep learning, qui permettent de faire plein de choses, et notamment, justement, les réseaux de neurones convolutifs. Alors, je reprends mon sujet, pour le coup, mais là, je l'ai déjà exposé hier. Donc, regardez la vidéo, tout simplement. Regardez cette vidéo-ci pour bien comprendre l'idée. Voilà. Là, j'ai vraiment détaillé. Là, j'ai pris mon temps de détailler. dans cett\"), ('200011', \"é. Là, j'ai pris mon temps de détailler. dans cette vidéo-ci, je vais aller un peu plus vite l'idée c'est quoi? j'ai une base d'images qui sont étiquetées j'ai pris des Pokémon pour rigoler un peu les Bidrines et les Cubones et je vais présenter un réseau de neurones convolutif il y a deux couches il y a deux types de couches dans un réseau de neurones convolutif il y a la couche de convolution avec Conv2D et puis MaxPool, entre autres, mais il y en a d'autres choses possibles et ensuite ici on \"), ('200012', \" en a d'autres choses possibles et ensuite ici on a la couche perceptron les couches perceptron en réalité qui sont pleinement connectées une fois que j'ai créé mon modèle je peux prendre un individu supplémentaire et je peux prédire sa classe d'appartenance donc les schémas restent les mêmes de l'apprentissage supervisé on est bien dans le cadre de l'apprentissage supervisé la seule différence ici bien sûr c'est qu'on a des images à entrée mais inéfinie on est dans la catégorisation de document\"), ('200013', \"inéfinie on est dans la catégorisation de documents et là c'est des documents images Je dis ça parce que, par ailleurs, j'ai un cours de NLP avec les étudiants de Natural Language Processing, et ça fait partie des chapitres que j'aborde, donc la catégorisation des documents. Cette fois-ci, on a des données textuelles ici. Mais in fine, c'est la même trame. Et c'est ça qui est important. Je ne veux pas que mes étudiants soient dépendants des outils. Ça, déjà, c'est un point important. Donc, je mu\"), ('200014', \"s. Ça, déjà, c'est un point important. Donc, je multiplie les outils. Comme ça, ils remontent d'un cran et ils comprennent la trame, le fond. Ensuite, l'autre élément important également, c'est qu'on est dans un domaine, c'est la valorisation de la data. Donc, on a une trame aussi spécifique de valorisation de la data avec des compétences multiples. Déjà, si vous ne savez pas accéder à la data, faire des requêtes dans des bases ou accéder à des bases NoSQL, vous n'allez rien faire à la suite. Vo\"), ('200015', \"ases NoSQL, vous n'allez rien faire à la suite. Voilà, et ensuite, dans toute cette trame-là, avec au final, est-ce que j'ai de la valeur ajoutée? C'est ça l'idée, c'est vraiment ça. donc il faut bien voir ces différents aspects là et moi je suis très attaché à ces différents aspects là alors la base Pokémon je l'ai récupérée en ligne sur ce site-ci alors je n'ai pris que deux classes tout simplement présentons rapidement les données pour que tout le monde comprenne bien ce qu'on fait ce que nou\"), ('200016', \"t le monde comprenne bien ce qu'on fait ce que nous sommes en train de faire donc voilà je suis dans le réparateur démo habituel j'ai mon notebook qui est préparé ensuite j'ai un dossier d'image dans le dossier d'image il y a deux parties il y a train et test dans la partie train j'ai deux sous-dossiers les bidrills qui sont là c'est des guettes méchantes empoisonnées c'est ce qu'on m'a dit c'est ce qu'on m'a dit mes enfants et kubon qui a l'air moins agressif en tous les cas plus placide on va \"), ('200017', \"moins agressif en tous les cas plus placide on va dire je ne sais pas s'il est méchant ou pas d'ailleurs kubon donc dans la base train ici on sait bien une base de données qu'on a mais c'est une base d'image tout simplement on a un étiquetage qui est les sous-dossiers qui sont là, avec les bidrills d'un côté et les cubons de l'autre. De la même manière, dans l'apprentissage supervisé, dans le classement, dans la classification supervisée, j'ai la base test pour évaluer la qualité de mon modèle. \"), ('200018', \" base test pour évaluer la qualité de mon modèle. Et dans la base test, j'ai également des images bidrills dans le sous-dossier et des images dans le sous-dossier cubon. Ça, c'est vraiment très important parce que quand vous travaillez sur des données, la première chose, c'est comment je dois organiser mes données pour le présenter à mon outil. Ça, c'est la question numéro un. Si vous n'arrivez pas à charger les données sur l'outil, oubliez les traitements, oubliez le machine learning. Vous ne s\"), ('200019', \"raitements, oubliez le machine learning. Vous ne saurez rien faire parce que vous ne pouvez pas démarrer. Donc ça, c'est vraiment un élément Et ça, c'est des compétences additionnelles. Si vous ne savez pas lire un fichier image et l'afficher, oubliez tout de suite tout ce qui est traitement par ailleurs. Alors, une fois que j'ai bien détaillé tout ça, allons-y. J'ai démarré le notebook. J'ai démarré le notebook comme d'habitude. Voilà, et je l'ai préparé en amont, pour que tout le monde puisse \"), ('200020', \"i préparé en amont, pour que tout le monde puisse bien voir les différentes étapes. Toujours première étape, j'ai créé un environnement pour Keras dans son flot. Voilà. Très bien. Donc, j'ai installé ça. Très bien. et je vais vérifier le numéro de version. Donc ma version de Python, c'est 3.11.9. Ensuite, la version de TensorFlow que j'ai installée dans mon environnement, donc il faut toujours créer un environnement, c'est mieux. Moi, en TD, de toute manière, je crée un fichier IAML et je distri\"), ('200021', \"oute manière, je crée un fichier IAML et je distribue ça à mes étudiants pour qu'ils l'installent. Là, au moins, je suis sûr que ça fonctionne correctement. Voilà, donc là, ici, la version de TensorFlow que j'ai, c'est 2.16.1 et la version de Keras qui est associée à TensorFlow, ici, voilà, c'est 3.3.3. donc Keras, c'est un front-end qui me permettrait d'accéder à TensorFlow c'est comme ça que je dois les choses alors une fois que c'est bon ça très bien, on est prêt, donc on a tous les outils je\"), ('200022', \"ès bien, on est prêt, donc on a tous les outils je vais essayer de voir déjà les données, est-ce que je peux y accéder est-ce que je peux voir quelles sont les structures de données que j'obtiens, alors je spécifie le dossier par défaut et ici je vais lire une image déjà, pour voir si je l'affiche et voir les structures associées donc là je fais un open d'une image dans le train, dans Bidry. qui est dans traîne voilà je les charge avec ma potlib et je vérifie le type de l'image à c'est une matri\"), ('200023', \"et je vérifie le type de l'image à c'est une matrice numpy ensuite je regarde les dimensions de l'image donc il ya 504 502 voilà ça c'est le vertical horizontal je sais plus très bien et 466 donc c'est les dimensions ici en pixels c'est les définitions de l'image et on a trois canaux pourquoi parce qu'on a trois couleurs red green blue donc là on a les dimensions de l'image voilà horizontale verticale ou l'inverse je sais plus très bien et ensuite là on a trois canaux parce qu'on a trois couleur\"), ('200024', \"e là on a trois canaux parce qu'on a trois couleurs des trois couleurs primitifs alors pour voir les valeurs parce que c'est une sorte de matrice trois dimensions qu'on a c'est une matrice trois dimensions qu'on a pour voir la plage des valeurs dans ma matrice trois dimensions voilà on a donc la valeur minimale c'est 0 et la valeur maximale c'est 255 oui parce que un un élément, c'est un octet, en fait. Voilà ce qui se passe. Ok, très bien. Et j'affiche l'image. Et c'est un bidrill. Ouh là là, q\"), ('200025', \"affiche l'image. Et c'est un bidrill. Ouh là là, qu'est-ce qu'il a l'air méchant, le bidrill? Donc, c'est affiché. Alors, ce qui est important de bien voir, mais ça, j'ai expliqué dans la vidéo précédente, c'est que les images n'ont pas la même dimension, ici. Ils n'ont pas la même définition. Regardez, dans Train, voilà, celui-là, il est de taille 466,502, c'est celui que je viens de charger, Et celui-là, en revanche, il est de tailles différentes. 874, 869, vous avez vu ça. Donc déjà se pose l\"), ('200026', \"es. 874, 869, vous avez vu ça. Donc déjà se pose la question de l'harmonisation lorsque je vais charger les images. Je vais les présenter à un réseau de neurones. Il faut que dans l'input du réseau de neurones, on ait les mêmes dimensions. Donc quand je vais charger les images, il faut que je les harmonise en taille en tous les cas, en définition. Ça, c'est des éléments qu'il faut bien voir. Alors une fois que j'ai chargé une image et que j'ai vu à peu près les structures, On va maintenant charg\"), ('200027', \" à peu près les structures, On va maintenant charger la base train. On est d'accord? On va charger cette base-ci avec les deux classes, B-Drill d'un côté, et Kubon de l'autre. Il faut bien avoir cette structure-là, train ici. Il faut bien avoir cette structure-là. La même que dans PyTorch. Vous vous rappelez, dans PyTorch, j'ai utilisé exactement la même base. Et j'ai organisé les données exactement de la même manière. Donc, ce qui est valable sur PyTorch est valable sur TensorFlow Kerasi. Ça dé\"), ('200028', \"r PyTorch est valable sur TensorFlow Kerasi. Ça déjà, c'est un élément à voir. Alors, une fois que c'est bon ça, du coup, je vais parser le dossier train. Vous avez vu, image train, je vais parser le dossier image train. Alors pour ça, j'utilise un image data generator, voilà, et l'autre élément important, je vais présenter les données à un réseau de neurones, donc je vais rescaler les valeurs qui variaient entre 0 à 255, je vais les rescaler entre 0 et 1, en divisant les valeurs par 255, tout s\"), ('200029', \"re 0 et 1, en divisant les valeurs par 255, tout simplement. Du coup, on aura bien des valeurs qui valent 0 et 1 cette fois-ci. Très bien, très bien. Alors, autre élément très important, j'harmonise les tailles. Donc, il lit des images de tailles différentes, de définitions différentes, et quand il va les charger, il va les redéfinir pour qu'on ait bien cette taille 128-128. deuxième élément important ensuite on a bien trois canaux donc les couleurs les couleurs sont rgb red green blue et on va \"), ('200030', \"urs les couleurs sont rgb red green blue et on va les mélanger aléatoirement pourquoi parce que je vais présenter les images un réseau de neurones et si je passe d'abord les bidrilles il va corriger vers les bidrilles ensuite je passe et tu bonne il va corriger donc non non pour qu'ils corrigent bien qui corrige bien en tenant compte des deux classes, on va mélanger au hasard les images. Et l'autre élément important, c'est que quand on a des grandes bases d'images, on a intérêt à faire des trait\"), ('200031', \"des bases d'images, on a intérêt à faire des traitements par batch. Mais du coup, les images, les données qu'on manipule ne sont pas très lisibles en réalité. Donc c'est pour ça que c'est intéressant de faire un batch 16 égale à 1 ici, pédagogiquement, pour que à chaque image chargée, on puisse la visualiser directement. Sinon, on a des images empilées en fait. On a une matrice à quatre dimensions avec des images empilées. Et la visualisation devient un souci pour le coup. Vous êtes d'accord? C'\"), ('200032', \"ient un souci pour le coup. Vous êtes d'accord? C'est juste ça. Bien, alors, j'ai passé, il m'a dit, j'ai chargé 54 images qui appartiennent à deux classes. Eh bien, le type de la structure, c'est un directeur et l'itérateur. Donc là, je suis allé dans la documentation de Keras, et j'ai regardé les champs et les propriétés, les propriétés et les méthodes de directeur et de à temps. C'est ce qui m'a permis de voir à peu près qu'est ce que je peux en faire. C'est ce qu'il faut faire tout le temps.\"), ('200033', \"en faire. C'est ce qu'il faut faire tout le temps. Moi, bon je l'ai pas fait ici parce que l'affichage est trop long, mais j'ai fait un dire, j'ai regardé les éléments, j'ai regardé, j'ai inspecté ainsi de suite. Ça passe un peu de temps, je suis d'accord là dessus, mais c'est incontournable. Il faut inspecter, il faut lire la documentation et inspecter ce qu'on manipule. Voilà, vous pouvez pas manipuler sans savoir. Ce n'est pas possible. Alors, du coup, voilà, effectivement, on a bien les clas\"), ('200034', \" du coup, voilà, effectivement, on a bien les classes d'appartenance ici et les indices de classe. Le 0, ce sera le bidrill et le cubon, c'est 1. Donc, si je veux accéder aux images, alors, en fait, on a une structure itérative. Vous avez vu, c'est un directory itérator. Donc, pour accéder à chaque élément, je fais Next, tout simplement, et j'obtiens deux valeurs, deux éléments, l'image en elle-même et sa classe d'appartenance. Allons-y. Donc là, je prends l'image suivante, la première image. N'\"), ('200035', \" je prends l'image suivante, la première image. N'oubliez pas que ça a été chefflé. Du coup, il les a mélangés au hasard. Allons-y. Voilà. Déjà, les classes d'appartenance sont dans une matrice, en fait. Matrice 0.1. Donc, il a fait un codage disjonctif complet sur le label. Rappelez-vous d'un PyTorch. Dans PyTorps, ce qu'il utilisait, c'est des indices de classe. Là, ce qu'il fait, c'est qu'il crée un vecteur pour décrire les classes avec des valeurs 0,1. On a vu que la colonne 0, c'est bidrill\"), ('200036', \"leurs 0,1. On a vu que la colonne 0, c'est bidrill, donc ce n'est pas un bidrill parce qu'on a 0 là, et que la colonne 1, c'est cubon, et là on a la valeur 1. Donc l'individu qu'on vient de charger là, c'est un cubon. Ça a l'air comme ça, mais c'est super important, parce que du coup, dans votre réseau, il faudra en tenir compte. Dans la sortie de votre réseau, il y aura deux classes. Il faudra avoir les bonnes normalisations pour que quand je veux calculer la croissant tropie, ça matche avec ce\"), ('200037', \"ux calculer la croissant tropie, ça matche avec ce qu'on a ici. On est d'accord. Et là, on peut le savoir qu'en lisant la doc. On ne le sait qu'en lisant la doc. Vraiment, j'insiste beaucoup là-dessus. On ne le sait qu'en lisant la doc. Donc moi, je ne me suis pas lu chez la doc, tout simplement. Mais bon, c'est de l'anglais technique. Même moi, j'arrive à lire. Il n'y a aucune raison que vous n'y arrivez pas. alors du coup si je veux avoir la classe sous forme d'indice cette fois-ci je fais un \"), ('200038', \"asse sous forme d'indice cette fois-ci je fais un argmax tout simplement du vecteur si je fais un argmax du vecteur là je connais l'indice correspondant et là il me dit que c'est l'indice 1 parce que le 1 il est en position ici on a la colonne 0 et là la colonne 1 et du coup l'indice 1 il est bien en deuxième position qui est la colonne 1 c'est pour ça qu'on a 1 ici des éléments importants donc dans PyTorch on a des indices de classe dans TensorFlow Keras, on a un vecteur qui est issu d'un OneHo\"), ('200039', \"low Keras, on a un vecteur qui est issu d'un OneHot Encoding pour décrire les classes. Voyons maintenant l'image. Le type de l'image, c'est une matrice NumPy. Regardons les dimensions. Il a rajouté une dimension supplémentaire à cause du batch size. Rappelez-vous initialement, là j'ai fait un chargement brut. et regardez les dimensions, on avait horizontal, vertical et ensuite les canaux. Là, cette fois-ci, on a bien les mêmes informations, sauf que ça a été harmonisé, horizontal, vertical, les \"), ('200040', \"que ça a été harmonisé, horizontal, vertical, les canaux, et il y a une dimension supplémentaire, qui est le batch. Parce que quand on traite des très grandes bases d'images, il faut qu'on puisse faire des traitements par batch, par lot, pour accélérer les traitements. Sauf que du coup, on a des images empilées, en fait. Et ça, c'est difficile à comprendre dans un premier temps. Donc moi, je me suis astreint à mettre un batch S égal à 1, et du coup, il a rajouté bien une dimension avec une seule\"), ('200041', \"up, il a rajouté bien une dimension avec une seule. Et donc, j'ai une couche d'images empilées, mais il n'y a qu'une seule image dedans. Voilà, comment il faut voir les choses. Donc, cette dimension qui est égale à 1, là, je peux l'enlever avec un squeeze. Si j'enlève cette dimension égale à 1, on a bien la dimension d'image, que je peux afficher cette fois-ci. Vous avez bien vu, hein? Donc ça, c'est des éléments qu'il faut tenir compte, ça. Et vous savez comment? En lançant pas à pas, en regard\"), ('200042', \"ous savez comment? En lançant pas à pas, en regardant pas à pas les structures. C'est ça qui est important. Il faut vraiment que vous regardiez ça. Il faut vraiment que vous regardiez ça. Sinon, vous récupérez des tutos sur Internet, les gars, ils font tout comme si on a tout compris, et après, on ne comprend pas, et on n'arrive pas à transposer nos données, ce qui est juste impossible. Alors, une fois que c'est bon ça, je prends l'image suivante. de nouveau c'est un bidrill parce que le 1 il es\"), ('200043', \". de nouveau c'est un bidrill parce que le 1 il est en deuxième position et si je l'affiche voilà, on a de nouveau c'est un cubon parce que le 1 est en deuxième position et on voit bien le cubon ici ça y est, on est prêt et surtout on a compris la structure de nos données une fois que c'est bon alors je vais définir comme dans Pythonche je vais définir la structure du réseau rappelez-vous ici, je vais le faire ici de toute manière, mais sur la vidéo qu'elle a, j'ai mis  le plan qui permet de voi\"), ('200044', \"déo qu'elle a, j'ai mis  le plan qui permet de voir chacune des étapes vous avez bien vu ça voilà je vais le faire ici il faut absolument qu'on ait une idée de la trame globale et après le comment faire sur chaque étape on trouvera toujours c'est vraiment c'est bien un message sur lequel je suis extrêmement attentif c'est que déjà il faut avoir une très forte culture pour avoir une idée globale être attentif à ce qui se passe on peut pas tout apprendre tout de suite on peut pas tout savoir tout \"), ('200045', \"rendre tout de suite on peut pas tout savoir tout de suite ça n'existe pas on peut pas tout savoir tout de suite en revanche il faut avoir une culture générale suffisamment forte du domaine pour avoir une idée globale de ce qui se passe quand on est confronté à un problème donné. Une fois que c'est bon ça, on peut regarder par nous-mêmes si on sait ce qu'on fait. C'est ça ce qui se passe. Et après, c'est à nous d'être attentif. Moi je dis tout le temps aux étudiants, lisez au maximum, regardez l\"), ('200046', \" temps aux étudiants, lisez au maximum, regardez les vidéos sur YouTube, regardez ce qui se passe. Il faut absolument qu'on soit vigilant et qu'on soit attentif à ce qui se passe. Ça nous permet de faire les bons choix quand on a un un problème à traiter. Mais si on n'a pas une idée globale de ce qu'il faut faire, on ne peut pas commencer, parce qu'on n'a pas une idée des étapes qu'il faut enchaîner. Donc l'étape suivante, c'est quoi? L'étape suivante, c'est définir le réseau de neurones. Le rés\"), ('200047', \"vante, c'est définir le réseau de neurones. Le réseau de neurones convolutifs. Alors, bon, j'ai fait très simple ici. En fait, ce qui se passe, c'est que j'ai repris exactement l'idée qui est là. Il est où, là? Voilà. Définition de la classe. Voilà, attendez, je fais ça. Voilà. Là, c'est comment faire sur PyTorch. Vous avez vu, hein? Je vais refaire exactement la même chose, mais sous TensorFlow Keras. C'est ça qui est important. Donc, je vais retraduire exactement ce réseau-là, mais en utilisan\"), ('200048', \"traduire exactement ce réseau-là, mais en utilisant TensorFlow Keras. Et on va voir si ça marche aussi bien. Mais ça, c'est vraiment très intéressant. Vous avez un outil, vous avez un autre outil, et vous devez pouvoir les configurer de manière équivalente pour pouvoir faire les traitements. Regardons. Regardez mon réseau. Donc l'input shape, c'est ça. Les images ont été normalisées, il y a trois canaux. Voilà, très bien. Ensuite, je mets un conf2d, exactement comme ici. Voilà, très bien. Donc l\"), ('200049', \"2d, exactement comme ici. Voilà, très bien. Donc là, c'est les outputs channel, si vous voulez. Là, c'est le kernel size, et là, c'est la fonction d'activation. exactement comme on a fait ici. Vous avez vu, conv 1D, voilà les dimensions, ça c'est le input channel. Ensuite, j'applique le relu. Une fois que j'ai ça, je fais le max pool, exactement comme ici. J'ai fait un pool, là. Max pool. C'est un peu enchaîné, là, j'aurais dû détailler ici. Mais c'est des idées. Et ensuite, je refais un autre c\"), ('200050', \" c'est des idées. Et ensuite, je refais un autre conv 2D. Et ensuite, je fais un max pool. Là, c'est le output channel, kernel 16, la fonction d'activation. Et j'ai fait un max pool sur ces éléments-là. Exactement comme j'ai fait l'enchaînement ici. Regardez les deux vidéos en parallèle en faisant un arrêt sur image, une émission que j'adore, avec ce qu'on a ici et ce qu'on a avec Kerastin sur flow. Et regardez bien, ça c'est vraiment important. Faire ce parallèle-là est vraiment important pour \"), ('200051', \"Faire ce parallèle-là est vraiment important pour comprendre ce qu'on est en train de manipuler. ensuite je vais créer les couches pleinement connectées, les couches denses si vous voulez, et donc je passe par un flatten, exactement ici je passe par un flatten. Très bien, une fois qu'on a flattené, j'ai les couches denses. Alors un élément qui est très important c'est que dans Paytorch il y avait un enjeu, c'est le calcul de la dimension au démarrage de la couche denses, donc ça j'ai expliqué co\"), ('200052', \"rage de la couche denses, donc ça j'ai expliqué comment faire pour calculer cette dimension là ce qui est intéressant dans keras c'est que non il le calcule automatiquement lui même donc on n'a pas le problème d'incohérence quantifié les produits matriciels ça c'est pas mal je vais pas dire que l'un est meilleur que l'autre je vais pas le lancer dans ce genre de débat totalement stérile voilà mais quand même mais quand même je trouve je trouve que c'est un peu plus simple un peu plus plus simple\"), ('200053', \"e c'est un peu plus simple un peu plus plus simple à mettre en œuvre parce qu'on n'a pas besoin de créer déjà deux méthodes un jeu définit mon réseau et j'empile les couches dedans alors n'oubliez pas qu'il ya la couche input d'entrée ici avec les bonnes dimensions ça c'était un élément important alors une fois que ces plaques platenne je mets les couches dans ce voilà avec tout en l'activation relu vous avez vu un produit avec les couches dans et là la couche de sortie n'est pas normalisée on a\"), ('200054', \"t là la couche de sortie n'est pas normalisée on a une sorte de logit une fonction d'appartenance ici en revanche j'applique une activation softmax alors pourquoi j'applique une activation softmax ici pourquoi je fais pas comme sous pytorch parce que la variable cible se présente maintenant comme une matrice issus d'un one hot encoding alors que dans pétorch on avait des indices de classe n'oubliez pas la variable cible maintenant voilà du coup on va opposer les probas d'appartenance avec le vec\"), ('200055', \"n va opposer les probas d'appartenance avec le vecteur 0 1 là donc on aura en sortie de notre réseau donc des un vecteur de probas d'appartenance à la classe 0 à la place ainsi de suite voilà et nous ici on a un vecteur de 0 1 pour identifier la classe d'appartenance et c'est ces deux informations là qui va opposer du coup la la fonction de perte, j'aurais pu utiliser un MSE, exactement comme la vidéo que j'avais faite précédemment avec le PyTorch pour la classification multiclasse. Vous êtes d'\"), ('200056', \"h pour la classification multiclasse. Vous êtes d'accord là-dessus. Mais ça, c'est un élément important. Il fallait en tenir compte. Il fallait absolument voir ici que, là, pour le coup, j'ai bien deux neurones dans la couche de sortie et j'ai une activation softmax. Sinon, ce n'est pas cohérent. Sinon, ce n'est pas cohérent ce qu'on en est plus. Et quand il va calculer la fonction de perte, ça ne va pas le faire. Qu'est-ce qu'il reste à faire après? Après, il faut paramétrer l'apprentissage, l'\"), ('200057', \"rès? Après, il faut paramétrer l'apprentissage, l'entraînement. Donc déjà, il faut que je cherche en mémoire, on est d'accord là-dessus. Et très intéressant également, avec Keras Stensocflow, c'est qu'il vous décrit l'architecture de votre réseau, donc vous comprenez exactement ce qu'on est en train de manipuler et le nombre de coefficients qu'il est en train de calculer. C'est pas mal. Encore une fois, qu'importe l'outil, qu'importe le flacon pour vous connaître l'ivresse, disait un philosophe \"), ('200058', \"ur vous connaître l'ivresse, disait un philosophe qui avait compris plein de choses. Là également, on a deux outils, PyTorch et Kerstension Flow, qu'importe, l'essentiel, c'est qu'on fasse bien un réseau de convolution, enfin un réseau de neurones convolutifs. Alors, une fois que c'est bon, j'ai bien une idée de mon architecture, qu'est-ce qui se passe? Je définis les paramètres de mon apprentissage. Maintenant, vous avez compris, je ne vais pas revenir en détail là-dessus, d'une part, il y a la\"), ('200059', \"revenir en détail là-dessus, d'une part, il y a la fonction de perte, donc je prends un catégorique de crocentropie, même si on est une classe binaire, il a été codé avec deux colonnes 0,1. Donc pour lui, c'est du multiclasse, même si c'est avec deux classes. Sinon, on n'aurait qu'une seule colonne, en fait. On a deux colonnes pour décrire la variable cible, deux colonnes de 0,1, via un one-hot encoding. Donc c'est pour ça que j'ai pris un catégorique de crocentropie. C'est important de le savoi\"), ('200060', \"rique de crocentropie. C'est important de le savoir, tout simplement. Ensuite, l'autre élément important, c'est l'algorithme d'optimisation, mais j'ai pris Adam, exactement comme PyTorch. Encore une fois, c'est une variante des centres de gradient stochastique. Sans c'être plus efficace, qui excite la documentation. Alors, avec Keras TensorFlow, on peut suivre un paramètre en particulier pour voir la qualité de l'apprentissage. Ici, j'ai décidé de suivre la curation. On n'est pas obligé, en fait\"), ('200061', \"e suivre la curation. On n'est pas obligé, en fait. On peut aussi se cantonner à la fonction de perte. Eh bien, du coup, je peux lancer l'entraînement. Donc là également, l'entraînement, oui, sur PyTorch, rappelez-vous, voilà le fit sous Python d'accord, pourquoi pas maintenant qu'on comprend exactement ce qui se passe il n'y a pas de soucis, j'ai expliqué en détail ce qui se passait dans le fit bon, sous Keras TensorFlow on appelle la fonction fit c'est différent et on met le nombre des poches \"), ('200062', \"it c'est différent et on met le nombre des poches tout simplement zou alors alors je ne vais pas dire que c'est mieux du coup que Keras, je ne dirais pas ça Moi, tout simplement, je dis que dans Keras Tension Flow, on a déjà des éléments prêts à l'emploi qui nous facilitent la vie. En revanche, dans PyTorch, on a plus de possibilités de faire exactement ce qu'on veut. Mais il faut maîtriser ce qu'on veut faire. C'est ça la différence. Très bien. Mais bon, c'est à nous de savoir. C'est à nous de \"), ('200063', \"Mais bon, c'est à nous de savoir. C'est à nous de savoir ce qu'on veut faire et c'est à nous de savoir comment on veut obtenir nos résultats. C'est ça qui est important. Alors, il a l'appris, vous avez vu, vous avez le LOS qui est là, et vous avez la curéatie qui est là. Donc, sur la base d'apprentissage, il fait 100% de succès. Très bien, donc il classe toutes les... Mais bon, c'est sur la base d'apprentissage, on est d'accord là-dessus. On voit que le LOS, là, qui est le... Donc, le cross-entr\"), ('200064', \" que le LOS, là, qui est le... Donc, le cross-entropie, catégoriel cross-entropie, là. Alors, il a commencé là, puis il a baissé, baissé, baissé. Il est quasiment nul. Donc, il a une sorte d'apprentissage parfait sur l'échantillon d'apprentissage. vérifions comment on peut alors travailler sur une image supplémentaire là je reprends une image suivante de l'échantillon traîne je récupère son label et j'affiche l'image donc c'est un bidrill, vous avez vu du coup le A maintenant il est en première \"), ('200065', \"vez vu du coup le A maintenant il est en première position, il est en colonne 0 d'accord c'est bien un bidrill comment le réseau va faire sa prédiction sur cette image là donc je fais un print de predict et il revoit un vecteur de probabilité d'appartenance aux classes. Ça, c'est l'appartenance, la première valeur, c'est l'appartenance à la classe 0 et là, c'est l'appartenance à la classe 1. Et on va prendre la valeur max, c'est des probabilités.  Donc, la somme des probas fait 1 cette fois-ci, \"), ('200066', \"  Donc, la somme des probas fait 1 cette fois-ci, à la différence de PyTorch, où on avait des valeurs d'appartenance, mais qui ne sont pas normalisées. Là, on a des valeurs d'appartenance qui sont normalisées, et en réalité, c'est des probas d'appartenance. Alors, on voit que l'appartenance à la classe 0 est de 0, donc 999. Donc, la prédiction du modèle, si on prend le max, si on prend le argmax, la prédiction du modèle, ce sera tout simplement bidrill. et ça match bien avec la classe d'apparten\"), ('200067', \"idrill. et ça match bien avec la classe d'appartenance, avec le label qui était le bidril. Donc, qu'est-ce qui se passe alors? On va évaluer notre modèle sur l'échantillon test. C'est nouveau, très bien. Donc, ici également, il faut que je parse les images en appliquant les mêmes transformations. Et une fois qu'on a appliqué les transformations, on va appliquer le modèle sur chacune des images. Et on va confronter la prédiction du modèle avec la classe d'appartenance. Zoom. Allons-y alors. Là, j\"), ('200068', \"classe d'appartenance. Zoom. Allons-y alors. Là, je parse la base test. On est bien d'accord. C'est bien la base test que je suis en train de parser là. Ici, avec l'harmonisation en RGB, toujours. Batches égale à 1 parce que je vais faire la prédiction image par image. Et je n'ai pas besoin de shuffle cette fois-ci. Non, ça ne sert à rien pour le coup. On va les classer là. Donc, qu'importe l'ordonnancement ici, qu'ils soient tous les bit drills d'abord et les kubons après, ce n'est pas un probl\"), ('200069', \"d'abord et les kubons après, ce n'est pas un problème pour le coup. en classement, chaque individu est traité de manière indépendante des autres individus de la base de test. Ça également, c'est un élément sur lequel j'insiste beaucoup, parce que les étudiants pensent qu'on peut calculer des paramètres sur l'échantillon test. Non, non, que nenni. Sur l'échantillon test, on va traiter chaque individu de manière indépendante des autres. Donc du coup, non, il n'y a aucun paramètre à calculer sur l'\"), ('200070', \"p, non, il n'y a aucun paramètre à calculer sur l'échantillon test. Alors, le nombre d'observations est n. Voilà, 20 observations. Du coup, pour calculer l'accuratie, qu'est-ce qui se passe? Je crée une liste pour les prédictions, je crée une liste pour les classes d'appartenance, je vais itérer sur les images en utilisant le n qui est là, les 20 individus, la valeur 20. Qu'est-ce qui se passe? Je fais un next pour accéder à chacune des images, je fais la prédiction du réseau, donc du coup j'ai \"), ('200071', \"e fais la prédiction du réseau, donc du coup j'ai un vecteur de probabilité, et je regarde la prédiction en utilisant un argmax du vecteur de probabilité. De la même manière, j'ai récupéré le label, qui est une vecteur de 0,1. Et pour avoir la vraie classe d'appartenance, je fais aussi un argmax. J'aurais pu faire également dans quelle colonne est le 1. C'est pareil, c'est bien ça. Très bien. Et je mets dans deux listes la prédiction et la vraie classe d'appartenance. Et j'affiche les deux liste\"), ('200072', \"classe d'appartenance. Et j'affiche les deux listes. Allons-y. Très bien. Donc à chaque fois, il fait appel à Keras-TensorFlow. Très bien. TensorFlow ici en l'occurrence. Et on a les prédictions et les classes d'appartenance. Et on voit qu'il y a deux erreurs là. là, qui est codé 0, et celle-là, qui est un cubon, qui sera... Là, c'est un bidrill qui sera classé cubon, et là, c'est un cubon qui sera classé bidrill. Très bien. Calculons l'accuratie, on le connaît, du coup, on a 18 sur 20 ici, en t\"), ('200073', \", on le connaît, du coup, on a 18 sur 20 ici, en termes d'accuratie. Voilà, 90%. qu'importe le flacon pourvu qu'on ait l'ivresse j'ai fait une vidéo où je montre comment on fait des réseaux de neurones convolutifs avec Péthorch on peut faire exactement la même chose avec Tensionflow Keras tout l'enjeu tout simplement c'est de savoir comment utiliser l'outil et dans ce qu'on a fait aujourd'hui il y avait un endroit peut-être sur lequel il fallait être très très attentif c'est comment je définis m\"), ('200074', \"être très très attentif c'est comment je définis mon réseau voilà l'écriture qu'il faut faire quand on travaille avec Tesson Flockeras et si on est sur PyTorch l'écriture est un peu différente tout simplement je l'ai mis elle était par là elle était par là, j'ai oublié bon elle était par là mais bon vous regarderez par vous c'est pour voir si vous suivez c'est l'astuce que je sors quand je commence à mélanger les pinceaux au tableau, c'est pour voir si vous suivez. Voilà, excellent travail à tou\"), ('200075', 'oir si vous suivez. Voilà, excellent travail à tous.'), ('210001', \"Bien, c'est parti! Alors dans cette vidéo, nous allons parler des réseaux de neurones convolutifs pour le classement d'images. Et pour cela, nous allons utiliser la librairie PyTorch, pour Python donc, avec son extension TorchVision. Alors j'ai mis le schéma global pour qu'on ait une idée d'ensemble tout de suite, et par la suite on rentrera dans les détails des traitements.\\n\\nQu'est-ce qui se passe? On a une base étiquetée d'images. Là, pour le coup, j'ai pris des Pokémon, je vais revenir dessus\"), ('210002', \"oup, j'ai pris des Pokémon, je vais revenir dessus. Je n'y connais pas grand-chose en Pokémon, j'avoue, mes enfants s'y connaissent mieux que moi. Mais j'ai trouvé ça amusant. Surtout qu'à une randonnée, mes étudiants, là, pour le coup, avaient fait un atelier où on faisait du classement d'images, du Pokémon, voilà. Mais en utilisant cette fois-ci, enfin, Keras, c'était en surflow. J'y reviendrai également par la suite. Voilà, donc on a une base d'images étiquetées. D'ici, on a les bidrills. App\"), ('210003', \"d'images étiquetées. D'ici, on a les bidrills. Apparemment, c'est la guêpe, on ne pas zonné. Enfin, je n'y connais pas grand-chose, mais elle a l'air bien méchante, en tout cas. Voilà, et on a notre ensemble d'images, c'est les cubones, qui a l'air un peu moins méchants. Donc ces bases étiquetées-là sont présentées en réseau de neurones convolutifs. J'en parlerai un peu rapidement par la suite des réseaux de neurones convolutifs. Ça fait partie des cours de deep learning, ça. Et une fois qu'on a\"), ('210004', \"es cours de deep learning, ça. Et une fois qu'on a un modèle prédictif, il y a deux parties. Il y a la couche de convolution qui est par là, une sorte de pré-traitement où on met en évidence les informations pertinentes des images. Et il y a aussi une forme de réduction de dimension là-dedans. Très bien, je ne suis pas spécialiste, mais c'est à peu près des idées globales. Une fois qu'on a ça, on vectorise les informations à la sortie, qui est présenté ensuite à un perceptron multicouche classiq\"), ('210005', \"ésenté ensuite à un perceptron multicouche classique, complètement connecté. Une fois qu'on a notre modèle, qui inclut ces deux parties-là, on peut prendre un individu supplémentaire et essayer de deviner sa classe d'appartenance. Exactement, schéma classique de liens en machine learning, si vous voulez. alors la particularité ici c'est qu'on fait du traitement d'image c'est pas mon cours, c'est un collègue qui fait le cours en Master 6 pour le coup mais bon, j'ai suffisamment de background pour\"), ('210006', \"oup mais bon, j'ai suffisamment de background pour savoir ce qu'il faut faire pour savoir les différentes étapes et mettre en oeuvre les choses et l'autre aspect important c'est qu'on va utiliser PyTorch alors revenons deux secondes sur PyTorch j'ai préparé différents éléments ici alors dernièrement j'ai fait des séries de vidéos sur Python. Alors pourquoi? Parce qu'à l'université, on fait des cours de deep learning, excellent, très bien. Moi également, d'ailleurs. Mais moi, j'ai plus d'appétenc\"), ('210007', \"lement, d'ailleurs. Mais moi, j'ai plus d'appétence pour Keras, Tinsorflow. Je trouve l'outil plus schématique, plus facile à expliquer aussi. Parce que c'est ça, c'est une chose de savoir le faire soi-même. Mais moi, mon boulot, en plus, c'est d'expliquer aux autres de manière à ce qu'ils comprennent. C'est un enjeu supplémentaire, ça. Et je trouve que Tinsorflow, Keras, surtout, permet de schématiser les différentes étapes pour que les étudiants comprennent bien ce qui se passe. Mais je ne sui\"), ('210008', \"s comprennent bien ce qui se passe. Mais je ne suis pas le seul à faire des cours de deep learning à l'université, bien sûr. Et il y a un de mes collègues qui demande aux étudiants d'utiliser PyTorch. Et là, les étudiants se perdent un peu. Alors, ces deux dernières années, justement, j'en avais discuté avec les étudiants. Et ils m'ont dit, mais on a des problèmes avec PyTorch. La première année, j'ai dit, attends, c'est qu'une librairie comme une autre. il suffit de regarder des tutoriels sur i\"), ('210009', \"e autre. il suffit de regarder des tutoriels sur internet et on sait le faire, il n'y a pas de problème là-dessus il faut tout simplement s'adapter au mode opératoire de Libéry donc j'avais dit oui, oui, très bien et cette année les étudiants m'ont fait les mêmes commentaires quand les étudiants font deux promotions différentes font les mêmes commentaires c'est qu'il y a un souci et les soucis c'est à moi de le résoudre c'est moi le responsable donc c'est à moi de trouver comment faire pour que \"), ('210010', \"onc c'est à moi de trouver comment faire pour que les étudiants puissent avancer alors du coup j'avais commencé à faire des tutos dernièrement, et il y a un dernier tuto, peut-être qu'il y en aura d'autres, mais en tout cas un dernier tuto, justement c'est l'utilisation de PyTorch avec les réseaux de neurones convolutifs. Alors, très rapidement, la librairie PyTorch est là, c'est une librairie phare de Deep Learning, il y en a plusieurs, notamment Kerastation Flow, mais PyTorch aussi, PyTorch ég\"), ('210011', \"t Kerastation Flow, mais PyTorch aussi, PyTorch également est extrêmement mis en avant quand on parle de Deep Learning, et dans PyTorch, il y a l'extension Torch Vision. qui est spécialisé pour le traitement des images, justement. Voilà, très bien. La page est là, vous regardez tout simplement, mais il explique bien ça ici. Voilà, pour computer vision. Donc jusque là, ça va. Alors moi, je me suis dit, ok, je vais essayer de mettre en place un réseau de neurones convolutif. Un réseau de neurones \"), ('210012', \"eau de neurones convolutif. Un réseau de neurones convolutif, c'est un des outils les plus emblématiques du deep learning, mais ce n'est pas le seul, on est bien d'accord là-dessus. en NLP également, Natural Language Processing, on utilise les modèles de deep learning. Mais bon, notamment une des applications phares qu'on met souvent en avant quand on parle de deep learning, c'est le classement, le traitement d'images. Voilà, classement d'images notamment. Et pour le classement d'images, on a un\"), ('210013', \"notamment. Et pour le classement d'images, on a une zone neuronal convolutive. Donc j'ai récupéré d'ailleurs l'image, elle est où la copie d'écran? Voilà, elle est par là. Voilà. Et on nous dit qu'effectivement, il y a deux parties, les opérateurs de convolution, et ensuite ici un perceptron multicouche. classique. Donc les opérateurs de convolution sont présentés là, traitement convolutif, et ce qu'il faut retenir, moi, ce que j'ai retenu en tous les cas, c'est que 1. Ils permettent de mettre e\"), ('210014', \"s les cas, c'est que 1. Ils permettent de mettre en évidence les informations pertinentes dans l'image, en fonction de ce qu'on veut mettre en évidence, c'est là qu'il faut avoir des spécialisations en images. Moi je comprends à peu près, mais bon, je ne suis pas spécialiste là-dedans, mais je sais à peu près ce que je fais, sinon ça ne sert à rien de le faire. Et ça c'est un premier aspect, et le deuxième aspect, c'est qu'ils permettent de réduire la dimensionnalité. Ah oui, parce que l'image e\"), ('210015', \"re la dimensionnalité. Ah oui, parce que l'image est de taille, admettons que l'on a une image en trois couleurs, avec les trois couleurs primaires de 128x128, le nombre de pixels c'est 128x128x3. Ça fait beaucoup ça. Du coup, mettre ça en entrée dans un réseau de neurones directement, d'un perceptron, on a des problèmes de surdimensionnalité justement. Donc là, c'est très important d'avoir ces opérateurs de convolution qui permettent d'améliorer les traitements par la suite. Et l'enjeu, il est \"), ('210016', \" les traitements par la suite. Et l'enjeu, il est souvent là. En fait, l'enjeu souvent, mais ça j'en reparlerai un peu plus tard, c'est comment on augmente les informations portées par les images, donc c'est l'augmentation de l'information. Et l'autre aspect important également, c'est la réduction de dimensionnalité. Pour qu'on ait un apprentissage plus solide, plus consistant. Voilà, je vais chercher le terme. bien, alors une fois que c'est bon ça du coup on va pouvoir y aller et je répète un p\"), ('210017', \"ça du coup on va pouvoir y aller et je répète un peu mon traitement on va utiliser la base Pokémon alors la base Pokémon en fait je l'ai trouvée sur internet, elle est là la base Pokémon elle est là Pokémon Data, Read Images et tout ça, voilà, il est là alors il n'y a pas deux classes il y a plus de deux classes, énormément de classes en réalité mais bon moi je suis sur un tutoriel donc je voulais absolument utiliser une base où on a des vraies images. Parce que c'est un vrai souci qu'on a souve\"), ('210018', \"mages. Parce que c'est un vrai souci qu'on a souvent avec les tutoriels sur Internet. Quand j'ai cherché, moi, qu'est-ce qui s'est passé? J'ai cherché un tutoriel sur PyTorch pour le classement d'images. Je suis tombé sur la site de PyTorch où on travaille sur la base CIFAR 10. Ça, c'est un problème. Qu'est-ce qui se passe? La base CIFAR 10, c'est des données qui sont encapsulées dans un package. OK, pourquoi pas? Mais déjà, nous, ce n'est pas ça le cas. Les étudiants, quand ils vont faire leur \"), ('210019', \" le cas. Les étudiants, quand ils vont faire leur projet, ils vont travailler sur leur propre image, ils ne vont pas travailler sur des images qui sont encapsulées dans des packages. Non. Surtout que quand les données sont encapsulées dans des paquets, ils sont déjà pré-préparés. En fait, les données sont déjà préparées. Là, typiquement, dans le CIFAR10, si je regardais en détail, ils sont déjà mis sous forme de tenseurs. Mais dans la vie réelle, les images sont des JPEG, ils sont des PNG. c'est\"), ('210020', \" les images sont des JPEG, ils sont des PNG. c'est pas des tenseurs donc toute la partie je récupère les images et des prétraitements à faire en termes d'harmonisation en termes de nettoyage je sais pas comment ça se passe ensuite je le mets sous forme de tenseur je sais pas également comment ça se passe voilà et c'est un drame parce que j'ai vu ça donc déjà ça que j'ai commencé à cliquer à tiquer là dessus parce que les données qui sont capsulées dans des paquets je trouve ça pas bien du tout s\"), ('210021', \"s dans des paquets je trouve ça pas bien du tout surtout que voilà il ya tout un tas de préparation qu'on ne voit pas et surtout l'autre aspect qui j'ai trouvé assez c'est toujours comme ça sur Internet souvent, c'est que tout le monde se copie les uns les autres. Donc les tutoriels BIS qui font du classement d'images sur SIFAR, mon Dieu, sur SIFAR 10, mon Dieu, parce qu'il y a 10 classes, voilà, il y en a une quantité monumentale. Donc je ne voulais pas ça. Je ne voulais pas ça, je voulais abso\"), ('210022', \"lais pas ça. Je ne voulais pas ça, je voulais absolument utiliser une base où on a des vraies images. Parce que dans la vie réelle, quand vous-même vous travaillez ou quand les étudiants font leurs projets, Ils ont leur propre image qu'ils veulent traiter réellement. Donc, j'ai cherché des images. Vous pouvez prendre moi en photo et mes étudiants, c'était une possibilité. Mais bon, j'ai essayé de trouver des images d'une base plus sympa. Et j'ai pris les Pokémon. Je n'y connais rien à Pokémon, m\"), ('210023', \"pris les Pokémon. Je n'y connais rien à Pokémon, mais vraiment rien du tout. Je sais qu'il y a beaucoup de gens qui se passionnent de ça. Bon, pourquoi pas. Donc, j'ai récupéré ça et j'ai récupéré deux classes, tout simplement pour essayer de faire de la discrimination, de la classification binaire. alors la base elle est là du coup et voilà donc le répertoire des mots comme d'habitude là il ya mon pain à mon mon notebook que j'ai préparé et il ya les images qui sont là que j'ai scindé en deux a\"), ('210024', \"a les images qui sont là que j'ai scindé en deux au hasard train et test dans la partie traîne de sous dossier bidrill c'est celui de la guette méchante la empoisonné voilà et le cuban l'autre très bien alors bidrille j'y suis allé un bon Voilà, on a les images là.  dans les grandes icônes, très bien, voilà les images de la partie traîne, voilà les bidrills de la partie traîne, et les cubones, ils sont là. Ça, c'est la partie traîne, la partie apprentissage. Voilà, sur la partie test, bidrills, \"), ('210025', \"prentissage. Voilà, sur la partie test, bidrills, ils sont là, les images pour l'échantillon test, si vous voulez, et les cubones, ils sont là. Donc déjà, un élément très important, pour vos projets à vous, il faut que vous mettez vos images dans un dossier quelconque que vous descendez en train test et dans la partie train, chaque image d'une classe sont dans un dossier et le dossier est nommé avec le nom de la classe. Voilà, voilà. Donc si je prends un exemple, vous avez des images de tumeurs \"), ('210026', \"rends un exemple, vous avez des images de tumeurs malignes, vous avez des images de tumeurs bénignes, là du coup vous aurez bénigne, voilà, avec les images de ce ne sais pas quoi qui sont d'une tumeur bénigne et là vous mettez malignante pour avoir l'ensemble des images des tumeurs mais qui sont cette fois-ci malignes. Ainsi de suite. Ou bien, vous voulez classer des voitures, par exemple. Là, vous avez les voitures dont entraîne, vous avez la classe des voitures sportives, fast-back, si vous vo\"), ('210027', \"asse des voitures sportives, fast-back, si vous voulez. Et là, vous avez les voitures, je ne sais pas, SUV. Je n'avais pas lancé une polémique sur le SUV, mais on en parle beaucoup en ce moment. Voilà, et suivi. Et voilà. Et donc, chaque classe, vous les mettez dans ce dossier, les images. Voilà. Et le sous-dossier, vous le nommez avec le nom de la classe. Voilà. Et donc, on a bien cette organisation-là. Donc, ça, c'est des préalables, ça. Mais bon, c'est tout à fait accessible. Et vous pouvez m\"), ('210028', \"on, c'est tout à fait accessible. Et vous pouvez mettre vos propres images. Là, j'ai mis des Pokémon. Mais vous, vous mettez votre image que vous avez prise avec votre appareil photo, par exemple. Ou tout ce que vous voulez traiter. des images de boulons cassés versus des images de boulons pas cassés ou des choses comme ça. Je ne sais pas ce que vous voulez. Mais voilà donc l'organisation des données. C'est vraiment super important. C'était très important qu'on travaille sur des vraies images et\"), ('210029', \"important qu'on travaille sur des vraies images et non pas sur des images qui sont encapsulées dans des packages. Ça ne sert à rien. Ça ne sert à rien. Les images encapsulées dans des packages, c'est joli pour montrer les fonctionnalités, mais ce n'est pas transposable dans des traitements réels. j'ai acheté un peu mais ça m'énerve beaucoup de voir beaucoup de choses là dessus sur internet ceci étant fait une fois que j'ai lâché mon fiel allons-y on va lancer les traitements comme d'habitude j'a\"), ('210030', \" on va lancer les traitements comme d'habitude j'ai créé un environnement PyTorch j'ai mis à jour par rapport aux vidéos précédentes parce que je voulais être sûr d'avoir les dernières versions de PyTorch et Torchvision pour faire mes traitements c'est pour ça toujours que ça devrait être obligatoire à chaque fois que vous créez un notebook et que vous lancez, avant de lancer les traitements, vérifiez vos versions pour être sûr qu'il n'y a pas d'embrouille. Alors, j'affiche la version Python, tr\"), ('210031', \"embrouille. Alors, j'affiche la version Python, très bien, qui est là. J'affiche la version de PyTorch, de mon environnement, vous avez vu. Donc, c'est 3.11. Là, j'ai pris la 2.3.0, cette fois-ci, avec CPU, parce que je n'ai pas de GPU très performant sur ma machine. Ah, non, le jour université, je ne dis rien là-dessus. Voilà, très bien. Ensuite, j'affiche la version de TorchVision, qui est l'extension qui permet de faire du traitement d'images. Vous avez bien vu, là. Cette extension-ci, c'est \"), ('210032', \" Vous avez bien vu, là. Cette extension-ci, c'est ce que j'ai étudié en détail. J'ai la 0.18.0, toujours en mode CPU. Et j'importe ensuite les dossiers et les librairies supplémentaires. Je vais utiliser Matloply pour afficher les images, et NumPy pour faire différentes transformations sur les vecteurs ou les matrices. Alors, je vais importer les données traîne. Rappelez-vous, son bureau, démo. Voilà, donc je vais importer les données traîne avec les deux sous-dossiers qui sont là. Très bien, do\"), ('210033', \" les deux sous-dossiers qui sont là. Très bien, donc je branche déjà sur le dossier racine, qui est démo. Ensuite, je vais aller piocher dans l'image, avec traîne d'un côté, test de l'autre. Vous êtes d'accord, vous avez bien vu le chemin ici. Très bien. alors déjà les images sont dans toutes la même dimension ça c'est un élément important, regardez c'est le bidrill celui-là par exemple si je regarde ses propriétés c'est une image qui est de 466,502 voilà ensuite cette image-ci c'est notre dimen\"), ('210034', \"502 voilà ensuite cette image-ci c'est notre dimension donc c'est des images qui sont de dimensions différentes ça va poser problème ça donc lors du chargement il faut que je fasse un pré-traitement où j'harmonise les dimensions. Voilà. Ça, c'est un premier élément important. Ensuite, également, comme je veux utiliser PyTorch pour faire mes traitements de deep learning, il faut que les images les transforment en tenseurs. C'est des matrices compatibles PyTorch. C'est ni plus que des matrices, ma\"), ('210035', \"tibles PyTorch. C'est ni plus que des matrices, mais compatibles PyTorch. Donc, pour cela, j'ai créé une fonction de transformation. Donc, cette fonction de transformation est composée de D'où le terme qu'on pose ici. Un, une première fonction qui redéfinit les dimensions, qui les harmonise. Et deux, qui transforme les images harmonisées sous forme de tenseur. Voilà la fonction. Donc c'est une fonction que je prépare en amont pour pouvoir traiter lors du chargement. Ensuite, le chargement, qu'es\"), ('210036', \" lors du chargement. Ensuite, le chargement, qu'est-ce qui se passe? Je le fais avec ici image folder. C'est bien vu. Donc avec image folder, je prends le répertoire par défaut. mais le répertoire par défaut, c'est user, démo. Et je vais les piocer dans le sous-dossier images, train. Et là, j'applique la fonction de transformation quand il charge les images qui sont dans le dossier, dans le répertoire, dans le folder. C'est ce que je lance ici. Donc, rappelons-nous que dans train, il y a deux so\"), ('210037', \"onc, rappelons-nous que dans train, il y a deux sous-dossiers pour chaque classe. Là, c'est la classe bidrill avec les images associées et là, c'est la classe kubon avec les images associées. et donc il a chargé les dessous dossiers. Alors, le type de dataset, c'est image folder. Très bien, enfin de data train, là, c'est image folder. Ensuite, déjà, dans image folder, j'ai un peu étudié en amont, il y a une série de propriétés et de méthodes, et parmi les propriétés, on a la liste des classes, l\"), ('210038', \"parmi les propriétés, on a la liste des classes, la liste des dossiers qu'il a trouvés. Donc, il a dit, il y a deux classes possibles, Bidrill et Cubone. ok, une fois qu'on a ça on a les classes d'appartenance de chaque individu cette fois-ci et on a bien les valeurs 0, 0, 0, 0 du coup c'est bidrill les bidrill sont indicés 0 et les cubons sont indicés 1 c'est ce qu'on voit ici donc si j'apparie les deux on a une série de bidrill d'abord qui sont dans ce dossier là et on a une série de cubons en\"), ('210039', \" dans ce dossier là et on a une série de cubons ensuite c'est ce qu'ils montrent ici Bien, bien, bien. Si je comptabilise les fréquences absolues, je le fais ici. Voilà. Très bien. On a 24 bidrilles et 30 cubans. Donc, regardez bien les différentes opérations, parce que je jongle entre les listes, entre les vecteurs numpy, ainsi de suite. Il y a une série de transformations à faire. Très bien. Mais bon, en soi, ce n'est pas non plus violent. On est d'accord là-dessus. Mais c'est important de bie\"), ('210040', \"st d'accord là-dessus. Mais c'est important de bien savoir ce qu'on écrit. C'est pour ça que j'ai vraiment détaillé le tutoriel pour qu'on voie bien chaque étape. Dans le tutoriel de base de CIFAR 10 qui est en ligne ici, il y a plein de raccourcis qui sont faits. Mon Dieu, on s'y perd. On s'y perd. Quand je lis ça, je me suis pris la tête, je peux vous dire. En fait, j'ai chargé le notebook, j'ai lancé pas à pas et j'ai mis des vérifications à chaque étape pour regarder exactement ce qui était \"), ('210041', \"haque étape pour regarder exactement ce qui était produit. et je me suis dit justement baser un tutoriel sur ça, c'est juste pas possible c'est juste pas possible parce que je vais perdre tout le monde et surtout les étudiants n'arriveront pas à transposer ce que je fais sur leurs données, les données réelles les images qu'ils manipulent mais bon, c'est quand même la base que j'ai utilisée, je me suis calé sous cette base là et notamment sur l'architecture du réseau qui est là, où là mes compéte\"), ('210042', \"chitecture du réseau qui est là, où là mes compétences sont moindres pour le coup et donc je me suis basé sur les idées qui sont là, en essayant de les détailler, je vais revenir là dessus à l'heure. Très bien. Alors, pour itérer sur les images, pour l'instant, on a en fait un data loader. Enfin, on a un data, comment il l'appelle là? Un data folder, un image folder. Il faut qu'on puisse itérer sur ces images-là. Et donc, il faut les mettre dans une structure à part, cette fois-ci, c'est un data\"), ('210043', \"une structure à part, cette fois-ci, c'est un data loader. Ne me dites pas pourquoi, il a besoin de ça. Pour l'instant, il est branché sur le dossier, voilà. Il faut qu'il charge réellement, voilà, des images et qu'ils les mettent dans une structure qui soit itérable pour pouvoir boucler sur chacune des images en réalité. D'où le rôle de data loader qui est là. Donc, qu'est-ce qui se passe? Il prend la référence du dossier, data train, l'image folder, et il peut les charger par bloc. Alors, just\"), ('210044', \"lder, et il peut les charger par bloc. Alors, justement, dans le tuto CIFAR, il met des blocs de quatre. Mais du coup, on a des structures, on va expliquer ça aux étudiants, c'est juste impossible en réalité. Ça rend les traitements plus performants. Je comprends parfaitement ça. Voilà. Mais du coup, on a des images qui sont empilées. En fait, on a une image 1, 2, 3, X du nombre de batch size. Si je prends un batch size 4, par exemple, j'ai 4 images empilées ensemble. Voilà. Qu'il faut manipuler\"), ('210045', \"ges empilées ensemble. Voilà. Qu'il faut manipuler d'un bloc. Je comprends. C'est plus performant, c'est plus rapide. Les traitements batch souvent sont plus rapides en utilisant des systèmes de cache. Mais les étudiants ne comprennent pas du tout la structure qu'ils manipulent. Qu'est-ce qui se passe là? Pourquoi ce batch? Pourquoi les images sont empilées? Qu'est-ce qui se passe? Ainsi de suite. Donc, justement, pour éviter ça, pour éviter ça, voilà, j'ai fait un data loader et j'ai mis un bat\"), ('210046', \"voilà, j'ai fait un data loader et j'ai mis un batch size.  égal à 1. Comme ça, les images sont chargées de manière individuelle et on peut les visualiser de manière individuelle. C'est moins performant, mais là on n'a que cinquantaine d'images, on a 54 images, on ne va pas s'énerver là-dessus. Si vous avez des centaines de milliers d'images, là ce n'est pas une bonne idée du tout des batches égales à 1, on est d'accord. Mais bon, là on est sur une base exemple avec très peu d'images, on peut se\"), ('210047', \"ne base exemple avec très peu d'images, on peut se permettre de mettre un batch égal à l'autre aspect important c'est le shuffle shuffle ça veut dire qu'il va les mélanger au hasard on sait pas comment on fait ont été triés les données ça c'est un premier élément important le deuxième élément important on a vu que lors du chargement il a pris d'abord tous les tous les bridal comment il s'appelle déjà j'ai oublié le nom tous les bri et bidrille voilà et ensuite tous les cubones du coup quand je v\"), ('210048', \"ilà et ensuite tous les cubones du coup quand je vais présenter les images à mon réseau de neurones il va d'abord traiter tous les bidrilles d'abord ensuite pour corriger les poids synaptiques les coefficients du réseau de neurones pour calculer les gradients et ensuite il va reprendre tous les curants c'est pas bon ça donc d'où l'intérêt du shuffle ça permet de les mélanger au hasard et comme ça les différentes classes sont représentés au fur et à mesure de la descente de gradients stochastique\"), ('210049', \" à mesure de la descente de gradients stochastiques donc c'est ce que je fais ici et là on a une autre structure qui est itérable cette fois-ci alors justement on va itérons sur cette structure là donc là je mets la fonction itère pour rendre la structure réellement itérable, on peut l'utiliser soit dans des boucles soit dans des listes ainsi de suite très bien, donc là je rends la structure réellement itérable au sens de Python cette fois-ci et par exemple je prends le premier élément de la str\"), ('210050', \"par exemple je prends le premier élément de la structure itérable. Next. Donc, on va prendre le premier élément. Donc, dans la structure itérable, il y a deux informations. Il y a l'image en elle-même et il y a l'étiquette. Donc, la classe d'appartenance, si vous voulez. Très bien. Donc, voilà. Et quand je récupère le premier élément de ma liste itérable, de ma structure itérable, ici, l'image, c'est un tensor cette fois-ci. Et le label, c'est une valeur numérique qui représente l'indice de la c\"), ('210051', \"e valeur numérique qui représente l'indice de la classe d'appartenance. Alors, pourquoi on a un tensor? Parce que rappelez-vous que quand on a fait le chargement, ici, on avait fait une fonction de transformation d'un volet, où il a retaillé les images pour qu'ils aient les mêmes définitions, et ensuite, il les a transformées en tensor. Donc, le type de l'image, maintenant, c'est bien un tensor. C'est ce qu'on a là. alors un tenseur on ne peut pas l'afficher directement c'est une structure qui e\"), ('210052', \"s l'afficher directement c'est une structure qui est spécifique à PyTorch ici en l'occurrence donc il faut déjà que je le mette sous forme de numpy pour voir les dimensions voilà alors déjà il y a 4 dimensions donc très bien et la première dimension c'est la taille du batch donc si j'avais mis un batch size égal à 4 tout à l'heure on aurait 4 images empilées comme j'ai mis un batch size égal à 1 on a que seule image dans ma structure. Voilà, c'est le 1 qui est là. Ensuite, 3, parce qu'il y a 3 c\"), ('210053', \"t le 1 qui est là. Ensuite, 3, parce qu'il y a 3 couleurs. Rouge, vert, bleu. Red, green, blue. Très bien, on a 3 couleurs. Ensuite, ça, c'est le nombre de pixels horizon, enfin, voilà, et le nombre de pixels verticaux. 128, le nombre de pixels sur un axe et le nombre de pixels sur l'autre. 128, 128. Très bien. Bien, bien, bien. Donc, déjà, la première dimension, là, ça ne sert à rien en réalité. c'est comme si on avait un vecteur mais c'est comme si on a une matrice de taille 1 en fait c'est un\"), ('210054', \"e si on a une matrice de taille 1 en fait c'est un vecteur là on n'a pas une matrice de taille 4 on a une matrice de taille 3 en réalité et la quatrième dimension si je puis dire, en réalité elle n'a qu'une seule valeur voilà, donc on peut l'enlever donc c'est ce qui se passe avec squeeze le rôle du squeeze c'est d'enlever la dimension égale à 1 quand je fais squeeze maintenant il a squeezé justement il a squeezé, c'est marrant ça Voilà, la première dimension. Du coup, on a 3, 128, 128. Bien, ma\"), ('210055', \"ère dimension. Du coup, on a 3, 128, 128. Bien, mais quand je vais afficher l'image, voilà, Matplotlib, lui, il ne comprend pas ça. Lui, il a besoin d'abord le nombre de pixels, voilà, d'un côté, le nombre de pixels verticaux, horizontaux verticaux, et ensuite, uniquement la profondeur de couleur. Donc, j'ai besoin de réorganiser mes données selon les axes. D'abord, mettre cette information-là, nombre de pixels horizontaux, et ensuite, 128, le deuxième élément, c'est le nombre de pixels verticau\"), ('210056', \"uxième élément, c'est le nombre de pixels verticaux. Voilà. L'un ou l'autre, voilà, enfin, l'axe sur le premier axe, sur le deuxième axe. Et ensuite, seulement, les trois couleurs. Regardez bien, d'où le rôle de transpose qu'elle a. Regardez, le 1, ça veut dire que je vais mettre en première position celle-ci. Le 2 veut dire que je vais mettre en deuxième position celui-là. Et le 0 veut dire que je vais mettre en dernière position, en troisième position, l'indice 0, parce que les indices commenc\"), ('210057', \"osition, l'indice 0, parce que les indices commencent à 0 en Python, c'est ce 3-là. Donc si j'affiche les dimensions maintenant, on a bien celles qui sont manipulées, les dimensions usuelles manipulées par Matotlib. Très bien, donc, ça y est, je peux afficher l'image. Voilà. Là également, je peux vous dire qu'il fallait bien suivre le pas à pas l'histoire. Donc, quand j'ai regardé ce qui se passait sur le tuto Sifardis de Péthorch, j'ai vu qu'il y avait plein d'éléments qui étaient absolument si\"), ('210058', \"y avait plein d'éléments qui étaient absolument sibylains, incompréhensibles, abscons, et c'était impossible de mettre ça en vrac sans expliquer chacune des étapes. Donc, c'est pour ça que j'ai fait ces 13 scolaires, je suis absolument d'accord là-dessus, mais il me fallait absolument bien expliquer les différentes dimensions pour savoir comment on a affiché l'image qu'il y a là. C'était important. Donc, suivez vraiment en détail ça, parce que quand vous allez traiter vos bases avec des milliers\"), ('210059', \"and vous allez traiter vos bases avec des milliers d'images, là, il faudra voir les batch sizes, là, pour le coup. Un batch size supérieur à 1. Et à ce moment-là, il faudra bien gérer cette dimension-ci pour pouvoir afficher chacune des images comme vous le souhaitez, correctement, en tous les cas. Bien. Ça y est, je sais manipuler l'image. Rappelez-vous, j'avais créé ici une structure itérable et j'ai récupéré la première image. Donc, j'ai récupéré l'image qui était un tensor et il a fallu fair\"), ('210060', \"éré l'image qui était un tensor et il a fallu faire une série de manipulations pour la rendre affichable. Ensuite, j'ai récupéré le label, et le label ici, c'est l'étiquette 1. Et l'étiquette 1, si je récupère les classes d'appartenance, c'est un cubon. Donc, l'image, c'est un cubon. Ok, très bien. alors si je prends l'image suivante cette fois-ci, donc je fais next de nouveau sur ma structure itérable, donc rappelez-vous j'ai créé ici la structure itérable, j'ai initialisé l'itérateur si vous v\"), ('210061', \"re itérable, j'ai initialisé l'itérateur si vous voulez j'ai pris le premier élément qui était un cubon je prends l'élément suivant et je l'affige et cette fois-ci c'est un bidrill et ensuite je peux faire next, next, next oui d'accord là-dessus Bien, donc cette partie préparatoire-là était super importante. On manipule les données, il faut qu'on comprenne comment sont organisées les données et comment nous pouvons les rendre exploitables. Si on n'a pas ça, oubliez. Oubliez la data science, oubl\"), ('210062', \"n'a pas ça, oubliez. Oubliez la data science, oubliez le machine learning, oubliez l'intelligence artificielle, parce qu'au départ, il y a les données, il faut les manipuler, il faut les présenter de la manière adéquate pour qu'on puisse faire les traitements dessus derrière. Et de manière adéquate, souvent, ça dépend de comment ça a été réfléchi par rapport au package que je veux utiliser. C'est ça. On ne peut pas faire de cours de machine learning à l'université sans parler des packages. Parce\"), ('210063', \"ing à l'université sans parler des packages. Parce qu'il y a la théorie brute, mais ensuite, il y a la manière avec laquelle les packages l'ont mis en œuvre, l'ont mis en musique. Et ça, c'est vraiment important. Là, pour le coup, c'est vraiment important. Une fois que c'est fait, on va utiliser, on va maintenant mettre en place notre réseau de neurones convolutif. Alors rappelez-vous, j'ai expliqué les principes dans les vidéos qui sont là, donc je ne vais pas trop m'attarder là-dessus. Il faut\"), ('210064', \" je ne vais pas trop m'attarder là-dessus. Il faut d'abord créer une classe de calcul. Voilà, donc j'ai créé la classe qui est là. Je vais revenir un peu ici. Donc il y a différentes parties, il y a des couches de convolution. Donc là, je crée une classe héritière, rappelez-vous dans PyTorch. On crée une classe héritière, c'est ça l'idée, de module. Voilà, c'est ça notre zone neurone. Ensuite, dans la surcharge du constructeur, on liste les objets qu'on veut utiliser, on instancie les objets qu'\"), ('210065', \"s qu'on veut utiliser, on instancie les objets qu'on veut utiliser. Et dans le forward, qui est la méthode par défaut de la classe, on réalise réellement la succession de traitements. Donc là, on a les structures dans un constructeur et dans Fortward, on définit l'enchaînement des traitements. Alors qu'est-ce qu'on a ici? On a une partie convolution, une partie max pool, donc tout ça expliqué ici. Je vous laisse regarder en détail ici. Vous avez le traitement convolutif. Ensuite, vous avez le po\"), ('210066', \"le traitement convolutif. Ensuite, vous avez le pooling. Très bien, regardez. Je vous laisse regarder parce que je ne vais pas trop m'attarder là-dessus. et à la fin, à la sortie, on a une structure vectorisée, qu'on a avec le flatten. Très bien, et donc on a une partie linéaire, et on a un perceptron complètement connecté, et avec une succession de couches dans le perceptron, complètement connecté.  Une dimension initiale, on va en discuter de la dimension initiale, qu'on réduit à 120, qu'on ré\"), ('210067', \"a dimension initiale, qu'on réduit à 120, qu'on réduit à 84 et qu'on réduit à 2 pour avoir deux classes. Et les structures qui sont là sont linéaires. Très bien. OK, mais qu'on va ensuite, sur lesquelles on applique une fonction de transfert. On est bien d'accord là-dessus. Sauf la dernière. Alors, pourquoi la dernière? Parce qu'on va utiliser une autre fonction de perte cette fois-ci, qui a besoin que les données soient présentées d'une certaine manière. Très bien. Donc, c'est pour ça tout simp\"), ('210068', \" manière. Très bien. Donc, c'est pour ça tout simplement. Donc là, on a les outils et là, on a les calculs. Donc on a une convolution sur laquelle on applique une fonction de transfert relu, très à la mode, regardez, c'est très utilisé en traitement d'image, ça désactive les valeurs négatives, tout simplement, on peut le voir comme ça. Très bien, une fois qu'on a fait cette transformation-là, on fait un max pool là-dessus. De la même manière, sur le résultat de ça, le x qui est là, vous avez vu \"), ('210069', \" le résultat de ça, le x qui est là, vous avez vu le x qui est là, il est passé en paramètres ici. On va réappliquer une convolution qui permet de mettre en évidence ces informations importantes. Ensuite, on a une fonction transfert relue qui désactive de nouveau les valeurs négatives. Et ensuite, on réapplique un pool pour réduire la dimensionnalité. Mais une fois qu'on a ce résultat-là, il faut qu'on applique un flatten pour la vectoriser. Mais quelle est la taille du vecteur? C'est ça la vrai\"), ('210070', \" quelle est la taille du vecteur? C'est ça la vraie question. Et ça, j'ai beaucoup... C'est quelque chose sur lequel les étudiants s'interrigent énormément. On a fait une série de manipulations qui permettent de réduire la dimensionnalité, mais quelle est la taille de la dimension finale, alors? Avant de faire le perceptron sur lequel je réapplique la réduction de la dimensionnalité. Quelle est la taille de la dimension ici et quand je fais le flatten? C'est pour ça qu'il faut faire un cours d'u\"), ('210071', \"atten? C'est pour ça qu'il faut faire un cours d'un spécialiste de traitement d'images qui explique exactement ces différents éléments, ça ne s'improvise pas à ça. Moi, généralement, je passe vite là-dessus et je dis aux étudiants, mais en fait, il y a des outils qui permettent de... Vous avez compris les idées globales, c'est très bien. Et on a des outils qui permettent de calculer ces éléments-là. Et c'est ce qu'on va utiliser. C'est ce qu'on va utiliser. Alors, hop là, je vais mettre ça un pe\"), ('210072', \"a utiliser. Alors, hop là, je vais mettre ça un peu par là. Et je vais mettre cet outil-là ici. Qu'est-ce qui se passe? Au départ, on a une image 128, 128, avec trois couleurs. Voilà, vous voyez d'accord là-dessus. Très bien, voilà, on va là. L'out channel, ça va être le 6 qui est là, et le kernel size, c'est 5. Vous avez bien vu, hein? Donc, on part de 3, out channel, c'est 6, kernel size, c'est S. Vous verrez ça en détail avec votre prof. Moi, j'applique les idées de qui sont là, tout simpleme\"), ('210073', \"j'applique les idées de qui sont là, tout simplement. Donc ça, c'est une première couche déjà, avec les paramètres par défaut de StrayDepadding. Ça, également, vous regardez en détail avec votre prof. Ça, c'est la première couche. Elle est là. Ensuite, ça, c'est le conf 2D. Vous voyez, d'accord. Je fais un max pool, voilà. Et dessus, j'ai réduit à 2,2. C'est ça, hein. J'ai fait d'abord un conf 2D, qui était avec un transfert relu. et ensuite je fais un max pool. Voilà, 2, 2. Ensuite, je rajoute \"), ('210074', \"ais un max pool. Voilà, 2, 2. Ensuite, je rajoute de nouveau un conf 2D qui prend cette fois-ci 16 en hôte et toujours 5. Donc le 6, était le 6 qui est là, le 16, c'est celui-là, et le 5. Je rajoute ça et je rajoute de nouveau un max pool. Et voilà la dimension finale. Chanel, hauteur, largeur. Hauteur des pixels et de largeur. Voilà. Donc, c'est en utilisant cette information-là que je vais calculer la dimension du flatten qui est là. C'est très important parce que sinon, ça ne marchera pas. Si\"), ('210075', \" important parce que sinon, ça ne marchera pas. Sinon, ce n'est pas cohérent. Vous faites du calcul matriciel, il va falloir du produit matriciel. Si vous faites des produits matriciels sur des matrices qui ne sont pas compatibles, ça ne marchera pas. Ça ne peut pas marcher. Donc, ce calcul de la dimensionnalité-là, elle est super importante. Donc, j'ai utilisé ce site-là excellent, excellent voilà il y a un github là, ça vous permet d'avoir des détails là dessus, mais moi j'ai utilisé ce site l\"), ('210076', \"détails là dessus, mais moi j'ai utilisé ce site là qui permet d'avoir ça, mais c'est vraiment très important, c'est important de pouvoir calculer les dimensions du vecteur flatinisé bien une fois que c'est bon là, du coup je monte la structure, très bien et je vais revenir, donc on voit bien les calculs Ensuite, il fait un relu sur le flat-en 1, ici avec réduction de dimension. Il refait un relu sur le flat-en 2, avec réduction de dimension. Et ensuite, il refait un linéaire. Pas besoin de relu\"), ('210077', \"ensuite, il refait un linéaire. Pas besoin de relu cette fois-ci. Avec deux classes, il ne faut surtout pas faire de relu cette fois-ci. Alors, les valeurs qu'on a là, comme on a ici une fonction linéaire, les valeurs varient de moins l'infini à plus l'infini. Et en fait, on a une fonction d'appartenance, une fonction d'appartenance, c'est en verbalité, qui n'est pas forcément normalisée, donc ce ne sont pas des probas cette fois-ci, mais qui fonctionne comme des probas. C'est comme des fonction\"), ('210078', \"ctionne comme des probas. C'est comme des fonctions score d'appartenance, si vous voulez, qui soit négatives, soit positives, qui partent de moins infini à plus l'infini, mais le argmax vous donne bien la classe prédit. Alors, j'aurais pu faire un softmax, mais à ce moment-là, il fallait bien faire attention à la fonction de perte qu'on veut utiliser par la suite. Donc là, on n'a pas fait Softmax. Pourquoi? Parce qu'on va utiliser une fonction de perte spécifique. Très bien. Alors, une fois que \"), ('210079', \" perte spécifique. Très bien. Alors, une fois que j'ai créé ma structure, je l'instancie. Donc ça y est, elle est instanciée maintenant. Qu'est-ce qui se passe? Comme d'habitude, il faut définir la fonction de perte et l'algorithme d'optimisation. Alors, pour la fonction de perte, pour varier les plaisirs, on est d'accord là-dessus, j'ai pris la grosse entreprilose. Et c'est là qu'il faut lire la doc. Je lui dis tout le temps à mes étudiants, lisez la doc des logiciels, des packages que vous uti\"), ('210080', \"ez la doc des logiciels, des packages que vous utilisez. Sinon, vous ne savez pas comment ça a été fait, vous ne savez pas comment ça a été interprété par le programmeur, des gens comme vous et moi. Et du coup, vous faites des traitements sans savoir et vous avez des résultats, vous ne comprenez pas leur teneur. C'est juste impossible. Avec la cross, j'aurais pu utiliser un MSE. À ce moment-là, il fallait que j'organise différemment mes données. Vous êtes d'accord là-dessus. Là, en l'occurrence,\"), ('210081', \"Vous êtes d'accord là-dessus. Là, en l'occurrence, j'ai choisi de prendre le cross-anthropylos et il me dit, il y a deux éléments à opposer. les inputs, donc les sorties de votre réseau de neurones, cette fois-ci, même si ça s'appelle input, il nous dit, on va utiliser les logites non normalisés de chaque classe. C'est pour ça qu'il fallait prendre un linéaire à la sortie, ici, et non pas un softmax en l'occurrence, linéaire à la sortie, ici, parce qu'avec le crocentropilos, en entrée, on prend \"), ('210082', \"ce qu'avec le crocentropilos, en entrée, on prend les logites non normalisés. qui vous donne un degré d'appartenance à la classe en réalité, mais qui n'est pas normalisé. No need to be positive or sum to 1. Tout simplement, on prend le argmax. Ce n'est pas une proba, mais ça se comporte comme une proba, comme le Canada Dry, si vous voulez. Bon, je ne vais pas faire de la pub pour des produits, mais c'est ça. Ce n'est pas une proba, mais en fait, ça vous donne le degré d'appartenance. Et donc, en\"), ('210083', \"ça vous donne le degré d'appartenance. Et donc, en prenant le argmax, on a la classe prédite. Ça, c'est le premier élément. et il va opposer à l'indice de classe. Donc là, pour le coup, avec cette fonction de perte-là, on n'a pas besoin de créer des matrices 0,1 en classement multiclasse. Ce n'est pas nécessaire. Il suffit de prendre les indices de classe qui sont codés 0,1,2,3, ainsi de suite, selon le nombre de classes d'appartenance. Donc, en multiclasse, n'allez pas faire une matrice de 0,1 \"), ('210084', \"multiclasse, n'allez pas faire une matrice de 0,1 comme je l'ai fait moi-même ici. Moi, j'ai créé la matrice de 0,1 ici, parce que j'utilise le critère, le MSE, le min score d'erreur. Si on veut utiliser le cross-loss-entropie, il faut prendre les indices de classe, coder 0,1,2 jusqu'à k, k-1, parce qu'on commence à 0 les indices. Alors, nous, ça marche pourquoi? Parce qu'on a vu que les classes sont codées 0,1. on l'a vu ici ça target que des 001 donc c'est bon ça c'est bon, on est vraiment ici\"), ('210085', \"1 donc c'est bon ça c'est bon, on est vraiment ici sur des indices de classe on a que deux classes en l'occurrence mais c'est bien les indices de classe 0 c'est bidrill et 1 c'est kubon je continue il faut expliquer plein de choses ça y est c'est chargé donc qu'est-ce qu'il faut maintenant il faut programmer le fit Alors, la programmation du fit reste classique de tout ce qu'on voit avec PyTorch. Qu'est-ce qui se passe? Voilà, on définit un nombre de pages, donc des poches. C'est le nombre de pa\"), ('210086', \"e de pages, donc des poches. C'est le nombre de passages sur la base entière. Très bien. Ensuite, pour chaque élément à traiter, voilà. Ici, pour le coup, je vais créer, je vais prendre un élément éthérable. Je récupère les données, donc l'image et son étiquette correspondante. Je mets à zéro les gradients, parce qu'on va refaire un calcul des gradients, justement. Voilà. Je calcule la sortie du réseau appliquée sur l'image. Le input là, c'est l'image. Le input là, c'est l'image. Voilà. Et j'ai \"), ('210087', \"image. Le input là, c'est l'image. Voilà. Et j'ai la sortie du réseau. Ensuite, je vais calculer la fonction de perte entre la sortie du réseau et la vraie classe d'appartenance. Très bien. En utilisant le cross-anthropylos. besoin que les données soient présentées d'une certaine manière. Vous avez bien vu, très bien. Et ensuite, on calcule les gradients, à partir de la perte calculée, on calcule les gradients et on met à jour les poids synaptiques. Et une fois que c'est fait ça, on met à jour l\"), ('210088', \"es. Et une fois que c'est fait ça, on met à jour les poids synaptiques, je suis parlé trop vite, on calcule les gradients et on les rétropropage. C'est ce que fait le backward. Et ensuite, on met à jour les poids synaptiques. Une fois que c'est fait, je récupère la loss, l'additionne, parce qu'on l'a calculé sur chaque individu, et il faut le calculer sur un époche, sur l'ensemble d'un individu d'un époche, sur l'ensemble de la base train, en fait. Voilà. Et très bien, je la stocke. Donc, le los\"), ('210089', \"t. Voilà. Et très bien, je la stocke. Donc, le loss is là, c'est le stockage. Alors, il y a un élément que je n'avais pas dit parce que je suis passé un peu vite là-dessus, c'est l'algo d'apprentissage. Il y a différents d'algo, il y a la descente de gradient stockacique classique, moi, j'ai choisi ici le ADAM. Adam, en fait, c'est une variante, tout simplement, des centres de gradient stochastique, mais qui est censée être plus efficace, plus performante. C'est ce que dit, en tous les cas, l'ar\"), ('210090', \"rformante. C'est ce que dit, en tous les cas, l'article. L'article est là, vous pouvez cliquer là-dessus, je suis allé le voir, bien sûr. Et donc, par rapport aux tutos qu'on voit en ligne, où c'est CIFAR10 qui est utilisé, là, en l'occurrence, je me suis dit, on va changer au lieu de SGD, j'ai pris Adam, tout simplement, qui est une variante. tout simplement. Mais ça influe sur la qualité de l'apprentissage, on est bien d'accord. En tout cas, sur la rapidité d'apprentissage. Voilà. C'est sur la\"), ('210091', \"r la rapidité d'apprentissage. Voilà. C'est sur la convergence. Donc, ces deux éléments-là sont très importants. Alors, j'en reviens du coup. Je peux lancer d'ici l'entraînement du modèle et là, ça prend un peu de temps. Là également, le jour, on fournira un. Enfin bref, très bien. Donc, j'ai fait 15 époques, mais à bout de 15, ça converge très bien. Donc, je vais lancer et je vais suspendre la vidéo. Et je reprends quand c'est terminé. Je lance. Voilà, 15 époques. Je récupère chaque valeur des \"), ('210092', \" Voilà, 15 époques. Je récupère chaque valeur des pertes pour chaque époche. Allons-y. Donc, ça va commencer. Très bien. Ça va descendre. Je suspends la vidéo et je reprends. Bien, je reprends. Donc, il a fini ici. Vous avez vu, il a fini l'apprentissage. et la fonction de perte est partie de 38, il est un peu backnodé au fur et à mesure, et enfin, il a plutôt convergé, il n'est quasiment 0 là. C'est très bien. Déjà, vérifions pour une image. Je prends une image suivante de mon data ITER. Vous a\"), ('210093', \"prends une image suivante de mon data ITER. Vous avez vu, j'avais créé une structure itérable pour itérer sur chacune des images. Je prends une image suivante. On voit bien que c'est Bidrill. Regardons, on voit bien, j'applique le modèle sur l'image, qui est un tensor ici. et voilà le score d'appartenance pour Bidrill et voilà le score d'appartenance pour Kubon et j'avais dit, on prend en fait l'argmax donc la valeur qui maximise la fonction d'appartenance tout simplement qui n'est pas normalisé\"), ('210094', \"partenance tout simplement qui n'est pas normalisée on voit bien qu'on a des valeurs négatives et la somme ne fait pas 1 c'est exactement ce qui a été dit dans le crocentropie voilà, très bien mais en revanche pour attribuer la classe d'appartenance on peut calculer le argmax ici c'est 0 et c'est bien un bidrill rappelez-vous c'est 0 c'est bidrill et 1 c'est cubone donc on voit bien que c'est 0 et c'est bien un bidrill donc il classe plutôt bien il classe plus trop bien et la vraie classe d'appa\"), ('210095', \"il classe plus trop bien et la vraie classe d'appartenance c'était 0 qu'on obtient ici donc il classe plutôt bien si on veut obtenir que les chiffres directement, parce que là, on a une liste, on met ici entre crochets le premier élément de la liste. Alors, pourquoi je détaille ça? Parce qu'après, je vais l'utiliser sur l'échantillon test. Donc, il faut que tout le monde contremise un. Donc, là, j'ai la classe prédite. Là, j'ai la vraie classe d'appartenance, mais elle est sous forme de liste. E\"), ('210096', \"appartenance, mais elle est sous forme de liste. Et donc, pour accéder au premier élément de la liste, je mets double crochet zéro, tout simplement. Et si on veut avoir la classe explicité, je fais comme ceci. Qu'est-ce qui se passe alors? Maintenant, j'en suis à ce stade-là. Je vais prendre chaque individu de l'échantillon test et je vais appliquer les réseaux dessus pour savoir si c'est un Bidrill ou un Cubone. Sauf que j'ai plusieurs images ici. Et on va voir ce qui va se passer. Alors rappel\"), ('210097', \"i. Et on va voir ce qui va se passer. Alors rappelez-vous, l'échantillon test, il est ici. L'échantillon test a la même structure, le sous-dossier test a la même structure que celui de train, mais il n'y a pas les mêmes données, on est bien d'accord là-dessus. Donc on a le sous-dossier bidrill avec les images associées et on a le sous-dossier gubon avec les images associées. Donc premier élément, il faut que je charge le dossier test en image folder et ensuite que je mette en structure itérable,\"), ('210098', \"der et ensuite que je mette en structure itérable, en data loader. Et ensuite, je vais appliquer la prédiction sur chaque image de mon data loader. C'est ce que je vais faire ici. Donc, je charge l'image folder. Je me branche sur le dossier image test. Sous dossier image test de démo. Et j'affiche les classes d'appartenance. Donc, on a la série des bidrills et on a la série des cubons. ensuite cette structure là qui est pour l'instant branchée sur le dossier voilà je la transforme en une structu\"), ('210099', \"r le dossier voilà je la transforme en une structure itérable donc je mets en data lauteur et de nouveau je mets un BAT16 égal à 1 sinon il faudrait faire des prédictions par bloc ce qui rend la chose plus compliquée là au moins on pourra faire une prédiction élément par élément donc le BAT16 égal à 1 est super important encore une fois ici pour qu'on fasse une prédiction élément par élément Sinon, il faudrait faire une prédiction par bloc, gérer les... Enfin, à vous de gérer ça. Très bien. Donc\"), ('210100', \" les... Enfin, à vous de gérer ça. Très bien. Donc, le batch les A1 est super important, encore une fois ici. Et ensuite, voilà, je prends chaque objet, voilà, de mon dataset loader, de mon datatest loader, voilà, de ma structure littérable. Je récupère l'image et son label. Je calcule la sortie du réseau sur l'image. Ensuite, je recris qu'à... Je calcule la classe prédit, je récupère la classe réelle et j'affiche les deux listes. Classe prédit et classe réelle. Très bien. On a bien les différen\"), ('210101', \"t classe réelle. Très bien. On a bien les différents éléments. Je me suis embêté à mettre plein de commentaires pour que ce soit bien lisible. En français, on est d'accord là-dessus. Très bien. Donc là, on a les prédictions de notre réseau. Voilà, en appliquant les différents éléments pour chaque image de l'échantillon test. Et là, on a les vraies classes d'appartenance. Qu'est-ce qu'il reste à faire? il faut confronter les deux listes tout simplement, en mettant sous forme de vecteur numpy. C'e\"), ('210102', \"ement, en mettant sous forme de vecteur numpy. C'est ce que je fais ici. Et j'ai une accuratie de 90%. Voilà ce qui se passe. J'ai fait un traitement très simple, on est d'accord là-dessus. Encore une fois, le deep learning, je vois à peu près, plutôt pas mal, même très bien. Mais le deep learning, pour moi, je l'applique plutôt surtout dans le LLP, si vous voulez, dans le text mining, dans le traitement des données textuelles. Traitement d'images, ce n'est pas trop ma spécialité, mais j'ai suff\"), ('210103', \"s, ce n'est pas trop ma spécialité, mais j'ai suffisamment de background pour m'adapter. Alors, quand je m'adapte, généralement, je fais quand même des exercices avec mes étudiants, mais souvent, j'utilise Kera Stenserflow. On peut le faire avec PyTorch. Je sais que certains de vos enseignants vous le demandent. Voilà, une idée globale, un traitement simple qui permet de traiter une base d'image de votre propre crue, votre propre base d'image. Donc, je vais vous montrer les différentes étapes. I\"), ('210104', \"nc, je vais vous montrer les différentes étapes. Il y a plein d'endroits où j'ai fait très simple. On est d'accord là-dessus, parce que l'idée est d'expliquer les choses. Mais il y a plein d'optimisations possibles. Et l'autre aspect également, c'est le modèle prédictif. Comment améliorer le modèle prédictif? Donc, il y a deux choses ici. Un, pour l'amélioration, les pistes, il faut aller sur le site du gars qui a traité les données Pokémon. Il parle de data augmentation. Donc, il a mis base mod\"), ('210105', \"arle de data augmentation. Donc, il a mis base modèle, c'est à peu près l'idée que moi j'ai mis en œuvre. Il dit qu'on peut améliorer le modèle avec la data augmentation. Et l'autre élément important également, c'est de travailler sur des modèles plus complexes, avec le choix des paramétrages extrêmement complexes. Mais bon, vous avez un cours là-dessus, donc vous êtes capable de le faire. Ça, c'est le premier élément sur lequel je voulais revenir. Le deuxième élément, on aurait pu faire la même\"), ('210106', \"r. Le deuxième élément, on aurait pu faire la même chose avec Air. Souvent, il y a des gens qui disent, ouais, avec Air, on ne peut pas faire du deep learning, etc. OK, OK, OK, deux, trois requêtes Google et ça le fait. donc moi je savais parce que je m'intéresse beaucoup à R aussi et je suis allé voir tout simplement Torch R, Torch for R et je vais regarder il y a aussi Torch Fichon et d'ailleurs, je le dis à la fin j'avais commencé à faire mon tuto avec Torch R à la base, je me suis dit on va \"), ('210107', \"tuto avec Torch R à la base, je me suis dit on va varier les plaisirs et tout ça finalement j'ai abandonné l'idée, pourquoi? parce que je sais très bien que mes étudiants vont travailler sur Python, avec PyTorch Donc, faire un tuto sur quelque chose qu'ils ne vont pas utiliser, je ne voyais pas trop d'utilité. Mais sachez qu'on peut faire exactement le tutoriel que je viens de faire là, on peut le faire exactement de la même manière, avec la même trame, avec Torcher. Et vous avez tous les outils\"), ('210108', \" trame, avec Torcher. Et vous avez tous les outils pour le faire convenablement. Je vous enjoins à le faire, ça peut être un exercice intéressant. Peut-être que je vais le demander dans mon cours de R. J'ai un cours de programmation R par ailleurs. Alors, c'est un cours de programmation, mais bon, c'est peut-être une idée ça. Bon, à voir, à voir. Il faudrait que je réfléchisse un peu là-dessus. Voilà, excellent travail à tous.\"), ('220001', \"### Utilisation d'un Perceptron Simple dans un Problème Multiclasse avec PyTorch\\n\\nDans cette vidéo, nous allons aborder l'utilisation d'un perceptron simple pour résoudre un problème multiclasse. Pour ce faire, nous utiliserons la base de données Iris, qui est bien connue et souvent utilisée dans les tutoriels et exercices.\\n\\n#### Schéma du Perceptron Simple\\n\\nLe perceptron simple que nous allons utiliser a trois sorties, car la variable cible a trois modalités : Setosa, Versicolor, et Virginica. \"), ('220002', \"ois modalités : Setosa, Versicolor, et Virginica. Cela nous place dans un problème multiclasse.\\n\\n#### Particularités de l'Utilisation de PyTorch\\n\\nLa particularité de cette vidéo est que nous allons utiliser la librairie PyTorch, qui est bien connue et récemment utilisée dans une vidéo précédente pour un perceptron multicouche dans un problème binaire. PyTorch est une librairie puissante et flexible, mais elle peut être complexe pour les débutants. Pour aider les étudiants à mieux comprendre et u\"), ('220003', \". Pour aider les étudiants à mieux comprendre et utiliser PyTorch, j'ai décidé de créer une série de vidéos sur Python.\\n\\n#### Préparation des Données\\n\\nPour préparer les données, nous devons d'abord charger le fichier Iris. Nous avons quatre variables explicatives (caractéristiques des fleurs) et une variable cible (espèce de la fleur). Nous allons centrer et réduire les données pour les rendre compatibles avec PyTorch.\\n\\n#### Modélisation avec PyTorch\\n\\nNous allons créer un perceptron simple avec \"), ('220004', \"orch\\n\\nNous allons créer un perceptron simple avec trois sorties. Pour cela, nous devons encoder la variable cible en utilisant un encodage disjonctif complet (OneHotEncoder). Ensuite, nous devons utiliser une fonction de transfert softmax pour normaliser les probabilités.\\n\\n#### Structure du Réseau\\n\\nLa structure du réseau comprend une couche d'entrée, une couche de sortie, et des combinaisons linéaires entre les couches. Nous devons définir les coefficients et les intercepts pour chaque modalité \"), ('220005', \"efficients et les intercepts pour chaque modalité de la variable cible.\\n\\n#### Apprentissage et Évaluation\\n\\nNous allons entraîner le modèle sur les données d'apprentissage et évaluer sa performance sur les données de test. Nous devons également définir la fonction de perte et l'algorithme d'optimisation.\\n\\n#### Évaluation des Prédictions\\n\\nPour évaluer les prédictions, nous devons transformer les probabilités en un vecteur de prédiction et créer une matrice de confusion pour comparer les prédiction\"), ('220006', \" matrice de confusion pour comparer les prédictions avec les vraies valeurs.\\n\\n#### Conclusion\\n\\nEn résumé, nous avons montré comment utiliser un perceptron simple avec PyTorch pour résoudre un problème multiclasse. Nous avons préparé les données, défini la structure du réseau, entraîné le modèle, et évalué les prédictions. PyTorch offre une grande flexibilité, mais nécessite une compréhension approfondie des méthodes et des outils disponibles.\\n\\nMerci d'avoir regardé cette vidéo. N'hésitez pas à p\"), ('220007', \"rci d'avoir regardé cette vidéo. N'hésitez pas à poser des questions ou à laisser des commentaires si vous avez des doutes ou des suggestions. Bonne chance dans vos projets de machine learning !\"), ('230001', '### Correction et Amélioration du Texte\\n\\nBien sûr, voici une version corrigée et améliorée du texte :\\n\\n---\\n\\n**Introduction :**\\n\\nDans cette vidéo, je vais parler de deep learning et en particulier de la librairie PyTorch. PyTorch est une bibliothèque développée par Facebook qui est largement utilisée dans le domaine du deep learning. Vous pouvez trouver la page web de PyTorch [ici](https://pytorch.org/) et la page Wikipédia [ici](https://fr.wikipedia.org/wiki/PyTorch).\\n\\n**Utilisation de PyTorch :'), ('230002', \"dia.org/wiki/PyTorch).\\n\\n**Utilisation de PyTorch :**\\n\\nÀ l'université, nous travaillons beaucoup sur le deep learning. Nous utilisons des techniques de machine learning classiques, comme les données tabulaires, le traitement du langage naturel (NLP), et les données d'images. Chaque enseignant utilise ses propres bibliothèques, mais ces dernières années, les étudiants ont souvent du mal à s'adapter à PyTorch.\\n\\n**Problèmes rencontrés :**\\n\\nLes étudiants me disent souvent qu'ils ont du mal à comprend\"), ('230003', \"nts me disent souvent qu'ils ont du mal à comprendre PyTorch. Cependant, je trouve que PyTorch est une bibliothèque très puissante et flexible. Pour les aider, j'ai créé un tutoriel sur PyTorch en 2019. Ce tutoriel est disponible [ici](https://github.com/username/tutorial-pytorch).\\n\\n**Analyse des tendances :**\\n\\nJ'ai fait un Google Trends pour voir l'évolution de l'intérêt pour PyTorch. En 2019, il y avait une augmentation linéaire de l'intérêt pour PyTorch. En 2024, l'intérêt reste relativement \"), ('230004', \"ur PyTorch. En 2024, l'intérêt reste relativement stable, avec des pics au printemps, probablement en raison des projets universitaires.\\n\\n**Tutoriel PyTorch :**\\n\\nPour aider les étudiants à mieux comprendre PyTorch, j'ai décidé de créer un atelier de remise à niveau. Ce tutoriel sera disponible [ici](https://github.com/username/tutorial-pytorch-2024).\\n\\n**Structure du Tutoriel :**\\n\\nLe tutoriel commence par la création d'un environnement Python avec les packages nécessaires. Ensuite, nous allons ch\"), ('230005', \" les packages nécessaires. Ensuite, nous allons charger les données et les préparer pour l'apprentissage. Nous allons créer un perceptron simple et un perceptron multicouche, et enfin, nous allons évaluer les modèles sur un ensemble de test.\\n\\n**Création de l'Environnement :**\\n\\nPour créer un environnement propre, nous allons installer Python 3.11 et les packages nécessaires, comme Pandas, Scikit-learn, et Seaborn.\\n\\n**Chargement et Préparation des Données :**\\n\\nNous allons charger les données et le\"), ('230006', \"Données :**\\n\\nNous allons charger les données et les préparer pour l'apprentissage. Nous allons centrer et réduire les données pour les mettre à l'échelle.\\n\\n**Création du Perceptron Simple :**\\n\\nNous allons créer un perceptron simple avec une couche d'entrée et une couche de sortie. Nous allons définir la structure du réseau et la fonction de transfert.\\n\\n**Création du Perceptron Multicouche :**\\n\\nNous allons créer un perceptron multicouche avec une couche intermédiaire et une couche de sortie. Nous\"), ('230007', \"couche intermédiaire et une couche de sortie. Nous allons définir la structure du réseau et la fonction de transfert.\\n\\n**Évaluation des Modèles :**\\n\\nNous allons évaluer les modèles sur un ensemble de test et calculer l'accuracy.\\n\\n**Conclusion :**\\n\\nPyTorch est une bibliothèque puissante et flexible. Avec un peu de pratique, les étudiants pourront facilement s'adapter à PyTorch. J'espère que ce tutoriel les aidera à mieux comprendre et à utiliser PyTorch.\\n\\n---\\n\\nJ'espère que cette version corrigée \"), ('230008', \"yTorch.\\n\\n---\\n\\nJ'espère que cette version corrigée et améliorée du texte est claire et fluide. N'hésitez pas à me faire savoir si vous avez besoin de plus d'aide !\"), ('240001', \"Bien, c'est parti ! Dans cette vidéo, je vais parler du package e-p-widget, qui permet de créer des formulaires interactifs dans des notebooks. Pour que tout le monde puisse profiter de cette vidéo, je vais rapidement resituer les choses.\\n\\nLorsque l'on crée des modèles de machine learning, il est crucial de penser aux services après-vente. On crée le modèle, il fonctionne bien, et une fois que c'est bon, on pense que ce n'est plus notre problème. Mais en réalité, c'est toujours notre problème, n\"), ('240002', \" Mais en réalité, c'est toujours notre problème, notamment de créer une application interactive qui permet aux utilisateurs d'exploiter le modèle. Que ce soit pour le classement supervisé sur des données standards, le classement d'images, ou même en non-supervisé, comme la classification automatique où l'idée est de rattacher des individus à des groupes constitués, il est important de créer ces applications.\\n\\nPour créer ces applications, il existe différentes pistes. J'en parle souvent avec mes \"), ('240003', \"e différentes pistes. J'en parle souvent avec mes étudiants, notamment ceux du Master 1 de cette année. Je leur ai dit que j'aimerais qu'à la fin, ils aient une application qui permette de saisir à la main les caractéristiques des individus et de reconnaître d'une part le cluster d'appartenance, et d'autre part, les caractéristiques des clusters qui expliquent l'appartenance. Il est important d'expliquer pourquoi un individu est rattaché à un groupe, en mettant en avant les caractéristiques qui \"), ('240004', \"upe, en mettant en avant les caractéristiques qui justifient ce rattachement.\\n\\nJ'ai fait des exemples avec Streamlit, Gradio, et même une application console. Cependant, il est toujours difficile de trouver le bon équilibre entre donner des indications trop précises et ne pas en donner suffisamment. Si je donne des indications trop précises, les étudiants se contentent de répéter ce que j'ai dit. Si je ne donne pas d'indications, ils ne savent pas quoi faire.\\n\\nDans l'exercice que j'ai donné à me\"), ('240005', \"s quoi faire.\\n\\nDans l'exercice que j'ai donné à mes étudiants, il y avait trois parties : clustering, interprétation et création d'une application. J'ai donné des indications, mais j'ai insisté sur le fait que, vu le temps imparti, une application console faisait très bien l'affaire.\\n\\nJ'ai découvert un projet très intéressant réalisé par mes étudiants, qui ont utilisé IP Widget. Cet outil permet de créer des interfaces visuelles pour créer des applications interactives. Je ne savais pas qu'on po\"), ('240006', \"plications interactives. Je ne savais pas qu'on pouvait créer des formulaires dans un notebook avec cet outil. Curieux comme je suis, j'ai exploré cela et j'ai trouvé des vidéos explicatives sur YouTube.\\n\\nJe vais maintenant vous montrer comment utiliser IP Widget pour créer une interface graphique dans un notebook. Je vais utiliser Anaconda comme environnement de base. IP Widget est présent par défaut, ce qui est pratique. J'ai mis à jour la version pour m'assurer qu'elle est récente.\\n\\nPour affi\"), ('240007', \"ion pour m'assurer qu'elle est récente.\\n\\nPour afficher les éléments graphiques dans l'interface, j'utilise l'outil display du package ePython. Je vais d'abord créer une application console dans un notebook, bien que je ne recommande pas cette méthode. Ensuite, je vais montrer comment utiliser IP Widget pour créer une interface graphique plus esthétique.\\n\\nPour créer une application console, je vais saisir un prix hors taxe, appliquer la TVA et afficher le prix TTC. Cependant, l'affichage apparaît\"), ('240008', \"icher le prix TTC. Cependant, l'affichage apparaît dans la barre de recherche du notebook, ce qui n'est pas très joli. Pour une application console, il est préférable d'utiliser un fichier.py séparé.\\n\\nPour créer une interface graphique avec IP Widget, je vais utiliser un textbox pour saisir la valeur, un bouton pour calculer et afficher le résultat. Voici le code pour créer cette interface :\\n\\n```python\\nimport ipywidgets as ipw\\nfrom IPython.display import display\\n\\n# Créer un textbox pour saisir l\"), ('240009', \"y import display\\n\\n# Créer un textbox pour saisir la valeur\\ntextbox = ipw.FloatText(description='Prix hors taxe:', value=0.0)\\ndisplay(textbox)\\n\\n# Créer un bouton pour calculer le prix TTC\\nbutton = ipw.Button(description='Calculer prix TTC')\\ndisplay(button)\\n\\n# Fonction pour calculer le prix TTC\\ndef calculate_ttc(event):\\n    value = textbox.value\\n    ttc = value * 1.2\\n    display(ipw.HTML(f'Prix TTC: {ttc:.2f}'))\\n\\n# Associer la fonction à l'événement de clic du bouton\\nbutton.on_click(calculate_ttc)\"), ('240010', \"t de clic du bouton\\nbutton.on_click(calculate_ttc)\\n```\\n\\nLorsque vous cliquez sur le bouton, la fonction `calculate_ttc` est appelée, qui récupère la valeur saisie dans le textbox, multiplie par 1.2 et affiche le prix TTC.\\n\\nPour intégrer l'affichage du résultat directement dans l'interface graphique, vous pouvez utiliser un objet de sortie supplémentaire :\\n\\n```python\\n# Créer un objet de sortie pour afficher le résultat\\noutput = ipw.HTML()\\ndisplay(output)\\n\\n# Mettre à jour la fonction pour afficher\"), ('240011', \"output)\\n\\n# Mettre à jour la fonction pour afficher le résultat dans l'objet de sortie\\ndef calculate_ttc(event):\\n    value = textbox.value\\n    ttc = value * 1.2\\n    output.value = f'Prix TTC: {ttc:.2f}'\\n```\\n\\nEn utilisant IP Widget, vous pouvez créer des interfaces graphiques interactives directement dans vos notebooks. Vous pouvez également utiliser des objets supplémentaires comme des checkboxes, des drop-downs, et des radio-buttons pour enrichir vos applications.\\n\\nPour les étudiants qui créeron\"), ('240012', \" vos applications.\\n\\nPour les étudiants qui créeront des applications l'année prochaine, cette méthode pourrait être une solution intéressante. Il est important de lire la documentation pour explorer toutes les fonctionnalités disponibles.\\n\\nEn conclusion, IP Widget est un outil puissant pour créer des applications interactives dans des notebooks. N'hésitez pas à l'explorer et à l'utiliser pour vos projets. Merci à mes étudiants pour leur travail et leur découverte de cet outil.\"), ('240013', 't leur découverte de cet outil.'), ('250001', \"Bien, c'est parti. Dans cette vidéo, je vais parler du déploiement des modèles de clustering sur les individus supplémentaires. Pour cela, nous allons développer une web app très simplifiée avec Streamlit, qui permet de créer des applications très simples avec quelques lignes de code. Résumons vite les choses pour que tout le monde comprenne bien de quoi je parle.\\n\\nJ'ai terminé mon cours de clustering pour cette année, en tous les cas, dans notre Master Informatique. Comme d'habitude, je finis l\"), ('250002', \" Master Informatique. Comme d'habitude, je finis la session, l'année, le semestre, par une double évaluation, une un peu plus théorique et une pratique sur machine. Et je prépare les étudiants pour ça. Bien sûr qu'il faut les préparer. Donc, je leur ai dit, systématiquement, la trame est la même. Après, ce que vous faites réellement dépend de ce qu'il y a dans les données, des caractéristiques des données, et puis des caractéristiques de l'étude. Mais la trame globale reste un peu la même. Et j'\"), ('250003', \" Mais la trame globale reste un peu la même. Et j'en ai parlé dans cette vidéo que j'avais mise en ligne l'année dernière, pour les étudiants de l'année dernière justement, on voit bien la date, mars 2023, c'est bien à ce moment-là qu'on finit ce cours-là. Et je leur ai dit que l'idée c'est qu'on a des données, et on met en place un algorithme de clustering. de classification automatique en français.\\n\\nAlors, qu'est-ce qui se passe? Il y a un premier problème, c'est qu'il faut identifier les clus\"), ('250004', \"ier problème, c'est qu'il faut identifier les clusters et inclure dans cette problématique-là le choix du nombre de groupes, de clusters. Donc, ça, c'est un premier élément. Choisir le nombre de clusters, ensuite délimiter les clusters. Ensuite, une fois qu'on a créé les groupes, il faut les interpréter, la deuxième partie. Et ensuite, il y a une troisième partie très importante, c'est le déploiement. la mise en place de votre modèle. Il faut le mettre en place dans les systèmes d'information de\"), ('250005', \"mettre en place dans les systèmes d'information des entreprises. On néglige souvent ça dans les formations data science, mais pourtant c'est extrêmement important. Étudier les techniques de manière théorique, c'est très bien, c'est très bien, je ne dirais jamais le contraire, mais après il faut les mettre en œuvre, il faut les mettre en application, et ça passe par le montage d'applications. Donc cette partie, cette troisième partie, j'y suis extrêmement attaché.\\n\\nL'année dernière, j'avais deman\"), ('250006', \"rêmement attaché.\\n\\nL'année dernière, j'avais demandé aux étudiants de classer automatiquement des individus décrits, qui sont décrits dans un fichier Excel, qui sont recensés dans un fichier Excel, décrits par les variables. Cette année, bien sûr que j'ai changé. Bien sûr que j'ai changé, et je voulais créer une application qui permet de rentrer les caractéristiques des individus, de cliquer sur un bouton quelconque ou de faire une action quelconque, et avoir les clusters d'appartenance avec les\"), ('250007', \"que, et avoir les clusters d'appartenance avec les distances au cluster, et avec des éléments d'interprétation. Voilà ce que je demandais.\\n\\nBon, moi ici je vais travailler sur Streamlit mais je leur ai dit, vous pouvez faire une application console ça marche très bien également. On peut faire une application console, on peut faire une application console, on saisit les caractéristiques avec des inputs tout simplement, c'est niveau première année et une fois qu'on a saisi les caractéristiques des\"), ('250008', \"et une fois qu'on a saisi les caractéristiques des individus avec des inputs on appelle le pipeline, le modèle qui est recensé dans un pipeline et on a les résultats avec predict et avec transform les pipelines j'en ai suffisamment parlé bon ça c'était l'idée globale, ensuite quand j'ai présenté ça aux étudiants, il fallait voir leur tête mais bon je connais suffisamment les étudiants pour savoir qu'ils râlent toujours d'abord c'est un atavisme venu de je ne sais où mais bon ils râlent toujours,\"), ('250009', \"enu de je ne sais où mais bon ils râlent toujours, c'est le premier réflexe et ensuite quand ils sont dans l'action ils le font, bien sûr qu'ils le font c'est ça et c'est ce que je veux, je veux absolument que les étudiants sachent être dans l'action. C'est très important. Il faut avoir du recul, bien sûr. Il faut réfléchir à ce qu'on fait, mais il faut être performant quand on est dans l'action. Bien.\\n\\nAlors, l'idée, c'est ça. Du coup, on a les données et on veut créer à la fin, ici, une web ap\"), ('250010', \"données et on veut créer à la fin, ici, une web app. Une application en stand-alone, si vous voulez, qui permet de rentrer les caractéristiques à la main, de cliquer sur un bouton quelconque et d'avoir les clusters prédits par le modèle. C'est ça, l'idée. Donc, cette partie-là, je vais passer très vite là-dessus, mais ces trois éléments-là vont toujours de pair, généralement, dans une étude.\\n\\nBien, alors, tout ça étant dit, je parle très rapidement des données. Alors, les données sont là, il y a\"), ('250011', \"nt des données. Alors, les données sont là, il y a deux éléments. Là, c'est le notebook que j'ai préparé en amont, et les données, c'est les iris, tout simplement, mais sans la variable cible, donc uniquement avec les quatre descripteurs, c'est pas le length, c'est pas le wif, pétale link, pétale wif et j'ai enlevé la classe D c'est de les regrouper donc on va faire 3 groupes il n'y a pas de mystère là dessus donc ici le véritable enjeu c'est la création de la web c'est une forme de corrigé je m\"), ('250012', \"création de la web c'est une forme de corrigé je m'adresse au mot aux étudiants du M1 que j'ai eu en cours cette année c'est une forme de corrigé, ça vous donnera une idée de ce que j'attendais de vous dans la session qu'on avait fait jeudi très bien, je referme ça et du coup je peux lancer alors bon comme d'habitude ça ne change pas donc j'importe les données donc là j'ai mon notebook donc je commence par importer les données donc je mets le répertoire de travail et j'importe le fichier, j'affi\"), ('250013', \"ertoire de travail et j'importe le fichier, j'affiche la description des données donc il n'y a pas de données manquantes dans les iris et élément très important quand même on a les valeurs minimales par variable ici et les valeurs maximales par variable ici. Voilà, on a les plages de valeurs, du coup. Très bien, ça donne une idée. Et on a les valeurs moyennes. Mais bon, en l'occurrence, ici, ce qui m'intéresse, c'est les valeurs minimax parce que je vais les délimiter. Je vais les utiliser pour \"), ('250014', \" je vais les délimiter. Je vais les utiliser pour délimiter les saisies dans l'application finale. Très bien, donc voilà, on a les données. Et pour l'analyse, on va dire que tout est simple. Du coup, je vais créer directement le pipeline. Alors, qu'est-ce qu'il y a? Le pipeline, très rapidement, Un pipeline, c'est quoi? Un pipeline, c'est une structure Python de SeekitLearn qui permet d'empiler des séries d'actions. Alors, on peut mettre en place des actions conditionnelles, j'en ai parlé dans l\"), ('250015', \" des actions conditionnelles, j'en ai parlé dans les colommes Transformers et tout ça, mais on peut, au plus simple, faire simplement des séquences d'opérations. Voilà ce que je fais ici. Alors, dans mes séquences d'opérations que je fais, dans mon pipeline, j'en ai mis trois opérations. Un, c'est un outil d'imputation des données manquantes. Il faut prévoir ça, déjà pour l'apprentissage, mais aussi en prédiction. L'imputation des données manquantes nous permet de lancer les algorithmes de machi\"), ('250016', \"tes nous permet de lancer les algorithmes de machine learning, quand on le met en place, très bien, ça c'est bien également, mais il ne faut pas oublier que cette amputation de données manquantes doit être également mise en œuvre quand on déploie le modèle sur un individu supplémentaire. Donc, quelle stratégie on va utiliser là-dedans? Ça, c'est un élément important. Alors, on va utiliser le simple imputaire de Sikitler. Je n'ai pas d'action sur Sikitler, mais ça reste un fabuleux outil quand mê\"), ('250017', \"Sikitler, mais ça reste un fabuleux outil quand même, que j'apprécie à chaque fois que j'utilise. Donc, le simple imputaire, qu'est-ce qui se passe? Tout simplement, il impute par défaut la moyenne. Alors, on peut dire, ouais, c'est super frustre ça, la moyenne, c'est pas bon et tout ça. Mais bon, n'oublions pas que la moyenne est le meilleur prédicteur endogène. est le meilleur prédicteur endogène d'une variable, au sens des moindres carrés, le normel 2. Donc, déjà, ça, c'est une piste possible\"), ('250018', \"normel 2. Donc, déjà, ça, c'est une piste possible. Et on peut aller sur des outils sophistiqués, mais ceux qui sont basés sur des recrétions corseuses, par exemple, si des variables sont deux à deux orthogonales, on revient à la moyenne, on est d'accord là-dessus. Donc, la moyenne, c'est pas... Bien sûr, on peut faire différemment, mais une des solutions possibles, c'est la moyenne. Et il y a un sens à utiliser la moyenne. c'est le meilleur prédicteur endogène d'une variable au sens des moindre\"), ('250019', \"icteur endogène d'une variable au sens des moindres carrés. N'oublions jamais ça. Voilà, donc je prends la moyenne ici parce que ce qui m'intéresse, c'est le déploiement, on est d'accord. Donc c'est pour ça que je prends la moyenne ici, voilà, et l'imputation se fait en amont de l'algorithme de machine learning. Ça également, ça peut être discutable, ça. C'est-à-dire que, quel que soit l'algorithme que je mets derrière, soit je fais une CH, K-means, un DB scan, un Birch, ou tout ce que vous voul\"), ('250020', \"ns, un DB scan, un Birch, ou tout ce que vous voulez, c'est toujours l'imputation que je fais en un moment qui va marcher, quelles que soient les différentes méthodes derrière. Ça aussi, c'est extrêmement discutable. Mais bon, on va au plus simple ici. C'est la création de la web app qui m'intéresse. Bien, alors, pour signifier qu'il y a une donnée manquante, on utilise le nom de NumPy. Donc, not a number. Ce n'est pas vraiment une donnée manquante, en fait. Mais bon, par défaut, c'est ce qu'il \"), ('250021', \"te, en fait. Mais bon, par défaut, c'est ce qu'il utilise. Vous avez vu, par défaut, c'est ce qu'il utilise. L'outil simple imputer. Donc, nous, pour signifier qu'il y a une donnée manquante, il faut qu'on lui mette un NOTA number de numpy. Ça, il ne faudra pas oublier ça. Bien, voilà, ça c'est le premier outil. Ensuite, le deuxième outil, qu'est-ce qu'il y a? On fait un standard scalaire. C'est un centrage-réduction. Ce n'est pas obligatoire, on est d'accord là-dessus. Mais bon, ici, c'est pour\"), ('250022', \" est d'accord là-dessus. Mais bon, ici, c'est pour empiler des opérations. Donc, j'ai mis un standard scalaire qui centre et réduit. Après imputation, il y a des données manquantes. Alors, élément très important, si je reviens sur l'imputation de seconde, ici, s'il y a une donnée manquante sur l'individu supplémentaire, c'est la moyenne de l'échantillon d'entraînement qui est utilisée. On est d'accord, potentiellement dans l'échantillon test, entre guillemets, il peut y avoir qu'un individu. Voi\"), ('250023', \"re guillemets, il peut y avoir qu'un individu. Voilà, donc l'imputation par la moyenne n'a aucun sens, on est d'accord là-dessus. Donc c'est bien la moyenne calculée sur l'échantillon d'entraînement qui nous intéresse. alors après le standard scalaire qu'est-ce qui se fasse, je mets un camins tout simplement je mets un camins avec trois groupes, très bien voilà avec les paramètres  des foot-pulls que je mets là pour que, voilà, tout simplement, pour que ce soit reproductible. Une fois que j'ai f\"), ('250024', \"our que ce soit reproductible. Une fois que j'ai fait ça, donc, j'entraîne mon pipeline sur mes données et on a bien les différentes étapes qui sont là. Simple imputer, standard scalaire et camins. Voilà. Avec la structure. Voilà. Très bien. Alors, pour essayer de voir qu'est-ce qui se passe, le premier élément, c'est l'inspection des groupes. Donc, je fais un prédict. Après avoir fait le fit, l'entraînement du modèle, je fais le prédicte sur les mêmes données. Pour voir comment se fait l'attrib\"), ('250025', \" mêmes données. Pour voir comment se fait l'attribution des groupes aux individus. C'est ce que je fais là. Et j'avais demandé trois groupes. Donc, oui, j'avais demandé trois groupes ici, on est d'accord. Comme j'ai demandé trois groupes, forcément, j'ai trois groupes qui sont labellisés 0, 1 et 2. Et dans le premier groupe, les groupes 0, j'ai 50 individus. Dans le deuxième, dans le groupe 1, j'ai 56. Et dans le groupe 2, numéro 2, j'ai 44 individus. Très bien. Alors, autre élément important ég\"), ('250026', \"idus. Très bien. Alors, autre élément important également, il a travaillé sur les données centrées réduites que je peux visualiser. Donc, c'est ce que je fais ici. Voilà, donc j'affiche, je récupère l'étape qui correspond au centrage réduction, je fais un transforme sur les données là-dessus, et j'affiche les données, et je vois bien les données centrées réduites qu'il a utilisées pour lancer les camines. Très bien. Alors, à ces données centrées réduites, je vais rajouter le compte d'appartenanc\"), ('250027', \"réduites, je vais rajouter le compte d'appartenance qui a été obtenu ici, avec le prédicte. Vous avez vu, avec le predict, j'ai obtenu le groupe d'appartenance. Donc, à ces données centré réduites que j'ai créé, le data frame que j'ai créé là, je rajoute le groupe d'appartenance, et ça me permet de calculer, du coup, vous avez vu, les données sont là, centré réduite avec les groupes d'appartenance, ça me permet de calculer les moyennes conditionnelles. Alors, calculer les moyennes conditionnelle\"), ('250028', \"elles. Alors, calculer les moyennes conditionnelles sur les variables centré réduites, donc les moyennes conditionnellement aux clusters d'appartenance, ça permet de voir le rôle des variables dans la caractérisation de chaque cluster. C'est ça l'idée. Alors, intéressant ici, comme les données sont en santé réduite, on peut comparer les résultats d'une variable à l'autre. On peut voir les échelles de valeur, c'est ça qui est important ici. Sinon, non, on voit bien que les pages de valeur ne sont\"), ('250029', \" non, on voit bien que les pages de valeur ne sont pas les mêmes. On avait vu ça ici, là ça part de 4.3 à 7.9, là c'est 2.0 à 4.4, on n'a pas les mêmes échelles de valeur, on n'a pas les mêmes plages. Donc, comparer des moyennes conditionnelles sur des variables originelles, ça n'a aucun sens. On peut utiliser les valeurs test, j'en ai suffisamment parlé là-dessus. Mais une solution simple également, c'est de comparer tout simplement avec les moyennes conditionnelles sur des variables centraire \"), ('250030', \"ennes conditionnelles sur des variables centraire réduites. Comme on a enlevé les unités, voilà les valeurs qui sont là, et c'est des écarts en écart type. Donc là, par exemple, c'est pas l'inf pour le groupe 0. donc en moyenne pour CEPAL LYNF la moyenne, en moyenne on est à un écart-type en dessous de la moyenne pour le groupe 0 pour CEPAL LYNF et pour la même variable CEPAL LYNF, le groupe 2 en revanche, en moyenne on est à 1,16 fois au-dessus de l'écart-type non c'est 1,16 fois l'écart-type a\"), ('250031', \"de l'écart-type non c'est 1,16 fois l'écart-type au-dessus de la moyenne et là c'est moins un écart-type en dessous de la moyenne Ça, c'est 1.16 fois l'écart type au-dessus de la moyenne. C'est ça l'idée. Très bien. Donc, les valeurs positives et négatives, ça permet de savoir si on est en dessous ou au-dessus de la moyenne. En dessous ou au-dessus de la moyenne. Moi, je m'embrouille. Très bien. Et les valeurs-là sont comparables parce que c'est des écarts en écart type. Ça permet du coup de car\"), ('250032', \"des écarts en écart type. Ça permet du coup de caractériser les groupes. Alors, pour qu'on soit visible, ça, on l'a fait rapidement, mais après, je n'ai pas creusé. on l'a fait sous R. Là, je parle à mes étudiants, mais on peut le faire très bien sous Python également. Dans d'autres circonstances, je le fais sous Python, justement. Très bien, qu'est-ce qui se passe? Je fais un radar plot. Ça permet de caractériser les clusters. C'est ce que je fais ici. Donc là, je mets le radar plot, avec les m\"), ('250033', \"is ici. Donc là, je mets le radar plot, avec les moyennes conditionnelles qui sont calculées ici. Et du coup, je peux afficher les résultats qui me donnent un élément d'interprétation. Qu'est-ce qui se passe? Le cluster 0, par exemple, a des valeurs élevées. Donc, la valeur 0, là, sur l'abscisse, ça veut dire qu'on est à la moyenne, puisque les variables sont centrées. Voilà, plus 1, ça veut dire qu'on est une fois l'écart-type au-dessus de la moyenne, et moins 1, c'est moins une fois l'écart-ty\"), ('250034', \"yenne, et moins 1, c'est moins une fois l'écart-type en dessous de la moyenne. Très bien. Donc, on voit, par exemple, que le groupe 0, le cluster 0 qui est là, il a des valeurs élevées de sépale-luif mais il a des valeurs très basses sur pétale-lincph, pétale-luif et sépale-lincph voilà si on regarde le groupe 2 le vert là, il a des valeurs élevées sur pétale-lincph pétale-luif et sépale-lincph en revanche il a des valeurs proche de la moyenne pour sépal-luif parce que la moyenne elle est là, c'\"), ('250035', \"ur sépal-luif parce que la moyenne elle est là, c'est 0 et on voit le cercle, il est bien là on voit bien donc le groupe 2 c'est les fleurs qui a des grandes valeurs de sépale length, voilà, pétale length et pétale length et valeur moyenne égale à la moyenne globale de sépale length en revanche le groupe 0 a des valeurs élevées de sépale length, mais des petites valeurs on est proche de moins 1 là pour les autres variables donc elle est très caractéristique elle est très caractérisée par sépale \"), ('250036', \"ctéristique elle est très caractérisée par sépale length qui a des grandes valeurs cette fois-ci et en revanche si on prend le groupe 1 là, il a des valeurs moyenne quasiment sur toutes les caractéristiques, sauf sur Cépadluif où il est plus petit. C'est le groupe rouge, le groupe 1. Il a des valeurs moyennes partout, sauf sur Cépadluif où il a des valeurs plus petites. Il est plus petit en Cépadluif. Grosse fleur, moyenne fleur et petite fleur, sauf exagérément grande sur une des variables qui \"), ('250037', \"sauf exagérément grande sur une des variables qui est Cépadluif. C'est ça l'idée. On a la caractérisation ici. Donc, ça, c'est important, ça. Alors, bon, là, en calculant les moyennes conditionnelles brutes, même sans très réduite, pour le coup, on n'a pas des idées sur les effectifs. Donc, parfois, peut-être que c'est mieux de passer par les valeurs test. D'accord, là-dessus. Mais bon, déjà, ça donne une idée des caractéristiques ici. Alors, qu'est-ce qui se passe? Ce graphique-là, je vais le s\"), ('250038', \"est-ce qui se passe? Ce graphique-là, je vais le sauvegarder. Pourquoi? Parce que je vais l'intégrer dans mon application pour que l'utilisateur, quand il va rentrer les valeurs, ait une visibilité de ce qu'il obtient. donc il fallait installer Kaleido, j'ai installé moi Kaleido, je l'ai fait à la volée sauvagement ici, et du coup l'image je l'ai sauvegardée donc si je vais sur mon disque dans Déploy, dans ce sous-répertoire là, vous avez vu je l'ai mis dans Déploy Radar je l'ai sauvegardé et il\"), ('250039', \"'ai mis dans Déploy Radar je l'ai sauvegardé et il est là mon graphique que je vais intégrer dans mon application tout à l'heure donc je reviens, sinon il y avait deux frichies ici, je continue qu'est-ce qu'il me reste à faire alors? Ce qu'il me reste à faire maintenant, c'est d'exporter le pipeline pour le déploiement. On peut utiliser Pickle, mais en vrai, je préfère qu'on utilise Deal. Deal est nettement plus souple. Il permet de mettre plus d'éléments. Les mêmes choses que Pickle, il le gère\"), ('250040', \"'éléments. Les mêmes choses que Pickle, il le gère, mais il peut gérer plus de choses que Pickle lui-même, que le package Pickle. Je préfère qu'on utilise Deal. Et donc, qu'est-ce qui se passe? Toujours dans le dossier Deploy, je crée le workflow.sav en écriture binaire, et je dump mon pipeline là-dedans. Boum, ça y est. Ça y est, c'est fait. Et ce qu'il me reste à faire du coup, ce qu'il me reste à faire, c'est cette partie-là. On va créer l'application de déploiement. Chaque individu, on va re\"), ('250041', \"lication de déploiement. Chaque individu, on va rentrer ses caractéristiques, on clique sur un bouton et on a le cluster attribué avec les distances aux différents clusters, aux trois clusters, et une explication du type de classement. Et là, je vais utiliser le graphique. Alors, si je reviens sur mon disque, là, regardez. Voilà. Donc, on a le radar que j'ai exporté, qui permet d'interpréter les résultats. Et on a ici, alors justement parce que j'avais demandé ça aux étudiants, alors j'imagine q\"), ('250042', \"'avais demandé ça aux étudiants, alors j'imagine que, je ne sais pas ce qu'ils ont fait encore, je ne vais pas tout corriger encore, mais on aurait aussi pu mettre ces caractéristiques-là dans la simple fichée Zizon. On met des caractéristiques dans la fichée Zizon qu'on importe directement, et on peut avoir ces descriptions-là directement derrière. Ça aussi, c'est une possibilité, ça. On est d'accord là-dessus. Ça, on n'a jamais fait. On n'a jamais fait, j'en ai même pas parlé. Mais en informat\"), ('250043', \"ais fait, j'en ai même pas parlé. Mais en informatique, on doit savoir faire ça. On doit savoir faire ça, et je sais qu'ils savent le faire. Le truc, c'est qu'ils n'y pensent pas, généralement. Ça, c'est vraiment un vrai problème chez les étudiants, c'est qu'ils cloisonnent leurs connaissances. Et ils pensent que les connaissances qu'on a dans un domaine, on ne peut pas l'appliquer ailleurs, parce que non, non, c'est une autre matière, en fait. Mais si, si, justement. Et la vraie force, notre vr\"), ('250044', \"ais si, si, justement. Et la vraie force, notre vraie force, c'est qu'on a des compétences multiples et nous sommes capables de les marier pour nous rendre plus efficaces. C'est ça l'intérêt. On a une série de compétences, il n'y a pas de tunnel. On doit pouvoir justement les réunir pour être plus performants sur une étude en mariant différentes compétences, en combinant différentes compétences. C'est ça l'intérêt. Et c'est ce que je veux. C'est pour ça que j'en parle tout le temps. Alors, le de\"), ('250045', \"pour ça que j'en parle tout le temps. Alors, le deuxième point, bien sûr, j'ai exporté ici le fichier de pipeline. Qu'est-ce qu'il me reste à voir du coup? Mon application Streamlit. Comment j'ai développé mon application Streamlit? Alors, je l'ai fait en amont parce que je ne l'ai pas codé devant vous. Je ne vais pas commencer à écrire du code, c'est la meilleure manière de faire des erreurs. Alors, mon application, elle est là. Donc, j'importe le librairie, comme Streamlit, qui est installé da\"), ('250046', \"le librairie, comme Streamlit, qui est installé dans l'environnement base d'Anaconda. il y a Streamlit présent par défaut donc il n'y a rien à installer en plus on est d'accord là dessus bon moi je l'ai mis à jour quand même j'ai installé la dernière version  On est le 9 avril, là. Voilà. Donc, j'ai récupéré la dernière version d'Anaconda et j'ai fait une mise à jour de Streamlit. Voilà. Avec Conda, Conda UGDA, tout simplement. Voilà. Ensuite, Panda, pour la gestion des données, on verra après. \"), ('250047', \"nda, pour la gestion des données, on verra après. Deal pour chargement des paquets, du fichier PICL, du pipeline. Voilà. Et NumPy pour avoir le note-on-number. On est d'accord là-dessus. Alors, j'ai mis un petit titre à mon application. Voilà. Et ensuite, première chose que je dois faire, je dois charger le modèle. C'est ce que j'ai mis dans le chargement modèle. Je l'ai mis en cache, pourquoi? Parce qu'il y a une particularité du Streamlit, c'est qu'à chaque fois qu'on a une action, il relance \"), ('250048', \"t qu'à chaque fois qu'on a une action, il relance tout le code. Du coup, au bout d'un moment, ça peut être super lourd. Donc, on a l'intérêt à mettre tout simplement des caches. Il y a d'autres manières de faire. J'ai regardé avec le mécanisme des sessions, notamment, mais bon, ici, je vais vraiment au plus simple. Tout simplement, j'ai mis en cache le chargement des modèles. C'est ce que je fais ici. Très bien. qu'est-ce qu'il charge? Il charge dans le répertoire courant, ils sont dans le même \"), ('250049', \"dans le répertoire courant, ils sont dans le même dossier, vous avez le PNG, vous avez mon programme Streamlit, et vous avez le pipeline qui a été sauvegardé avec Deal, il va charger dans le répertoire courant, le workflow.sav. C'est ce qui se passe ici, je mets un trail finalit, simplement pour être sûr que le fichier est bien fermé, sinon il va être verrouillé. Donc là, je charge le modèle. Ensuite, mon espace de représentation, je l'ai fait en trois colonnes. Deux colonnes pour pouvoir entrer\"), ('250050', \" trois colonnes. Deux colonnes pour pouvoir entrer les voleurs, une colonne de vide pour mieux organiser, et une colonne de cinq éléments, de cinq colonnes. Voilà, une troisième colonne, un peu plus large, justement, pour pouvoir voir les résultats et voir le graphique. Ensuite, dans la colonne 1, du coup, colonne 1, 2, 3, dans la colonne 1, je fais les saisies des différentes valeurs. Et je mets les valeurs minimax, pour que l'utilisateur ait une visibilité sur ce qui peut rentrer ou pas. C'est\"), ('250051', \"e visibilité sur ce qui peut rentrer ou pas. C'est important, ça, hein? Vous ne pouvez pas rentrer comme ça, une zone d'édition, et vous ne dites rien, et le gars, il ne sait pas. Alors, il y en a qui mettent des slide bars. Ça, c'est une possibilité. Mais bon, je ne l'ai pas demandé ça. Encore une fois, cette application-là, vous aurez pu le faire avec une application console. Et faites tout simplement des inputs pour les saisies. C'est tout à fait possible. C'est tout à fait possible. Voilà. M\"), ('250052', \"ait possible. C'est tout à fait possible. Voilà. Moi, je fais Streamlit ici pour montrer que très facilement, on peut créer une web app, mais vous aurez pu, là, je m'adresse à mes étudiants, pour le coup, vous aurez pu faire une application console avec des inputs. C'est ce qu'ont fait certains d'entre vous. Je regardais vite fait parce que, comme j'ai rodé derrière vous, je regardais ce que faisaient les uns et les autres, et j'ai vu que certains ont utilisé cette solution-là, qui est très bien\"), ('250053', \"s ont utilisé cette solution-là, qui est très bien. Donc, je suis parti sur Streamlit, je mets les valeurs limites pour que l'utilisateur ait une bonne visibilité, et je mets des valeurs par défaut. Très bien. Ensuite, il y a le bouton calculer, mais encore une fois, dans Streamlit, vous faites valider sur une zone d'édition, bon, ça relance la page. Alors, je fais une fonction qui récupère les valeurs. Si les valeurs sont valides, il les met en flottant. Si les valeurs ne sont pas valides, ça g\"), ('250054', \"flottant. Si les valeurs ne sont pas valides, ça génère une exception, et la valeur envoyée, ça va être de note un number. En déploiement, on est d'accord une fois, et via le simple pluter, il va prendre la moyenne calculée sur l'échantillon d'entraînement. C'est ça. Le numpy none, il vient du fait que, dans le simple pluter, c'est ce qu'il utilise pour définir la valeur manquante. On est en déploiement, on est bien d'accord là-dessus. donc s'il veut déployer et que sur une des valeurs il y a un\"), ('250055', \"veut déployer et que sur une des valeurs il y a une valeur manquante, sur une des variables il y a une valeur manquante, il va remplacer par la moyenne calculée sur l'échantillon d'apprentissage voilà ce qu'il va faire et ensuite il fait le processus normal, il va centrer et réduire avec les moyennes et les cartes calculées sur l'échantillon d'entraînement et ensuite il va trivaux clamis en calculant les distances au baril centre des classes voilà donc là je récupère les valeurs tout simplement,\"), ('250056', \"à donc là je récupère les valeurs tout simplement, et je le mets dans un dictionnaire que je transforme après en data frame pendant. Qu'est-ce qu'il me reste à faire? Dans la troisième colonne, je fais un petit message et je charge le graphique radar qui permet de comprendre l'affectation aux classes. Ça, je l'avais dit, c'est très bien d'affecter aux classes, mais il faut dire pourquoi. Il faut que l'utilisateur ait une visibilité sur ce qu'il fait. Le graphique radar le fait très bien, il n'y \"), ('250057', \"ait. Le graphique radar le fait très bien, il n'y a que 4 variables ici. Je vois très bien ce qui va se passer. Ensuite, j'appelle la méthode predict du workflow qui a été chargé, qui a été chargé ici, le modèle qui a été chargé, la pipeline si vous voulez, et je calcule également la distance au baril centre. C'est ce qui se passe ici. On a une idée de la force de l'attribution au cluster tout simplement. Ceci étant fait, je vais pouvoir lancer. Je suis là, j'avais fait un dire au départ, il n'y\"), ('250058', \"Je suis là, j'avais fait un dire au départ, il n'y avait rien. Si je refais un dire maintenant, dans Deploy, il y a mon petit programme qui est là. Et on a le graphique radar qui va charger automatiquement. Et il y a le workflow, le pipeline qui a été mis dans un fichier avec deal. Allons-y alors, je lance mon programme. Stream lit run st-deploy.py. Voilà, zou, boum. une fois que c'est fait ça, on a ma web app qui est là. Et voilà. Alors on va réduire tout ça pour que ça n'embête pas la vue. La \"), ('250059', \"duire tout ça pour que ça n'embête pas la vue. La zone de gauche, la première colonne justement, c'est là où on peut retraire des valeurs, et on a des indications sur les valeurs limites, pour qu'on ne fasse pas n'importe quoi. Ici, avec le graphique radar, on a l'explication des attributions. Alors pour les valeurs par défaut que j'ai mis là, qui sont proches des valeurs moyennes, le cluster qui est attribué, c'est le cluster 1. Et le cluster 1, c'est le rouge là. Et vous voyez bien, c'est le c\"), ('250060', \" c'est le rouge là. Et vous voyez bien, c'est le cluster où on a des valeurs proches de la moyenne pour pétal-lynkph, pétal-luif et sépal-lynkph, et des valeurs basses pour sépal-luif. Vous êtes d'accord là-dessus. Et là, on a la distance, et là, ça s'est tranché. Regardons alors. Regardez. En fait, qu'est-ce qui se passe pour sépal-luif? C'est pas l'UIF, il a des valeurs basses pour toutes les variables, sauf pour C'est pas... Non, c'est pas ça que je voulais dire. Le cluster 0, qu'est-ce qui s\"), ('250061', \"que je voulais dire. Le cluster 0, qu'est-ce qui se passe pour le cluster 0? Pour le cluster 0, il a des valeurs élevées pour C'est pas l'UIF, largement au-dessus de la moyenne, et des valeurs faibles pour les autres variables. Donc je vais changer la valeur de C'est pas l'UIF, et je vais me rapprocher du maximum. On va voir s'il va l'attribuer au cluster 0, parce que c'est ce que me dit ce graphique-là. Allons-y, alors. Je mets 4.4 et je fais calculer. Et regardez le cluster attribué là. Il a m\"), ('250062', \"lculer. Et regardez le cluster attribué là. Il a mis le 0 ici. Et la distance au cluster, effectivement, la distance au bar et centre, voilà, la distance au cluster 0, c'est 2.8, au cluster 1, c'est 4.14, et le cluster 2, c'est 3.9. Et il l'attribue au cluster 0 parce que c'est la distance la plus faible. On est d'accord là-dessus. Donc, cet individu-là, avec ces descriptions-là, il l'attribue au cluster 0 parce qu'il a une valeur élevée de CEPA de l'UIF. C'est ça le truc. Et voilà, c'est tout l\"), ('250063', \"de l'UIF. C'est ça le truc. Et voilà, c'est tout l'intérêt de cet outil d'interprétation-là. C'est que vous montrez les valeurs, vous avez un mécanisme d'attribution, il faut comprendre pourquoi il l'attribue à tel ou tel cluster. Et ce graphique radar-là nous donne cette indication-là de manière très simplifiée. Je prends un autre exemple. Admettons que je vais voir qu'est-ce que je peux faire pour que l'individu soit attribué au cluster 2. Là, je tourne le problème à l'envers, ce n'est pas ça \"), ('250064', \"je tourne le problème à l'envers, ce n'est pas ça l'idée. Généralement, l'idée, c'est qu'on met les valeurs, et ensuite on interprète, on est d'accord. Mais regardez, le cluster 2, il a des grandes valeurs de pétal-ling, pétal-luif et sépal-ling. Et une valeur moyenne, à peu près, de sépal-luif. Donc sépal-luif, je mets proche de la moyenne, c'était 3 à peu près. Et pour les autres variables, je vais mettre des grandes valeurs. Donc sépal-ling, c'est ça. Je vais mettre 7, par exemple. Ensuite, p\"), ('250065', \"'est ça. Je vais mettre 7, par exemple. Ensuite, pétale luif, c'est valeur élevée, donc je mets 2.4 par exemple, proche du maximum. Et pétale length, je vais mettre 6.5. Si je clique sur calculer, là, en fait il l'a déjà fait parce que j'ai fait entrer. Voilà. Il l'a attribué au cluster 2. Pourquoi? Parce qu'on a des valeurs élevées de pétale length, là. Ensuite, on a des valeurs élevées de pétale luif, là, proche du maximum. et on a des valeurs élevées de CEPA length là également et quand on ca\"), ('250066', \"élevées de CEPA length là également et quand on calcule les distances de cluster là c'est très tranché effectivement parce que la distance de cluster numéro 2 est la plus faible et de loin c'est ce que je voulais c'est ce que je voulais donc je voulais que vous créiez une application qui permet de rentrer ces valeurs là qui permet d'obtenir le cluster attribué et les distances de cluster et je voulais également que vous montriez comment comprendre l'attribution au cluster. Quel est le mécanisme \"), ('250067', \"e l'attribution au cluster. Quel est le mécanisme derrière l'attribution au cluster? Quelles sont les variables qui ont joué dans cette attribution au cluster? Il y avait différentes manières de faire. Soit textuellement, vous mettez les caractéristiques d'un fichier que vous affichez automatiquement en fonction du cluster attribué. Et l'autre solution était de faire des graphiques radar, ou d'autres graphiques par ailleurs. Ça aurait pu être également un graphique factoriel où vous interprétez \"), ('250068', \"lement un graphique factoriel où vous interprétez les facteurs. on est des éléments d'interprétation des facteurs ça ça aurait été pas mal aussi parce que là on a plus d'informations pour le coup et bien voilà voilà je vous souhaite à tous un excellent travail.\"), ('260001', \"### Correction et Amélioration du Texte\\n\\nBien sûr, c'est parti. Dans cette vidéo, je vais parler de l'outil fonction transformateur de CKIT Learn. Vous avez la documentation qui est là. L'idée est d'appliquer des fonctions de transformation à des variables en intégrant ces transformations dans une structure pipeline. Les pipelines, j'en parle beaucoup en ce moment. On se demande pourquoi, mais j'en parle beaucoup. Les pipelines, voilà, et on avait vu justement qu'on pouvait définir des séquences\"), ('260002', \"t vu justement qu'on pouvait définir des séquences de traitement dans une structure particulière qui est proposée par Sikhitler. C'est la structure pipeline. Alors généralement, on empile des objets Sikhitler nous-mêmes, mais on peut aussi empiler nos propres objets, pourvu qu'on respecte le formalisme en créant les bonnes classes héritières. Ça, j'en avais parlé. C'est un premier point important.\\n\\nL'autre point important également, on avait vu dans cette vidéo, Column Transformer, qu'on pouvait\"), ('260003', \"ans cette vidéo, Column Transformer, qu'on pouvait choisir les variables, quand on a un ensemble de données, et qu'on ne veut traiter que certaines variables, ou d'autres variables, ainsi de suite, on pouvait choisir les variables que l'on veut traiter, avec Column Transformer. Jusque-là, pas de souci. Mais je reviens un peu en amont, dans les pipelines, normalement, on n'empile que des objets qui sont héritiers d'une classe particulière, qui est définie pour ça. Qu'est-ce qui se passe si on veu\"), ('260004', \" définie pour ça. Qu'est-ce qui se passe si on veut créer et utiliser nos propres fonctions, très simplement, sans avoir à définir des classes, sans avoir à programmer des classes ? Alors, ces fonctions de transformation-là, on peut les utiliser, effectivement, pourvu qu'on utilise le bon outil, ces fonctions de transformation. Donc, l'idée, c'est bien, dans un pipeline où il y a de la préparation de données plus des algos de machine learning, mais là, pour le coup, je suis vraiment sur la prépa\"), ('260005', \"is là, pour le coup, je suis vraiment sur la préparation de données. Dans un pipeline où il y a de la transformation de données, on peut utiliser des classes, qui sont héritières d'un certain nombre de classes, des classes prédéfinies par CKIT Learn, mais on peut également utiliser des fonctions simples. Et on peut même les appeler à la volée, on peut même les appeler définir à la volée, avec des fonctions lambda. On peut définir des fonctions, mais on peut aussi définir des fonctions à la volée\"), ('260006', \"ais on peut aussi définir des fonctions à la volée avec des fonctions lambda. Alors, pour bien montrer ça, je vais prendre un petit schéma ici. C'est juste illustratif. Ce que je veux, c'est voir avec vous la fonction transformeur. Mais bon, c'est toujours mieux si on met une histoire autour. Les traitements de but en blanc, sans voir la finalité, ça paraît toujours un peu, on se dit, qu'est-ce qu'il est en train de manipuler ? Qu'est-ce qui se passe ? Je vais faire une réaction logistique. J'ai\"), ('260007', \"asse ? Je vais faire une réaction logistique. J'ai ma variable cible qui est là, la classe, positif ou négatif. Et j'ai deux variables explicatives ou deux descripteurs, deux variables quantitatives donc ces deux variables-là je vais les traiter de manière différenciée. Le X1, je vais appliquer une fonction de transformation, une certaine fonction de transformation, et le X2, je vais appliquer une autre fonction. Donc, pour les scinders, j'utilise justement le Column Transformer qu'on avait vu d\"), ('260008', \"e justement le Column Transformer qu'on avait vu déjà dans cette vidéo-ci. Dans cette vidéo-ci, j'ai parlé ici de Column Transformer, donc je vais utiliser cet outil-là pour les scinders en 2, et pour le X1, en fait, je vais le passer au carré. Ensuite, pour le V2, je vais le discrétiser en prenant le seuil 0,5. Donc, on a deux fonctions de transformation différentes, on est bien d'accord là-dessus. Et l'intérêt de la faire, c'est que ces fonctions-là, voilà, on va pouvoir les définir très simpl\"), ('260009', \"ns-là, voilà, on va pouvoir les définir très simplement. En utilisant la fonctionnalité fonction de transformeur. Ensuite, qu'est-ce qui se passe ? Une fois que j'ai créé les nouvelles variables V1, V2, le Column Transformer va les concaténer. C'est bien ce qui est expliqué ici. Voilà. Par la suite, elles sont concaténées. Voilà. Une fois qu'elles sont concaténées, là, je peux appliquer la régulation logistique dans le nouvel espace de représentation. V1, V2. Et non pas sur l'espace initial. C'e\"), ('260010', \"tion. V1, V2. Et non pas sur l'espace initial. C'est l'idée. Tout ça étant inclus dans un pipeline. Donc, ce qu'on essaie de voir aujourd'hui, c'est comment je peux empiler facilement des fonctions de transformation. Voilà. En définissant soit des fonctions simples, avec un def, soit en les définissant à la volée, avec des fonctions lambda. Sans avoir à créer des classes. Sans avoir à créer des classes. C'est tout l'intérêt de l'affaire. Et ce qui est très intéressant également, c'est que du cou\"), ('260011', \"i est très intéressant également, c'est que du coup, le pipeline, je peux le sauvegarder. Alors, Piquel ne peut pas. Je vais en parler tout à l'heure pourquoi. Mais il y a un autre package qui le fait sans aucun problème, c'est le package Deal. Donc si on veut piqueliser des objets pipelines, je conseille d'utiliser Deal. Deal est nettement plus généraliste, il est capable de gérer plus de choses. Allons-y alors, une fois que tout ça est dit, on va pouvoir lancer l'affaire. J'ai préparé comme d'\"), ('260012', \"va pouvoir lancer l'affaire. J'ai préparé comme d'habitude mon petit notebook. Alors je montre le dossier, pour l'instant il n'y a que les données qui sont là, c'est des données très simples que j'ai générées, donc le train est là, il y a 150 observations, et le test est là, il y en a 5000. C'est les données générées, c'est juste pour montrer le fonctionnement de l'outil. Et bien sûr il y a mon notebook que j'ai préparé. Alors premier élément, bien sûr, comme d'habitude, je charge les données. J\"), ('260013', \"en sûr, comme d'habitude, je charge les données. Je charge les données d'apprentissage, donc dans ces classeurs là je prends l'onglet train là il y a 150 observations x1, x2 et classe donc on a bien cette partie là d'accord ensuite j'affiche les premières valeurs mais on avait vu dans Excel tout à l'heure donc il n'y a pas de difficulté là dessus voilà je prépare les structures pour mettre les descripteurs d'un côté et la variable cible de l'autre très bien, je fais la même chose pour les donnée\"), ('260014', \"e très bien, je fais la même chose pour les données test la deuxième feuille du classeur donc la feuille test ici je la charge, il y a 5000 observations encore une fois c'est un exercice de style parce que je vois mais pourquoi il n'y a pas 70% 30% monsieur, comme si c'était une religion mais non, non, non, non on est sur un exercice de style, c'est pour montrer le fonctionnement de l'outil, et notamment le fonction transformeur voilà, ensuite je prépare les structures alors dans un premier temp\"), ('260015', \" prépare les structures alors dans un premier temps je vais lancer la régulation logistique juste pour voir, voilà, et on va voir que l'espace initial n'est pas terrible terrible Alors, regardez, voilà. Donc là, je lance une régulation logistique standard, donc sans pénalité, donc ni reach ni lasso, vraiment la standard, et je mets random state 0 pour qu'on ait tous les mêmes résultats. Donc, j'affiche les coefficients, mais en soi, bon, ce n'est pas le plus important. Et maintenant, je mesure l\"), ('260016', \" pas le plus important. Et maintenant, je mesure les performances en test, sur l'échantillon test. Pas les performances en test, c'est 88% d'accuratie. OK. Alors maintenant, je vais créer le pipeline. Je vais créer ce pipeline-là. Donc le pipeline, qu'est-ce qu'il va faire ? Le X1, il va la transformer en le passant au carré. Et le X2, il va le discrétiser. Et la bande, c'est 0,5. Voilà. Ensuite, ces deux variables-là, il va les concaténer dans un nouveau data frame. Pour faire le traitement. Là\"), ('260017', \"n nouveau data frame. Pour faire le traitement. Là, la transformation, je l'applique uniquement sur le X. Mais possiblement, on peut l'appliquer aussi sur le Y. C'est ce que dit la documentation, là. C'est ce que dit la documentation, ici. Un optionnal Y. Voilà. Moi, je ne m'en tiens qu'au X, ici, qui est passé dans le transform. Mais on peut aussi l'appliquer sur le Y, du coup. On est d'accord là-dessus. Alors, je vais obtenir ces deux bases-là. Comment je fais ça ? Comment je fais ça, tout sim\"), ('260018', \" Comment je fais ça ? Comment je fais ça, tout simplement ? Je crée mon petit pipeline, et je crée d'abord le Column Transformer, qui va me permettre d'appliquer des traitements spécifiques pour X1 et X2. Alors, je l'ai fait de deux manières. Donc, Column Transformer, voilà. Dans le Column Transformer, ici, je fais un premier traitement pour le X1. Donc, c'est ce qui est passé ici. Le V1 est une fonction transformeur qui est appliqué sur le X1 vous avez vu, il y a trois paramètres ça c'est un id\"), ('260019', \"us avez vu, il y a trois paramètres ça c'est un identifiant, V1 c'est un identifiant, ensuite ici entre crochets c'est une liste, donc on peut avoir plusieurs variables, j'en ai mis qu'un seul mais on peut mettre une liste de variables ici tout à fait possible, si je veux appliquer le même traitement pour un ensemble de variables voilà, là pour le coup il n'y en a qu'un seul mais vous avez bien vu, c'est une liste on est d'accord, donc je peux mettre une liste de variables ici Il y en a qui en s\"), ('260020', \"ttre une liste de variables ici Il y en a qui en seuls. Donc, la nouvelle variable V1 sera issue du traitement de X1. Enfin, ce n'est pas une nouvelle variable, en fait. Donc, c'est l'identifiant du traitement. Et ce que fait la fonction, c'est que j'appelle fonction transformeur. Et la fonction de transformation, c'est une fonction lambda qui prend le X et qui le met au carré. C'est exactement ça. X1 est passé au carré. Vous avez bien vu, hein ? Donc, le formalisme est très simple. Ici, Column \"), ('260021', \" Donc, le formalisme est très simple. Ici, Column Transformer me permet d'appliquer sélectivement des traitements sur les colonnes. Très bien. Et sur la première colonne X1, qui est définie ici, mais je peux en avoir plusieurs qui ont les mêmes traitements, encore une fois. Très bien. Je définis une fonction de transformation, fonction transformeur. Je définis une fonction de transformation. Là, pour le coup, on le voit bien. Où l'idée, c'est de prendre le X et de le passer au carré. X1, en l'oc\"), ('260022', \"prendre le X et de le passer au carré. X1, en l'occurrence, ici. on peut donc définir une fonction de transformation à la volée sans avoir sans avoir à créer une classe c'est ça l'intérêt de la faire c'est vraiment ça l'intérêt de la faire c'est que du coup je peux appliquer des séries transformation sans avoir à créer des classes quand c'est des transformations simples s'il ya des paramètres en fait calculer et qu'il faut les sauvegarder là ça le fait pas là parce que vous pouvez pas les sauveg\"), ('260023', \"e fait pas là parce que vous pouvez pas les sauvegarder il n'y a pas d'outils là il faut Il faut utiliser une classe. Parce que quand il va sérialiser par la suite, ces paramètres calculés là, qui sont dans la classe, il faut les sauvegarder avec l'objet. Oui, d'accord. Mais là, je n'ai pas de paramètres à sauvegarder. on est d'accord, je prends le x1 et je le passe au carré. Donc il n'y a pas de paramètre à sauvegarder, c'est juste la fonction à sauvegarder. Donc je peux ici la définir à la vol\"), ('260024', \" sauvegarder. Donc je peux ici la définir à la volée. Ça c'est un premier élément. Ensuite pour le x2, qui est là, donc le x2 c'est ça. Le x2 c'est là, c'est ce chemin-là. Qu'est-ce qui se passe ? Je vais la comparer à un seuil 0,5 et je code 0 ou 1 en l'occurrence. Je vais la binariser si vous voulez. Alors j'aurais pu faire une fonction lambda. J'aurais pu faire une fonction lambda, exactement pareil. mais j'essaie de montrer qu'on peut faire différemment. Qu'est-ce qui se passe ? La fonction \"), ('260025', \"ifféremment. Qu'est-ce qui se passe ? La fonction de binarisation, je la définis en amont. C'est bien une fonction, ce n'est pas une classe. On est d'accord ? C'est bien une fonction. Donc c'est dev qui prend en entrée un paramètre x, qui représente donc une colonne des données. Ça peut aussi représenter une matrice, si on a un ensemble de variables ici. Si on a un ensemble de variables ici, si on a plusieurs variables, le x va représenter une matrice. Mais là, je n'ai qu'une seule variable. Don\"), ('260026', \"trice. Mais là, je n'ai qu'une seule variable. Donc le x, du coup, ne représente qu'un vecteur. On est d'accord là-dessus. Donc la deuxième transformation, là c'est l'identifiant, c'est une fonction de transformation qui prend en entrée ma binarisation. Et le prototype de ma binarisation, en fait, c'est une fonction qui prend en entrée un vecteur, x, et qui renvoie un autre vecteur. Un vecteur, et c'est cohérent avec le fait que je passe qu'une seule variable ici. S'il y en a plusieurs, c'est un\"), ('260027', \"eule variable ici. S'il y en a plusieurs, c'est une matrice qui est passée, encore une fois. Très bien. Alors là, la fonction, je prends un pi, je prends quand x est inférieur à 0,5, je code 0, sinon je code 1. C'est exactement ce qui est défini ici. Vous avez vu ? Bien. Donc, je lance ça, et ça y est, j'ai ma préparation de données. Mais qu'est-ce qu'il me reste à faire du coup ? Ce qu'il me reste à faire, c'est créer le pipeline complet avec d'une part la transformation, ça c'est l'étape 1 si \"), ('260028', \"une part la transformation, ça c'est l'étape 1 si vous voulez, ça c'est l'étape 1, et l'étape 2, c'est la régulation logistique. Voilà, donc je vais empiler ces deux étapes-là dans mon pipeline. C'est exactement ce qui se passe là. Donc je crée le pipeline, voilà. Et dans le pipeline, je mets la préparation de données. Donc prépa data, c'est le Column Transformer que j'ai défini ici. On est d'accord ? Très bien. Et la deuxième étape dans mon pipeline, c'est la régulation logistique avec exacteme\"), ('260029', \"line, c'est la régulation logistique avec exactement le même paramétrage qu'au-dessus. On est d'accord ? Voilà, c'est ce que j'ai fait. Une fois que j'ai fait du cours, je vais relancer le fit du pipeline. Et pour accéder à un des éléments particuliers, ça j'ai expliqué en détail dans cette vidéo. Dans paramétrage, je montre que quand on a une séquence de traitement, on peut accéder à chacune des séquences de manière individuelle et accéder soit à ses propriétés, soit même faire appel à ses méth\"), ('260030', \"à ses propriétés, soit même faire appel à ses méthodes. On est d'accord là-dessus. donc je fais un feed et pour la régulation logistique j'accède aux coefficients et voilà les coefficients bon les coefficients en soi ils ne sont pas le plus important ici très bien alors j'avais dit qu'on peut accéder à chacune des étapes or il y a deux étapes là j'accède à la régulation logistique la dernière étape mais je peux accéder à la première étape de transformation donc j'aimerais voir les données transf\"), ('260031', \"sformation donc j'aimerais voir les données transformées j'aimerais voir les données transformées donc c'est ce qui se passe ici donc je vais accéder ici à l'étape nommée préparation, et j'appelle la fonction transformer sur Xtrain. Et je dois avoir les variables transformées ici. Et on a bien les deux colonnes, avec là les valeurs passées au carré sur cette colonne-ci, on a les valeurs qui sont là, très bien, et la deuxième colonne, c'est les données binarisées. C'est bien les postpones 1, 1, 0\"), ('260032', \"nnées binarisées. C'est bien les postpones 1, 1, 0, 1. C'est 1, 0, 1. Ils ne s'appliquent que sur les X ici. Mais encore une fois, on pourrait intégrer le Y. C'est ce que dit la documentation. Encore une fois, c'est ce que dit la documentation. Je n'ai pas testé, moi. Mais bon, ça doit pouvoir se faire s'ils le disent. Alors, qu'est-ce qu'il reste à voir ? La performance en test. 98, 44. C'est des données artificielles. D'accord ? Donc, il ne faut pas trop s'énerver là-dessus. J'ai manipulé, si \"), ('260033', \"t pas trop s'énerver là-dessus. J'ai manipulé, si à moi, les données pour bien montrer que la transformation, ça peut être très efficace parfois. Mais ce n'est pas le propos. Le vrai propos, c'est l'utilisation de fonctions transformeurs. C'est ça le vrai propos ici. C'est que je peux appliquer des transformations de variables dans un pipeline, je peux empiler des transformations de variables, sans avoir à créer des classes pour le faire. On peut définir soit des fonctions à la volée, des foncti\"), ('260034', \" définir soit des fonctions à la volée, des fonctions lambda, soit définir des fonctions et l'insérer dans le pipeline, dans le Column Transformer qui est inséré dans le pipeline par la suite. C'est puissant, c'est très puissant. Dernier point alors, est-ce que je peux sauvegarder ? Alors là, j'ai lu la doc. Bien sûr que je lis la doc. Et il y a ça. Si on utilise une fonction lambda, le transformeur ne peut pas être piqué. Un piqulé. Enfin, notre piquable. Piqulable. C'est pas bon, ça. J'ai essa\"), ('260035', \" piquable. Piqulable. C'est pas bon, ça. J'ai essayé. Ça ne marche pas, effectivement. Si on utilise le package pickle, mais il y a notre package, en fait. J'ai beaucoup regardé cet aspect-là. Il y a notre package qui lui permet de le faire. Alors, qu'est-ce qui se passe ? J'en ai parlé plusieurs fois de Deal. Je l'ai utilisé là, je l'ai utilisé là. Encore une fois, on est d'accord là-dessus. Regardez, il dit, Deal va étendre les fonctionnalités de Pickle. Ah, très bien, ça. Pour la sérialisatio\"), ('260036', \"de Pickle. Ah, très bien, ça. Pour la sérialisation et la déstéréalisation des objets Python. Très bien. Et là où c'est intéressant, c'est que, voilà, il peut intégrer dans le processus les fonctions lambda. Ah, ben, c'est excellent, ça. que demande le peuple ? du coup je vais bien picler mon objet j'ai bien sérialisé si vous voulez mais en utilisant deal et le fait que j'ai mis une fonction lambda du coup ce n'est pas pénalisant on pouvait penser que ça l'était mais avec deal ça ne l'est pas al\"), ('260037', \"r que ça l'était mais avec deal ça ne l'est pas alors allons-y qu'est-ce qui se passe ? pour l'instant il n'y a rien il y a les deux fichiers le fichier de données et le notebook Donc, j'appelle deal, j'ouvre le fichier en écriture, binaire, je dump le pipeline dans le fichier et je close le fichier. C'est ce que je fais ici. Si j'affiche le contenu du dossier, on a bien le workflow.sav qui est là. Très bien, ça c'est nickel. Et pour bien être m'assuré qu'il n'y aura pas d'embrouille par la suit\"), ('260038', \"assuré qu'il n'y aura pas d'embrouille par la suite, je supprime la référence du workflow. Voilà, très bien. Donc, hop là, c'est parti. Alors, pourquoi je fais ça ? Parce que je vais essayer de recharger dans une autre variable. Et je ne veux pas qu'il y ait une interférence ou quoi que ce soit, ou qu'on croit que mon chargement n'est pas bon et que c'est le premier workflow que j'ai utilisé. Genre, il fait un truc en 12D. Non, non. Là, il est supprimé. On est bien d'accord. Donc, on n'a plus le\"), ('260039', \"pprimé. On est bien d'accord. Donc, on n'a plus le workflow. On n'a plus le pipeline. Alors, qu'est-ce qui se passe ? J'ouvre le fichier en lecture cette fois-ci. C'est le sens du R qui est là. J'ouvre le fichier en lecture. Et j'utilise Deal pour le loader. Très bien. J'affiche les étapes et on voit bien les étapes. On voit bien les différentes étapes. Je mets ça ici. Est-ce que je peux le voir ? Tac, tac, tac. Voilà, un peu comme ça. Regardez. Donc, il y a les transformers. C'est le Column Tra\"), ('260040', \"Donc, il y a les transformers. C'est le Column Transformer. Ensuite, il y a un premier Column Transformer. c'est la manipulation de x1. Avec la fonction qui est là, mais il ne le montre pas. Mais on a bien le pointeur de la fonction. Ça, c'est un premier point. Ensuite, il y a une deuxième fonction transformeur. Donc, le transformeur, là, c'est le Column Transformer. Il y a une deuxième fonction de transformation qui, cette fois-ci, s'applique sur x2. Il s'appelle v2, son identifiant, si vous vo\"), ('260041', \"r x2. Il s'appelle v2, son identifiant, si vous voulez. Ce n'est pas le nom de variable, parce qu'il peut y en avoir plusieurs ici. Et ensuite, une fois qu'on a fait ces transformations-là, l'étape suivante, c'est la régulation logistique, qui est juste là. Bien, qu'est-ce qui se passe ? Je vais le réappliquer sur l'échantillon de test pour voir, sachant que le workflow, à la base, j'avais détruit, donc c'est bien ici le workflow chargé, cette fois-ci que je manipule. Allons-y, et bien sûr qu'il\"), ('260042', \"is-ci que je manipule. Allons-y, et bien sûr qu'il a bien fonctionné, et voilà. Malgré qu'il y ait des fonctions lambda, c'est parce que j'ai utilisé Deal, le package Deal qui est là. donc comme j'ai utilisé le package Deal qui est là, malgré que dans mes fonctions de transformation, il y a des fonctions lambda ça marche et que pas alors aux étudiants concernés je sais pas qui c'est mais peut-être qu'ils se reconnaîtront voilà voilà vous allez faire un pipeline à mon avis, ça me paraît évident e\"), ('260043', \"ire un pipeline à mon avis, ça me paraît évident et peut-être qu'il y aura des transformations à faire sélectivement sur des colonnes ça aussi ça me paraît évident et dans cette transformation là, peut-être que vous allez surcharge des classes, ou peut-être que vous allez définir des fonctions à la volée. Si vous voulez définir des fonctions à la volée pour pouvoir faire cette transformation-là, à ce moment-là, je vous conseille d'utiliser fonction transformeur. Voilà. C'était le but de cette vi\"), ('260044', \"on transformeur. Voilà. C'était le but de cette vidéo. Allez, excellent travail à tous.\"), ('270001', \"Bien, c'est parti ! Dans cette vidéo, je vais parler de l'analyse conjointe qui combine une méthode factorielle avec une méthode de classification automatique, ou de clustering, si vous préférez. Il y a deux aspects que je veux absolument discuter : comment définir le bon nombre de facteurs et quelle stratégie mettre en place pour définir ce nombre. Je vais partir sur une piste dans un cadre particulier, mais j'espère que cela vous donnera des idées pour essayer de voir comment le faire dans un \"), ('270002', \"ées pour essayer de voir comment le faire dans un cadre qui se rapproche plus de ce que nous traitons.\\n\\nLe premier point est important : comment définir le bon nombre de facteurs ? Quelle stratégie mettre en place pour définir ce nombre ? Le deuxième point, qui me paraît important, est que bien évidemment, l'ensemble des traitements, on va le mettre dans un pipeline. Tout le monde est bien d'accord, il y aura un pipeline. Si vous ne faites pas un pipeline, je m'inquiéterai beaucoup quand même. D\"), ('270003', \" pipeline, je m'inquiéterai beaucoup quand même. Donc, la semaine prochaine, pour ceux du lundi, mais que je ne vois pas lundi parce que c'est férié, du coup, je les verrai, je vous le dis. Je vais m'attendre à ce qu'on fasse un pipeline. J'espère avoir été suffisamment clair là-dessus.\\n\\nDans un pipeline, comment on paramètre une des méthodes intermédiaires ou les méthodes intermédiaires d'un pipeline ? Rappelez-vous, un pipeline, c'est un empilement de méthodes, un séquencement de méthodes, et \"), ('270004', \"ment de méthodes, un séquencement de méthodes, et chaque méthode est paramétrée. Et comment, sans avoir à tout réinstancier, comment je peux manipuler les paramètres d'une des méthodes particulières dans la séquence ? Et comment accéder aux résultats d'une des méthodes en particulier qui est dans la séquence ? Très bien.\\n\\nRevenons un peu en arrière, ensuite je reviendrai sur le schéma SI et parlons de l'analyse conjointe, très rapidement. La référence, elle est là, il n'y a pas le meilleur. C'es\"), ('270005', \"rence, elle est là, il n'y a pas le meilleur. C'est quand même une des meilleures, je ne vais pas m'embrouiller comme dans le journal de Bridget Jones, les trois meilleurs, ah non mais finalement les cinq meilleurs, puis les trente meilleurs, non, non, une des meilleures, référence que je connais sur l'analyse des données, voilà, c'est le livre de Lebas, Morino et Piron. Et justement, à la page 200, j'ai révisé un peu avant de faire ma vidéo, les auteurs parlent justement de la complémentarité e\"), ('270006', \" auteurs parlent justement de la complémentarité entre analyse factorielle et classification. Classification au sens clustering, on est bien d'accord là-dessus. Parce que le terme classification, son acception a un peu évolué au fil du temps. Très bien, donc c'est pour ça que moi maintenant je parle clustering, pour éviter les confusions. Mais bon, on parle classification automatique, c'est bien une analyse non supervisée, un clustering, si vous voulez. Bien.\\n\\nDans son livre, il dit que les deux\"), ('270007', \"voulez. Bien.\\n\\nDans son livre, il dit que les deux peuvent se combiner pour avoir des résultats plus pertinents. Et justement, plus loin, ici, là, il donne une sorte de guide, une sorte de guide de cette démarche combinée. Donc, ils disent, d'abord, dans un premier temps, il faut faire une analyse factorielle. Donc, on projette les individus dans un espace factoriel. Très bien. Une fois qu'on a projeté les individus dans l'espace factoriel, on fait une classification sur facteurs. Bien évidemmen\"), ('270008', \"it une classification sur facteurs. Bien évidemment, tout ça est intéressant que si on a moins de facteurs que de variables. N'oublions pas que les facteurs sont une forme de compression, et que les informations les plus importantes, les patterns, sont sur les premiers facteurs, et que les facteurs qui restent, en réalité, on peut les considérer comme du bruit, qui proviennent des fluctuations d'échantillonnage. On peut le voir comme ça. Donc c'est ce qui est expliqué ici, c'est très bien expliq\"), ('270009', \"st ce qui est expliqué ici, c'est très bien expliqué donc ce livre là il est en ligne encore une fois, je remonte ce livre là il est en ligne je conseille sa lecture, vraiment c'est un excellent livre et moi je m'intéresse à cette partie là la complémentarité entre analyse factorielle et classification et je m'appuie sur le guide qui est là donc on fait une analyse factorielle on fait une classification de ce factor et dans ce cadre là, d'ailleurs la distance euclidienne elle est tout à fait lic\"), ('270010', \"s la distance euclidienne elle est tout à fait licite pour le coup comme les facteurs sont orthogonaux 2 à 2 très bien, utiliser la distance euclidienne dans cet espace de représentation est tout à fait licite pour réaliser par exemple un K-mins, n'importe quelle méthode de classification automatique, puisqu'on va calculer les distances dans ce cadre là et justement il soulève un point important qui est là, c'est que il dit la difficulté parfois réside dans le nombre de facteurs à retenir et bie\"), ('270011', \"réside dans le nombre de facteurs à retenir et bien oui C'est d'autant plus... Alors, il donne un lien, mais dans le lien qui est là, le paragraphe 4.2.3, on parle du choix de facteurs dans un cadre général de l'analyse factorielle. Nous, on est dans un cadre particulier. On veut chercher le sous-ensemble de facteurs le plus performant pour réaliser une classification automatique derrière. Quel est le bon nombre de facteurs pour que la classification automatique soit pertinente, pertinente, soit\"), ('270012', \"tion automatique soit pertinente, pertinente, soit efficace. C'est ça l'idée. Donc, on est dans un cadre un peu différent, mais la question reste tout à fait très importante. Si on prend tous les facteurs, c'est comme si on est dans l'espace originel. Et donc, s'il y a du bruit dans les données, on va ingérer tout le bruit. Donc, il faut essayer de retenir le bon nombre de facteurs. Ensuite, il met un autre kickline. J'insiste beaucoup, mais bon. Très bien. Comment descriire les classes? Très bi\"), ('270013', \" Très bien. Comment descriire les classes? Très bien. Et si on est dans un espace factoriel, moi, qui essaye de positionner les individus dans l'espace factoriel, par exemple quand on traite les individus supplémentaires. Par exemple. Bien. Voilà, voilà. Donc ce guide-là, à mon avis, il est intéressant. Je dis ça, je dis rien, mais je pense qu'il est intéressant. Alors du coup, j'ai déjà fait des vidéos là-dessus. Dans cette vidéo-ci, j'en parle justement, mais dans cette vidéo-ci, en fait, les \"), ('270014', \"justement, mais dans cette vidéo-ci, en fait, les variables actives sont toutes qualitatives. Donc faire un k-means directement sur des variables En qualitative, ça peut se faire en utilisant des... Mais il n'y a pas de librairie qui le fait de toute manière, donc on oublie. Très bien. Donc ce que je montre, c'est que je passe par une ACM, donc une analyse des correspondances multiples, et ensuite je fais le k-mint dans l'espace factoriel. Mais là, c'était quasiment obligé, si vous voulez, parce\"), ('270015', \"à, c'était quasiment obligé, si vous voulez, parce que comme les variables actives, comme les descripteurs sont qualitatifs, je n'ai pas de librairie qui sait traiter ça directement. Ou bien il y en a, mais bon, c'est des librairies plus ou moins exotiques dont la qualité de calcul, il faut absolument regarder en détail. Donc ici, se projeter dans un espace factoriel semble être un passage obligé. Quand on a des descripteurs quantitatifs, quand les variables sont toutes quantitatives, est-ce que\"), ('270016', \"es variables sont toutes quantitatives, est-ce que quand même, ça reste, du coup, on peut faire directement la camise dans l'espace original, est-ce que passer par un espace factoriel serait quand même intéressant? dans le livre justement il explique ça il explique ça en disant voilà il explique ici en disant que en réalité en prenant que les qu'un premier facteur ça permet d'avoir une forme de filtrage c'est ça l'idée c'est qu'on a une forme de filtrage « Tiens, c'est pas mal ça. » On a une for\"), ('270017', \"filtrage « Tiens, c'est pas mal ça. » On a une forme de filtrage. Du coup, on ne conserve que les informations pertinentes, pertinentes, il faut définir ce que c'est que pertinent, pour réaliser après la classification automatique. C'est ça, il faut vraiment lire ça. Il faut vraiment lire ça, parce que ça nous donne vraiment un excellent point de vue sur la nature même de la classification automatique et de la question cruciale de la qualité de l'espace de représentation quand on applique des mé\"), ('270018', \"'espace de représentation quand on applique des méthodes de machine learning, tout simplement. La qualité des données est essentielle, mais on peut se projeter dans un espace de représentation qui met plus en évidence cette qualité des données-là pour qu'on ait des résultats réellement pertinents. Alors, du coup, je reviens à mon schéma. Je reviens à mon schéma et regardons l'affaire. Donc, ça, c'est l'éternité originelle. Ensuite, je fais un centre d'adduction parce que je vais faire une ACP no\"), ('270019', \"tre d'adduction parce que je vais faire une ACP normée. Il y a les librairies qui ne le font pas automatiquement. c'est le cas typiquement de PCA de Sikitlern, une fois que les données sont en tout cas réduites, en tous les cas, centrage, réduction, réduite, c'est très important, je fais une ACP. Et ensuite, une fois que je fais une ACP, dans l'espace factoriel, je fais un CAMIT. Et quand j'ai un individu supplémentaire, qu'est-ce qui se passe? Et que je veux le projeter, que je veux le projeter\"), ('270020', \"t que je veux le projeter, que je veux le projeter dans mon espace factoriel, et ensuite, regarder le groupe de rattachement. Il y a une série d'opérations à faire. Et justement, les fers à la masse, extrêmement fastidieux, et ça enduit des erreurs, justement. Qu'est-ce qu'il faut faire? Ici, pour l'individu supplémentaire, il faut le centrer et réduire, déjà, en utilisant la moyenne et l'écart-type calculés sur l'échantillon d'apprentissage. L'échantillon qui a permis la construction de la part\"), ('270021', \"chantillon qui a permis la construction de la partition. C'est ça l'idée. De toute manière, si j'ai un seul individu, l'écart-type vaut 0, et vous allez faire un auto-number. On est d'accord? Donc oui, pour l'individu supplémentaire, j'en ai un seul. Il faut absolument utiliser de la moyenne et d'écartée pour calculer sur les champs d'apprentissage. Ensuite, donc c'est juste un transforme, on est d'accord. Ensuite, qu'est-ce qui se passe? Cet individu-là, je vais le projeter dans l'espace factor\"), ('270022', \"ividu-là, je vais le projeter dans l'espace factoriel. Je le projette en tant qu'individu supplémentaire. On est d'accord là-dessus. Donc n'allez pas faire un fit transforme là. Non, de toute manière, il y a un seul individu. Calculer l'espace factoriel avec un seul individu, ça va être très difficile. On est d'accord là-dessus.  dessus. Donc non, non, non, on le projette en tant qu'individu supplémentaire dans l'espace déjà construit. Une fois qu'on l'a projeté, on va calculer la distance au ba\"), ('270023', \"u'on l'a projeté, on va calculer la distance au bar et centre pour faire l'affectation. C'est lui qui le fait automatiquement, c'est l'outil qui le fait. Vous faites transform, vous avez les distances au bar et centre, et si vous faites predict, vous avez la classe rattachée. J'avais dit en cours, mais je le dis ici également, faire ça manuellement, c'est la meilleure manière de faire des erreurs, parce qu'on va oublier les étapes, on va faire des mauvaises manips. tout l'intérêt de la faire, c'\"), ('270024', \"s mauvaises manips. tout l'intérêt de la faire, c'est de mettre cette séquence de traitement-là dans un pipeline. Voilà. Le mettre dans un pipeline, et comme ça, là, on fait un feed transform, simplement. Et quand j'ai un individu supplémentaire, je fais juste un transform. Ou un predict, et c'est lui qui va appliquer, c'est l'outil, c'est le pipeline de c'est qui te l'air, qui va appliquer les différentes étapes. Bien, c'est bien ça. Bien. Alors, pour voir ça, on va utiliser un jeu de données u\"), ('270025', \", pour voir ça, on va utiliser un jeu de données un peu particulier. C'est là que je disais que je me place dans un cadre particulier. On va utiliser les données, les ondes de Brehmann, qui est dans le fameux livre de Brehmann, Friedman et puis Olsen-Leston. On dit tout le temps Brehmann, mais ils sont quatre dans l'histoire. Brehmann, Friedman, Olsen-Leston. Très bien. Alors, c'est des données qui sont... On a un générateur, c'est des données synthétiques, c'est des données artificielles. On a \"), ('270026', \"nthétiques, c'est des données artificielles. On a 21 descripteurs. et on a des classes prédéfinies. Alors ça, ça reste toujours une vraie question. Pourquoi on s'embête à faire du non-supervisé si on a les classes, les vraies classes d'appartenance? C'est une vraie question. Alors généralement, on utilise ces données étiquetées là pour évaluer les méthodes d'apprentissage non-supervisé, pour voir leur comportement, pour ainsi de suite. Donc je me place dans un cadre très spécifique ici. Nous, on\"), ('270027', \" place dans un cadre très spécifique ici. Nous, on n'a pas ça. Nous, on n'a pas ça. Donc, il faudra réfléchir, et non pas à des mesures externes, mais à des mesures internes, pour comparer les différentes solutions. D'accord, mais je vais revenir là-dessus. Très bien. Alors, j'ouvre la base rapidement. Voilà. Voilà la fameuse base. Hop, là, j'aurai dit ça un peu. Très bien. On a les différentes variables. Là, je réduis un peu encore, parce qu'on ne va pas bien. Voilà, on a les 21 descripteurs, e\"), ('270028', \"ne va pas bien. Voilà, on a les 21 descripteurs, et là, la vraie classe d'appartenance à trois catégories. Très bien. alors pourquoi j'utilise une base étiquetée? parce que ça me permet justement d'utiliser des mesures externes d'évaluation qui vont, même si on fait du clustering vont permettre de comparer les labels, les étiquettes fournies par le clustering avec les vraies classes d'appartenance alors pourquoi on s'embête à faire du méthode non supervisé, si on connaît les vraies classes d'app\"), ('270029', \" supervisé, si on connaît les vraies classes d'appartenance, c'est souvent utilisé, en fait, généralement, pour évaluer les méthodes et tout ça. Vous, typiquement, là, je parle à mes étudiants, pour le coup, vous, vous n'aurez pas les vraies classes d'appartenance. Donc, on ne peut pas utiliser des mesures externes. Il faut utiliser des mesures internes. Mais lesquelles? Parce qu'on va faire varier le nombre de facteurs. Et si on prend l'inertie, vous allez comparer des inerties, en fait, calcul\"), ('270030', \" vous allez comparer des inerties, en fait, calculées dans des dimensionnalités différentes, Ne me faites pas ça, s'il vous plaît. Ce n'est pas bon du tout, on est bien d'accord. Non, non, non, non, c'est interdit. On est d'accord. N'allez pas comparer des inerties calculées dans des espaces qui ont des dimensionnalités différentes. Ça n'a aucun sens. Donc, dans les mesures internes, il faut essayer de voir quelles mesures on pourrait utiliser qui permettent de dépasser cet écueil-là. Ou bien il\"), ('270031', \"i permettent de dépasser cet écueil-là. Ou bien il n'y en a pas, en fait. Du coup, il faut trouver d'autres artifices. Ou bien ce n'est pas possible aussi. Je ne sais pas, c'est à vous de voir. Dans ma vidéo, en tous les cas, moi, je me place dans un cadre très confortable, c'est que je connais la vraie classe d'appartenance, du coup mes différentes solutions, le continuum de solutions que je vais proposer, je peux les comparer avec des mesures externes. Mais encore une fois, c'est un cadre très\"), ('270032', \"xternes. Mais encore une fois, c'est un cadre très particulier que j'ai fait ici. Ce serait trop facile si je mets tout dans la vidéo, on est d'accord. Je donne des éléments, après c'est à vous de trouver la bonne extension. Oui, d'accord. Bon, après ce très long préambule, voilà, commençons. Alors, je rappelle deux secondes, donc j'ai deux questions. Un, dans un pipeline, comment je vais faire varier ça? Voilà, quelle stratégie utiliser pour faire varier le nombre de facteurs que je retiens dan\"), ('270033', \"re varier le nombre de facteurs que je retiens dans l'analyse conjointe et avoir un critère d'évaluation. Très bien, ça c'est un premier point. Le deuxième point, c'est que ces différentes étapes-là sont encapsulées dans un pipeline. Donc comment je peux sélectivement aller sur une des méthodes insérées dans un pipeline pour essayer de modifier ces paramètres et accéder à ces résultats? C'est ça les deux éléments que je mets en avant dans cette vidéo aussi. Allons-y alors. Donc, j'ai préparé. Al\"), ('270034', \"idéo aussi. Allons-y alors. Donc, j'ai préparé. Alors, comme d'habitude, je charge les données. Ceci, il y a 1000 observations. J'ai pris une version de 1000 observations. C'est un générateur, donc je fais ce que je veux. Il y a 22 colonnes, donc il y a les 21 variables, les 21 descripteurs, les variables actives. Et l'onde, c'est l'étiquette. Très bien, c'est l'étiquette. Je vais réduire ça un peu pour qu'on voie mieux. Voilà. Très bien. Donc, je vais sélectionner les variables actives, pour me\"), ('270035', \"e vais sélectionner les variables actives, pour mettre de côté l'étiquette, ondes, donc j'ai bien 20, 0, 1 jusqu'à V, 21, et je calcule la distribution des classes. Des classes, pas des clusters, des classes. Très bien, donc il y a trois classes, A, B et C, et les effectifs sont équilibrés. Alors, comme je vais l'utiliser au fil du temps, je récupère le nombre d'observations, parce que je viens d'avoir l'usage, il y a 1000 observations. et bien du coup je vais construire le pipeline alors il y a\"), ('270036', \"u coup je vais construire le pipeline alors il y a plein de warnings dans les camines dans les différentes nodes, même dans Seaborn, en fait il y a des warnings de future version c'est à dire que dans la version suivante telle méthode, tel paramètre va être déprécaté, ainsi de suite donc au bout d'un moment ça prend la tête si vous voulez, donc ce que je fais ici c'est que j'enlève les warnings tout simplement par rapport aux futures versions et au fait qu'il y a une fuite de moments dans les ca\"), ('270037', \"au fait qu'il y a une fuite de moments dans les camins quand on fait du multi-thread. Ça permet d'éviter les warnings qui rembournent juste l'esprit. Donc là, je construis mon pipeline. Je rappelle, je dois faire un centrage-réduction. C'est ce que je fais là. Ensuite, la deuxième opération, c'est que je dois faire une analyse factorielle, une ACP en l'occurrence, pour projeter les individus dans l'espace factoriel. C'est la deuxième étape qui est là. Et le dernier point, ensuite, une fois que j\"), ('270038', \"t là. Et le dernier point, ensuite, une fois que je suis dans l'espace factoriel, je vais lancer un k-means dans l'espace factoriel. C'est le troisième point qui est là. Alors, dans le standard scalaire, je n'ai pas de paramétrage. Donc là, qui s'appelle STD, c'est un tuple là. Ça, c'est le nom de la méthode qui est insérée. C'est l'identifiant de la méthode de machine learning ou de préparation de variables qui est insérée dans le pipeline. ça c'est le nom de la deuxième étape et dans la CP au \"), ('270039', \"'est le nom de la deuxième étape et dans la CP au départ je prends tous les facteurs, il y a autant de facteurs qu'il y a de variables dans mes données donc je suis dans l'espace originel et je prends un algorithme exact, sinon il utilise un autre algorithme du coup d'un coup à l'autre on n'a pas les mêmes résultats c'est très gênant ça, puis on a des petites dimensionnalités un petit jeu de données, donc on peut utiliser le calcul exact si vous voulez. Très bien. Et ensuite, la dernière étape d\"), ('270040', \"voulez. Très bien. Et ensuite, la dernière étape de mon pipeline, c'est les camines, avec trois clusters. Pourquoi? Parce que je sais qu'il y a trois classes. Donc, je vais comparer mes trois clusters avec mes trois classes, tout simplement. Le véritable enjeu ici, c'est le nombre de facteurs qu'on va garder dans la CP. On est d'accord là-dessus. Très bien, avec les paramétrages ici. Donc, je lance ça. Voilà. Et on a bien ces différentes étapes. Vous avez ici standard scalaire, vous avez l'ACP a\"), ('270041', \"Vous avez ici standard scalaire, vous avez l'ACP avec les paramétrages, et vous avez les camines avec les paramétrages. Donc on a bien un pipeline ici avec ces différents éléments-là. Très bien. Alors l'idée c'est qu'on a ces différents pipelines-là et on doit pouvoir manipuler les différents éléments intermédiaires-là. Notamment ici l'ACP. C'est l'ACP qui m'intéresse. Donc je dois pouvoir accéder à l'ACP sélectivement dans le pipeline et modifier ces paramètres. C'est ça l'idée. Alors, quand mê\"), ('270042', \"r ces paramètres. C'est ça l'idée. Alors, quand même, je vais regarder le nombre de clusters, les résultats fournis par ce premier traitement-là, où je prends la dimensionnalité totale, l'ensemble des facteurs. Donc là, le premier élément, c'est que je peux accéder sélectivement à chaque étape, à chaque méthode de mon pipeline en utilisant la propriété NamesStep. Avec NamesStep, vous pouvez accéder à une des étapes de votre pipeline. Donc là, c'est l'étape KM qui correspond au KMins. Qu'est-ce q\"), ('270043', \"st l'étape KM qui correspond au KMins. Qu'est-ce qui se passe? Je récupère les labels, donc les classes attribuées, les clusters attribués par la méthode. Donc, premier élément important, nous avons la possibilité d'accéder sélectivement à chacune des méthodes qui est insérée dans un pipeline. Et accéder à ses propriétés. C'est ce que je fais ici. Donc, on a bien ici, il y a trois clusters, j'ai demandé trois en trois, et voilà les effectifs. Est-ce que c'est les bons? Je ne sais pas. Voilà, trè\"), ('270044', \"-ce que c'est les bons? Je ne sais pas. Voilà, très bien, on verra ça plus tard. Alors, l'autre élément important, c'est que si je veux accéder à l'ACP, pareil, on le voit bien, l'ACP s'appelle ACP ici, le PCA s'appelle ACP. Donc, avec le name step toujours, j'accède à l'ACP.  Ce que je vais afficher ici, c'est les valeurs propres, c'est les variances qui sont associées aux facteurs. Prêt. Alors, première information importante, dans un pipeline, nous pouvons accéder sélectivement à chaque métho\"), ('270045', \" nous pouvons accéder sélectivement à chaque méthode insérée dans le pipeline et afficher ou manipuler les propriétés associées avec NamedStep. Donc là, je le fais pour les k-means et là, je le fais pour la CP. D'accord là-dessus? Alors, quelle stratégie je vais utiliser du coup? Quelle stratégie je vais utiliser pour définir ici le bon nombre de facteurs? Comme j'ai la vraie classe d'appartenance, je peux comparer les clusters attribués avec la vraie classe d'appartenance, avec des mesures exte\"), ('270046', \"vraie classe d'appartenance, avec des mesures externes. Encore une fois, je suis dans un cadre très particulier. Dans un autre cadre, on n'a pas les vraies classes, et du coup, il faudrait utiliser d'autres types de mesures. Mais moi, comme j'ai les vraies classes, je vais utiliser une mesure qui est la V-mesure. La V-mesure, c'est comme la F-mesure en supervisé, c'est qu'il y a deux mesures qui sont mises en avant, l'homogénéité, l'autre la complétude, c'est expliqué ici, et qui est calculée un\"), ('270047', \"létude, c'est expliqué ici, et qui est calculée une moyenne harmonique, pondérée. La valeur par défaut, c'est 1, cette pondération-là donne la même importance à ces deux entités-là. dans le CIUIA, des supports de courant qui expliquent exactement ce que c'est que l'homogénéité ce que c'est que la complétude on peut faire un peu le parallèle avec le rappel et la précision qu'on a en supervisé mais donc on est dans un cadre très particulier là, on est dans un cadre où on a la vraie classe d'appart\"), ('270048', \"est dans un cadre où on a la vraie classe d'appartenance et donc on va comparer notre cluster avec la vraie classe d'appartenance encore une fois si on n'a pas la vraie classe d'appartenance il faut partir sur une autre solution les mesures internes Et il faut bien voir quelle mesure interne serait appropriée. Moi, ma stratégie est très simple. Je vais faire varier le nombre de facteurs et je vais confronter les clusters obtenus avec les vraies classes. C'est ça l'idée. Je fais varier le nombre \"), ('270049', \"lasses. C'est ça l'idée. Je fais varier le nombre de facteurs et je confrote les clusters obtenus avec les vraies classes, tout simplement, en utilisant cette V-mesure-là. C'est ça l'idée. Maintenant que je t'ai dit, l'algorithme est très simple. Regardez. Donc, perf, ça va être la mesure. Je crée un vecteur de 0 que je vais remplir au fur et à mesure. Et au fil des itérations, qu'est-ce qui se passe? Je boucle avec, je commence à 1, ensuite 2, donc 1 facteur, 2 facteurs, 3 facteurs, jusqu'à 10 \"), ('270050', \"onc 1 facteur, 2 facteurs, 3 facteurs, jusqu'à 10 facteurs. Le max, je suis mis à 10 ici. Vous avez vu, le nombre de facteurs, j'ai mis à 10. Je vais faire un facteur, je fais les k-mints, je compare les clusters obtenus avec les vraies classes d'appartenance. Ensuite, je passe à deux facteurs, à deux composantes principales. Je refais la même étape, ainsi de suite. C'est exactement ce qui se passe. Et ce qui est intéressant, c'est quoi? C'est que, regardez, là j'ai créé le pipeline. Vous avez v\"), ('270051', \"e, regardez, là j'ai créé le pipeline. Vous avez vu, je l'ai appelé pipe. Voilà. J'ai créé, j'ai instancié, j'ai fait fit dans un premier temps. Je n'ai pas besoin de le réinstancier avec la nouvelle valeur du nombre de facteurs. Non, je n'ai pas besoin de ça. En réalité, sur un pipeline déjà construit, on peut modifier sélectivement les paramètres d'une des méthodes qui est incorporée dans le pipeline. Là, dans le pipeline existant, je vais modifier le paramètre de l'ACP. Ensuite, j'ai deux und\"), ('270052', \"fier le paramètre de l'ACP. Ensuite, j'ai deux underscore et j'ai le nom du paramètre. Donc, pour modifier un des paramètres d'une des méthodes qui est incorporée dans un pipeline, dans un ensemble, on met le nom de la méthode. Le nom, c'est ce qu'on a défini ici. Voilà, ACP. Une fois qu'on a mis le nom de la méthode, on met un double underscore. Et après le double underscore, on met le nom du paramètre qu'on veut manipuler. et le paramètre qu'on veut manipuler, c'est le nComponent. Et dans le n\"), ('270053', \" veut manipuler, c'est le nComponent. Et dans le nComponent, je mets d'abord 1, donc ça commence à 0, le nBFact, donc 0 plus 1, ça fait 1, ensuite 2, 3 jusqu'à 10. Et je fais un feed de ça. Ensuite, là c'est le résultat, qu'est-ce qui se passe? Une fois que j'ai le résultat, je vais appeler la mesure V-score, V-mesure, en comparant les classes d'appartenance, les vraies classes d'appartenance, avec les labels qui ont été fournis par les k-means qui sont associés à cette valeur de paramétrage. Vo\"), ('270054', \"ui sont associés à cette valeur de paramétrage. Voilà, nommé step, km. km, c'est les k-means qui sont là. Et le label s'associe. Et je récupère la vue mesure. Allons-y. On a les différentes valeurs de vue mesure. Alors toujours, on se dit, est-ce que voilà? On fait un petit graphique. Et on voit, quand j'ai un seul facteur, la vues-mesures n'est pas terrible. Quand je passe à deux facteurs, mais toujours à trois clusters, là on est tout le temps à trois clusters, donc le nombre de clusters n'a j\"), ('270055', \"à trois clusters, donc le nombre de clusters n'a jamais été modifié ici. La seule chose qu'on modifie, c'est le nombre de facteurs utilisés. Mais on travaille bien un nombre de clusters égal. Quand on passe à deux facteurs, la vues-mesures s'améliore grandement. Et ensuite, quand on passe à trois, quatre, on ne fait pas mieux. Donc, deux facteurs suffisamment dans ce cadre-là. Alors, du coup, je vais créer le pipeline final avec ces deux facteurs-là. Je fixe le paramètre de l'ACP à deux composan\"), ('270056', \"-là. Je fixe le paramètre de l'ACP à deux composantes. Voilà, un. Là, c'est le nom de la méthode insérée dans le pipeline. Deux underscore, deux underscore. Ensuite, le nom du paramètre. Voilà, je refais le fit et j'affiche ici l'inertie intra-class. Bon, très bien. Donc, je peux projeter les individus dans l'espace factoriel. Là, pour le coup, j'appelle d'abord le transforme du centrage de la déduction, de la standardisation. Ensuite, j'appelle le transforme de la CP. C'est ce qui me permet d'a\"), ('270057', \"le transforme de la CP. C'est ce qui me permet d'avoir les coordonnées factorielles. Donc, il y a bien deux étapes. D'abord, je fais le centrage de déduction. Ensuite, j'appelle la projection dans l'espace factoriel. J'ai les coordonnées factorielles. et je vais rajouter à mon data frame. Donc là, j'ai inséré dans un data frame. J'agrandis ça un peu pour qu'on voie bien le code. Voilà. Donc, j'ai créé un data frame. Alors, je l'ai fait de manière générique parce que si on prend 3, 4 et tout ça, \"), ('270058', \" générique parce que si on prend 3, 4 et tout ça, il faut que mon code s'adapte. Vous êtes d'accord? Voilà, c'est juste ça. Très bien. Comme j'en ai deux ici, c'est bien les deux facteurs qui sont là. Je vais rajouter la belle. Et du coup, je peux projeter les groupes dans l'espace factoriel. C'est des données très connues. Donc, il fallait bien deux facteurs. On est bien d'accord. Voilà, et les trois groupes sont à peu près là, on voit bien, premier groupe, deuxième groupe, troisième groupe ici\"), ('270059', \"mier groupe, deuxième groupe, troisième groupe ici, qui sont avec un V mesure qui est égal à, alors la V mesure on avait à peu près par là. Alors, pour conclure, Dans une procédure d'analyse conjointe qui mixe l'analyse factorielle avec une méthode de clustering il y a différents paramétrages possibles parmi les paramétrages possibles il y a le nombre de facteurs donc c'est une question clé Dans cette vidéo, je me suis positionné dans un cadre très favorable c'est qu'on connait les vraies classe\"), ('270060', \"ès favorable c'est qu'on connait les vraies classes d'appartenance Du coup, je peux comparer les différentes solutions en confrontant les clusters obtenus avec ces vraies classes-là. Très bien. Si on n'est pas dans ce cadre-là, c'est à vous de voir, parmi les mesures internes, laquelle je pourrais utiliser dans ce cadre-là, pour répondre à ça. Très bien. Ça, c'est un premier point important. On est bien d'accord là-dessus. Le deuxième point important, c'est que c'est quand même mieux d'incorpore\"), ('270061', \"tant, c'est que c'est quand même mieux d'incorporer ces différentes étapes-là dans un pipeline. Parce que du coup, quand on va faire le déploiement, ce sera plus facile à manipuler. Mais du coup, se pose une question clé, c'est comment dans un pipeline, je peux manipuler les étapes intermédiaires. Soit pour accéder aux résultats des étapes intermédiaires, soit pour modifier les paramètres de ces étapes intermédiaires. Et justement, dans notre outil, on avait bien vu, il y a deux outils qui sont \"), ('270062', 'il, on avait bien vu, il y a deux outils qui sont importants. Avec les Named Steps, on peut accéder aux résultats intermédiaires. Et avec SetParam, on peut fixer les paramètres des étapes intermédiaires. Voilà, très bonne révision à tous.'), ('280001', \"Bien, c'est parti. Dans cette vidéo, nous allons nous amuser un peu et parler de Microsoft Copilot et ChatGPT, en simplifiant. La question se pose alors : en tant qu'enseignant, comment dois-je intégrer cet outil dans mon dispositif pédagogique ? Dois-je l'interdire ? Ces questions sont importantes, mais il n'y a pas de réponse universelle.\\n\\nAvant, les étudiants allaient à la bibliothèque pour trouver des livres et faire une synthèse. Les mauvais étudiants recopiaient des livres, tandis que les \"), ('280002', \" étudiants recopiaient des livres, tandis que les bons étudiants essayaient de faire une synthèse pour ajouter de la valeur. Plus récemment, ils utilisent Internet pour des requêtes Google, et les mauvais étudiants recopient Wikipédia, tandis que les bons étudiants font une synthèse. Avec l'IA générative, la question change, car l'IA fournit des réponses toutes faites. Le travail de synthèse est donc fait par l'IA. Avec un modèle LLM comme ChatGPT, l'IA fait la synthèse. Alors, quel doit être le\"), ('280003', \"T, l'IA fait la synthèse. Alors, quel doit être le rôle de l'étudiant ? Y a-t-il une valeur ajoutée pédagogique pour eux ?\\n\\nEn master, ce problème n'existe pas. Les projets sont trop complexes pour que les solutions toutes faites soient utiles. Les étudiants doivent créer des applications, et les solutions toutes faites ne sont pas disponibles. Les outils d'IA générative basés sur les LLM, comme ChatGPT, peuvent aider à progresser rapidement. Cependant, en lecture, les questions complexes ne peu\"), ('280004', 'endant, en lecture, les questions complexes ne peuvent pas être posées. Les étudiants sont en phase d\\'apprentissage et doivent commencer par des questions simples pour progresser.\\n\\nUn de mes étudiants m\\'a dit : \"Monsieur, du Renault TD, tout le monde connaît mes fiches de TD qui font 30 km, ça ne changera pas.\" Pour accompagner les étudiants, je leur donne des tutoriels et des fiches de TD, mais je ne veux pas qu\\'ils fassent du copier-coller. Je change les données et les questions pour qu\\'ils ai'), ('280005', 'change les données et les questions pour qu\\'ils aillent plus loin que le tutoriel.\\n\\nUn étudiant m\\'a dit : \"Copilot propose du code que je retrouve dans votre tutoriel. C\\'est assez amusant.\" J\\'ai regardé et j\\'ai vu que Copilot utilisait mes propres tutoriels. Cela pose un défi pour l\\'enseignement. Je vais vous montrer quelques exemples.\\n\\nJ\\'ai ouvert Edge, activé Copilot et posé une question : \"Peux-tu me proposer un programme Python qui réalise une analyse discriminante prédictive ?\" Copilot a tr'), ('280006', 'e analyse discriminante prédictive ?\" Copilot a trouvé un exemple de programme et a proposé des données. J\\'ai vu que Copilot utilisait mes propres tutoriels. Cela interroge sur la manière dont nous devons réagir.\\n\\nJ\\'ai posé une autre question sur le NLP (Natural Language Processing) et Copilot a proposé un programme Python pour la catégorisation de documents. Il a utilisé FastText, un outil que je connais bien. Copilot a proposé plusieurs solutions, y compris des tutoriels que j\\'avais rédigés.\\n\\n'), ('280007', \"ns, y compris des tutoriels que j'avais rédigés.\\n\\nJ'ai posé une question sur la recherche de communauté dans les réseaux sociaux, un sujet de mon cours de machine learning. Copilot a proposé plusieurs solutions, y compris des tutoriels que j'avais mis en ligne.\\n\\nJ'ai posé une question sur un exercice sous R, un sujet de mon TD en Master 1. Copilot a proposé une solution basée sur les cartes de Kohonen. J'ai vu que Copilot utilisait mes propres tutoriels.\\n\\nComment devons-nous réagir ? Retirer mes\"), ('280008', \"toriels.\\n\\nComment devons-nous réagir ? Retirer mes tutoriels en ligne est hors de question. Empêcher les robots de scanner mes vidéos est également difficile. Cela pose des questions sur l'organisation des séances, des évaluations et la transmission de la connaissance. C'est un vrai challenge pédagogique et professionnel.\\n\\nEn conclusion, chaque séance est un challenge pour les étudiants. Il faut complexifier les questions pour qu'ils aillent plus loin. C'est intéressant et amusant de partager ce\"), ('280009', \" loin. C'est intéressant et amusant de partager cela avec vous. Excellent travail à tous.\"), ('290001', \"Bien, c'est parti ! Dans cette vidéo, je vais parler des pipelines, de l'outil pipeline de CKITLER. J'ai mis la page web là, un rappel très rapide : un pipeline est un outil qui permet d'empiler des séquences de traitement. C'est ce qui est expliqué ici, donc je ne vais pas réexpliquer ce qu'il y a là. Il faut simplement que chaque élément de la suite de traitement, si vous voulez, soit des instances de classes qui implémentent les méthodes `fit` et `transform`. Voici ce qui est écrit là égaleme\"), ('290002', \" et `transform`. Voici ce qui est écrit là également.\\n\\nAlors, je fais une petite première parenthèse : on peut mettre en fait des fonctions définies à la volée directement dans des pipelines. Il y a d'autres packages pour enregistrer ça, pas le pickle, mais il y en a d'autres. On va voir ça tout à l'heure, mais on peut faire également ce genre de choses. Donc, si vous avez des séquences de traitement de manipulation de données et que ce n'est pas nécessaire de les mettre dans des classes, c'est \"), ('290003', \" nécessaire de les mettre dans des classes, c'est tout à fait possible. Mais bon, c'est un autre propos, ce n'est pas mon propos aujourd'hui.\\n\\nMon propos aujourd'hui, c'est que dans la séquence de traitement, on va mettre en place des actions conditionnelles et pour cela, on va utiliser l'outil `ColumnTransformer`. Qu'est-ce qui se passe ? Souvent, les données qu'on manipule sont hétérogènes : certaines sont de certains types avec certaines caractéristiques. Du coup, on fait des actions conditio\"), ('290004', \"téristiques. Du coup, on fait des actions conditionnelles. On fait des actions conditionnelles : pour certaines variables, on fait tel type de transformation, pour tel type de variable, on fait d'autres types de transformation. Par exemple, certaines on fait du one-hot encoding, c'est pour les variables qualitatives, et quand c'est quantitatif, on fait de la standardisation. Donc, là, c'est pas le même type de traitement et on ne peut pas les empiler directement comme ça. Il faudrait mettre en p\"), ('290005', \"iler directement comme ça. Il faudrait mettre en place des actions conditionnelles dans un pipeline. On n'est plus dans les séquences simples de traitement. Dans les vidéos que j'ai faites sur les pipelines Python, j'ai beaucoup parlé de séquences simples de traitement, mais dans toutes les vidéos que j'ai faites, on a des opérations successives qui s'enchaînent tout simplement. Ce qui est intéressant avec `ColumnTransformer`, c'est qu'on peut mettre en place des actions conditionnelles selon le\"), ('290006', \"ttre en place des actions conditionnelles selon le type des variables, et ça, c'est très intéressant parce que du coup, ça enrichit notre traitement sans régime, notre traitement, et on a des pipelines qui sont un peu plus souples pour faire ce qu'on veut faire réellement.\\n\\nAlors, pour bien montrer ça, je vais reproduire un calcul sur l'analyse factorielle des données mixtes. Donc, ma page d'analyse factorielle est là, et parmi les méthodes qu'on va voir, c'est l'analyse factorielle des données \"), ('290007', \" va voir, c'est l'analyse factorielle des données mixtes qui est là. Alors, j'ouvre la page ici. Qu'est-ce qui se passe ? C'est une généralisation de l'ACP à des variables mixtes, qui sont hétérogènes, certaines quantitatives, certaines qualitatives. Et ce qui se passe en réalité, c'est qu'on peut appliquer l'ACP sur le jeu de données pourvu qu'on ait préparé les données convenablement. Donc, c'est ce qui est montré dans le support ici : en réalité, les données quantitatives, on les met à part, \"), ('290008', \"té, les données quantitatives, on les met à part, ensuite, les qualitatives, on les met à part, on les code en 0,1, et ensuite, on applique deux fonctions de normalisation, qui ne sont pas les mêmes. Donc, là, c'est un centre à réduction, et là, on prend la valeur codée 0,1 et on la déflate de la racine carrée de la fréquence. Bien. Alors, il y a des packages qui le font automatiquement. Il y a un package que j'apprécie beaucoup, justement, c'est `Scientist Tools`. Voilà. Et `Scientist Tools`, e\"), ('290009', \" `Scientist Tools`. Voilà. Et `Scientist Tools`, elle est programmée. Il y a la FDM qui est programmée dans `Scientist Tools`, effectivement. Du coup, on a une référence. On a une référence de résultats. Donc, du coup, je vais utiliser les mêmes données avec, d'une part, les résultats de `Scientist Tools` qui sont dedans. Là, j'ai préparé là. Voilà. Donc, j'ai chargé `Scientist Tools`. Et on va utiliser le même jeu de données, et on va voir si mon pipeline permet de retrouver ce résultat-là en u\"), ('290010', \"n pipeline permet de retrouver ce résultat-là en utilisant les possibilités de `ColumnTransformers`. Des actions sélectives selon les types des variables ou même on aurait pu aussi choisir les variables à utiliser directement en réalité.\\n\\nBien, alors très rapidement, j'ai schématisé l'affaire avec ceci. Toujours mettre un petit schéma, c'est toujours mieux. J'ai mis ici le schéma qui va nous diriger. C'est toujours très important. Donc, on a un tableau de données initial. On a un tableau de donn\"), ('290011', \"ableau de données initial. On a un tableau de données initial qui est hétérogène : certaines variables sont quantitatives, certaines sont qualitatives. Donc, je vais utiliser `ColumnTransformer` dans mon pipeline. Donc, ce que j'ai mis en pointillé là, c'est tout ce que j'ai dans mon pipeline. Je n'ai pas mis un petit icône pour montrer que c'est le pipeline, mais c'est bien le pipeline ce que j'ai mis en pointillé. Et donc, dans mon pipeline, qu'est-ce qu'il y a ? Il y a d'abord un `ColumnTrans\"), ('290012', \"'est-ce qu'il y a ? Il y a d'abord un `ColumnTransformer`, l'outil que j'ai présenté là, pour mettre d'un côté les variables quantitatives et de l'autre côté les variables qualitatives. Ensuite, sur les quantitatives ici, je fais un centrage réduction, une standardisation très simple. Très bien, ça c'est fait. Ensuite, pour les qualitatives, je fais d'abord le one-hot encoder en vertu de ce qui est décrit dans le cours. Le cours est là, voilà. Donc, je fais d'abord un one-hot encoder, cette part\"), ('290013', \"nc, je fais d'abord un one-hot encoder, cette partie-là. Et une fois que j'ai fait le one-hot encoder, j'applique ma pondération. C'est la partie qui est là cette fois-ci, bleue. Vous voyez ? C'est validé. Une fois que j'ai fait ces deux bases-là, ces deux sous-bases, voilà, le vert et le bleu ciel, ici, là, sont réunies. Elles sont consolidées. Elles sont remises ensemble, si vous voulez. Elles sont concaténées. Voilà. Elles sont concaténées. Et une fois qu'elles sont concaténées, voilà, j'appl\"), ('290014', \" une fois qu'elles sont concaténées, voilà, j'applique la CP et j'obtiens les coordonnées factorielles de la FDM. Donc, l'analyse factorielle des données mixtes. Et c'est ce que dit `ColumnTransformers`. C'est ce que dit `ColumnTransformers`. Alors, les données transformées seront concaténées. Alors, je fais un petit salut, un petit merci à Nusra et à Fatou, qui m'ont montré cette idée-là. Moi, je ne la connaissais pas. Mais une année, j'avais donné en examen des choses difficiles, comme je le f\"), ('290015', \"nné en examen des choses difficiles, comme je le fais d'habitude. Et elles me sortent ça de derrière les fagots, je me suis dit, tiens, tiens, un truc que je ne connais pas. J'ai regardé en détail et j'ai trouvé l'idée géniale. Il manquait tout simplement des éléments pour que ça fonctionne réellement parce qu'elles ont deux heures pour trouver une solution, la mettre en œuvre et s'assurer que ça marche. C'est très difficile, c'est vraiment difficile. Mais au moins, elles avaient trouvé l'idée. \"), ('290016', \"cile. Mais au moins, elles avaient trouvé l'idée. Moi, là, comme je suis un peu à tête reposée, du coup, comme disait la sanction de Sardou, la sanction des vieux amants, ça y est, on a marié le dernier. Moi, ça y est, mes étudiants sont en stage. Donc, ça y est, j'ai un peu de temps maintenant pour revenir et réfléchir à mes idées, voir en place les choses, refaire des vidéos pour partager ce que je trouve. Du coup, je me suis dit, il faudrait que je regarde ça pour pouvoir le mettre en place.\\n\"), ('290017', \"ue je regarde ça pour pouvoir le mettre en place.\\n\\nBien. Alors, qu'est-ce qu'on va faire ? On va reprendre les mêmes données qu'ici. Alors, les données sont là. Eh bien, c'est les joueurs de tennis. C'est une base de mon cru. Il n'existe pas. C'est moi qui l'ai inventé. Enfin, inventé, c'est moi qui l'ai collecté. On va prendre les joueurs des années 2000-2010, Djokovic, Federer, Murey et Nadal, il n'y a plus que Djokovic qui soit réellement performant là pour l'instant, voilà, et oui mais bon i\"), ('290018', \"ormant là pour l'instant, voilà, et oui mais bon ils nous ont fait rêver pendant près de 20 ans ces gars-là quand même, bien, donc on va utiliser les variables actives, les individus actifs qui sont là, avec les variables actives qui sont là, et on l'utilisera comme variable supplémentaire, là pour le coup c'est pour l'anecdote, leur classement en double. Certains ont bien fonctionné en double, comme Edbert par exemple, qui a été numéro 1 mondial dans les deux catégories. Puis d'autres, le doubl\"), ('290019', \" dans les deux catégories. Puis d'autres, le double, ils s'en foutaient totally. Bjornborg, je sais même, pourtant il a fait des bons doubles, il a gagné la coupe Davis en jouant les doubles. Donc, c'est pas un mauvais non plus en double. Mais bon, ça n'intéressait pas plus que ça. C'est pour l'anecdote. Le plus important, c'est que du coup, je vais mettre en parallèle les résultats de FactoMiner de ScientistTools, une sorte de FactoMiner pour Python si vous voulez, et on verra si en utilisant l\"), ('290020', \"thon si vous voulez, et on verra si en utilisant le pipeline, je vais retrouver les mêmes résultats. Je ferme ça, le pipeline, je vais bien le garder parce que je vais l'utiliser, et je mets en place également ces résultats-là parce qu'ils me serviront de référence. C'est ça qui est intéressant, ils vont nous servir de référence, on va regarder si on a les mêmes résultats. Mais l'idée de base, c'est bien de reproduire ce calcul-là. Allons-y alors, je ferme tout ça et j'ouvre mon petit notebook. \"), ('290021', \", je ferme tout ça et j'ouvre mon petit notebook. Comme d'habitude, j'ai tout vidé et comme d'habitude, je charge les données. Donc, j'en suis là, à cette première partie-là, où je vais charger le tableau. Je charge les données. Très bien. Donc, il affiche les données qui sont là. Et je vais isoler les individus et les variables actives. Rappelez-vous, c'est uniquement les joueurs avant les quatre fantastiques. Voilà. Très bien. Et on ne va pas prendre les classements de doubles. On prend unique\"), ('290022', \"rendre les classements de doubles. On prend uniquement les caractéristiques des joueurs en simple. Voilà. Donc, c'est ce que je fais ici. De nouveau, dans le tableau de données, on a exclu la dernière colonne qui concerne le double, et on n'a conservé que les joueurs avant les 4 fantastiques des années 2000-2010. Une fois que c'est ça, donc déjà là, alors, ici, `ColumnTransformers`, un outil de C-Kitler. Très bien, donc il est déjà prêt, il suffit de l'instancier. Ensuite, le `StandardScaler`, i\"), ('290023', \"t de l'instancier. Ensuite, le `StandardScaler`, il existe également, donc je n'ai plus qu'à l'utiliser en le paramétrant comme je veux. Un autre encodeur, il existe également. En revanche, le calcul de la pondération par la racine de l'inverse de la fréquence, ça, on ne l'a pas. Cette opération-là, on ne l'a pas. Ça, ce bleu-là, on ne l'a pas, cette partie-là. Donc, c'est à nous de la faire. Qu'est-ce qui se passe ? On peut le voir comme un centre réduction, sauf que on retranche 0 pour la moye\"), ('290024', \"re réduction, sauf que on retranche 0 pour la moyenne et au lieu de prendre l'écart type pour le scale, on prend la racine carrée de la fréquence. Qu'est-ce qui se passe alors ? Je vais utiliser une stratégie très simple, tout simplement je vais prendre donc l'outil `StandardScaler` de City Learn et je vais la surcharger. Voilà ce que je vais faire en fixant la moyenne à 0 donc, ne pas centrer. Et au lieu d'utiliser l'écart-type, je vais utiliser la racine carrée de la fréquence dans le scale. O\"), ('290025', \" la racine carrée de la fréquence dans le scale. On ne peut pas être un bon data scientist si on ne sait pas programmer. Et là, ça le montre bien. Si vous commencez à dire, ah ben non, il n'y a pas l'outil, donc je ne le fais pas. Je n'arrive pas à le faire avec l'outil, donc je ne le fais pas. Mais non, non, non, non, non, non. Si être data scientist, si être pressé bouton tout le monde peut le faire bien sûr qu'à un moment ou à un autre il va falloir aller mettre les mains dans le cambouis for\"), ('290026', \"alloir aller mettre les mains dans le cambouis forcément, forcément à un moment ou à un autre il faut mettre les mains dans le cambouis et c'est exactement ce qui se passe ici. Le `MyScaler` il n'existe pas et vous n'allez pas attendre je ne sais pas qui pour le faire. On peut le faire nous-mêmes. C'est juste un héritage de classe. Vous avez vu le code ? Il est super simple. Donc, qu'est-ce qui se passe ? J'ai créé ici une classe de `StandardScaler`. Ensuite, j'ai surchargé le constructeur. Et d\"), ('290027', \"er`. Ensuite, j'ai surchargé le constructeur. Et dans le constructeur, ce qui se passe, c'est que le centre, je le mets à false. Oui, parce qu'on ne retranche rien ici au numérateur. Donc, le centre, je le mets à false. On est d'accord ici. En revanche, je garde bien la réduction. Sauf que qu'est-ce qui se passe ? Une fois que j'ai fini le fit, je récupère la moyenne, parce qu'une moyenne de variable 0,1, c'est une fréquence. Vous êtes d'accord ? Une moyenne de variable 0,1, c'est une fréquence.\"), ('290028', \" Une moyenne de variable 0,1, c'est une fréquence. Donc, en tant que variance, je récupère la moyenne. Et en tant que paramètre scale, en tant que propriété scale de l'objet, je prends la racine carrée de cette variance-là. La variance étant la fréquence. Et on a bien cette formule-là, en bleu ici. Pas mal hein ? C'est largement notre portée. C'est largement notre portée. Bon, je n'ai pas osé le mettre à l'examen directement parce que je ne veux pas non plus déjà les étudiants sont stressés, ils\"), ('290029', \"pas non plus déjà les étudiants sont stressés, ils ont très peu de temps, il faut qu'ils assimilent déjà ce qu'on a fait ensemble ensuite ils doivent essayer de voir comment dépasser ce qu'on a fait ensemble et là, bon, je n'ai pas osé le mettre donc c'est pour ça que je fais quand même la vidéo là. Je suis en train de m'adresser aux étudiants du lundi matin que je vois jeudi pour l'occurrence en l'occurrence ici, voilà, donc du coup bon, j'ai regardé un peu mon sujet et je me suis dit, bon, ça \"), ('290030', \"gardé un peu mon sujet et je me suis dit, bon, ça quand même, c'est un peu, oui, je peux comprendre que ça soit des... En M2, je n'aurais pas hésité. En M2, je n'aurais pas hésité. Clairement, ça doit être à leur niveau. Bon, en M1, OK. Donc, ça, c'est une piste possible. Regardez bien le code qui est là. Alors, une fois que c'est fait, alors qu'est-ce qui se passe ? Je vais créer un sous-pipeline. Le sous-pipeline, c'est la partie bleue qui est là. Qu'est-ce qu'il y a dans ce sous-pipeline-là ?\"), ('290031', \"là. Qu'est-ce qu'il y a dans ce sous-pipeline-là ? Il y a d'abord le `OneHotEncoder` pour le codage 0,1, cette partie qui est là. Et une fois que je fais le `OneHotEncoder`, je vais appliquer ici le facteur de déflation. Donc, je crée un pipeline. Et dans ce pipeline-là, je mets le `OneHotEncoder`. C'est ce que je mets ici, que j'instancie. Vous avez bien vu, c'est l'objet ici que je suis en train de passer, qui s'appelle `encodage`. Et en tant que scaling, j'instancie ma classe, `MyScaling` que\"), ('290032', \"ue scaling, j'instancie ma classe, `MyScaling` que j'ai programmée au-dessus ici. Vous avez vu ? Alors, le `transform`, du coup, je n'ai pas besoin de le programmer. En fait, pourquoi je n'ai pas besoin de le programmer ? Parce qu'il va déflater avec le scale qui est là, qui est la racine carrée de la fréquence. Voilà. Donc, la seule chose que j'ai faite, c'est que dans le fit, je l'ai laissé faire ses calculs. Ensuite, j'ai récupéré la moyenne, qui est la fréquence quand on est calculée sur des\"), ('290033', \"qui est la fréquence quand on est calculée sur des variables 0,1. Et ensuite j'ai pris la racine carrée comme paramètre scale, comme propriété scale. Et je mets ça dans un pipeline. Du coup, mon pipeline global, c'est un pipeline de pipelines. On est d'accord là-dessus. Donc, le sous-pipeline, c'est cette partie bloquée. Bon, je peux l'entourer, je ne peux pas mettre des couleurs. Mais ce pré-pipeline, c'est un sous-pipeline, c'est un pipeline en fait, c'est un sous-pipeline qui concerne la part\"), ('290034', \" fait, c'est un sous-pipeline qui concerne la partie bleue qui est là. Bon, je n'ai pas de feutre ici, mais c'est bien la partie bleue qui est là. Donc, j'ai fait cette partie bleue-là. Maintenant, il faut que je dise à mon programme, quand le jeu de données va arriver, il faut que tu scindes en deux. D'une part, tu mets de côté les quantitatives, et d'autre part, tu mets de côté les qualitatives. Sur les quantitatives, on va appliquer le `StandardScaler`, et sur les qualitatives, on va applique\"), ('290035', \"rdScaler`, et sur les qualitatives, on va appliquer ce pipeline-là, qui est composé de deux instances de classe, qui est le `OneHotEncoder` et le `MyScaling`. Allons-y. Dans `PrepaFlow`, j'utilise le `ColumnTransformer`. Le `ColumnTransformer` me permet de définir des actions différenciées selon soit les listes des variables, soit les types des variables. Et moi, c'est ce que je fais ici. Je fais un `MacColumnSelector`, et dans le `MacColumnSelector`, je dis, tu mets de côté ceux qui ne sont pas\"), ('290036', \"tor`, je dis, tu mets de côté ceux qui ne sont pas des objets, donc les numériques en fait. Et sur ces variables numériques-là, tu appliques le `StandardScaler`. C'est la partie gauche qui est là, en vert. Et pour les qualitatives, cette fois-ci, on prend un `MacColumnSelector`, qui sont les objets cette fois-ci, donc les variables qualitatives. Qu'est-ce que tu vas faire ? Tu vas appliquer le pipeline qui est composé du `OneHotEncoder` et du `MyScaling`, qui est là. `PrepaQui`. Vous avez vu, c'\"), ('290037', \"Scaling`, qui est là. `PrepaQui`. Vous avez vu, c'est le `PrepaQui` qui est là. C'est génial. Non, mais franchement, voilà. Et si on lit la doc de `ColumnTransformer`, on voit bien. Ils sont transformés séparément, enfin, de manière séparée, séparément, voilà. Et ensuite, les variables générées, voilà, par chaque transformeur sera concaténée. Du coup, qu'est-ce qui se passe ? Ça, c'est mon `PrepaFlow`. Je vais rajouter dans mon pipeline, cette fois-ci, l'ACP. C'est la dernière partie qui est là,\"), ('290038', \"is-ci, l'ACP. C'est la dernière partie qui est là, pour avoir le pipeline complet. Le pipeline complet, il est composé. Il faut que je lance ça, ou j'ai oublié de le lancer. Très bien, il n'y a pas beaucoup de calculs à faire. Du coup, mon pipeline complet, tout entier qui est là, il est composé d'une part de la partie de préparation de données, avec deux chemins, c'est une sorte de graphe en fait. Ici la préparation de données `PrepaFlow`, vous avez vu, préparation `PrepaFlow`, avec deux chemin\"), ('290039', \"avez vu, préparation `PrepaFlow`, avec deux chemins via le `ColumnTransformer`, les quantitatifs d'un côté et les qualitatifs de l'autre. Et une fois que les données sont fusionnées, parce que c'est ce qui est fait automatiquement, ils les concatènent automatiquement, c'est ce que dit la documentation, je n'ai rien inventé, qu'est-ce qui se passe ? J'applique la CP. Avec deux facteurs, je vais faire dans le plan, tout simplement. Voilà. Donc, je crée mon pipeline. Donc, on a un pipeline de pipel\"), ('290040', \"crée mon pipeline. Donc, on a un pipeline de pipeline. C'est ça, l'idée de la faire. Une fois qu'on a ça, je peux faire le fit. Donc, je peux faire le fit sur les données originelles, ici. Le déactif, là, c'est la base en gris, ici. On est bien d'accord. Ce que je passe en entrée du fit, c'est la base. Donc, tous les éléments intermédiaires, c'est lui qui le fait. C'est le pipeline qui le met en place. Tous les éléments intermédiaires. Moi, la seule chose que j'ai à faire, c'est quand j'appelle \"), ('290041', \"ule chose que j'ai à faire, c'est quand j'appelle le fit, je passe les données originelles, non transformées, sans aucune préparation. Alors, je dis ça, je ne dis rien, mais une possibilité peut-être, c'est, je dirais, les données manquantes, de rajouter des imputaires par là. Là également, les imputaires ne sont pas les mêmes, selon qu'on est sur des variables qualitatives ou des variables quantitatives. Vous n'allez pas remplacer si vous avez des variables qualitatives vous n'allez pas remplac\"), ('290042', \"es variables qualitatives vous n'allez pas remplacer les données manquantes par la moyenne par exemple déjà remplacer par la moyenne on peut toujours discuter mais surtout pour une variable qualitative ça ne va pas le faire du tout, on est bien d'accord là dessus donc c'est une possibilité je dis ça, je ne dis rien mais c'est une possibilité de rajouter des gestions de données manquantes avec des imputaires chacun voit ce qu'il veut faire mais moi c'est une idée que je donne voilà donc très bien\"), ('290043', \"i c'est une idée que je donne voilà donc très bien Et voilà. C'est pas mal, hein ? Et vous voyez le schéma ? Voilà, c'est ça. C'est ce qui m'a inspiré pour faire ce petit graphique, en fait. J'ai regardé comment il prépare les choses ici. J'ai trouvé la présentation particulièrement pertinente. Et je me suis dit, je vais la refaire avec des icônes et des couleurs. Mais c'est bien ce qu'il fait ici, vous avez vu. Oh, je trouve ça génial. Je trouve ça vraiment génial. Une fois que c'est fait ça, a\"), ('290044', \" ça vraiment génial. Une fois que c'est fait ça, alors qu'est-ce qui se passe ? On va calculer les coordonnées factorielles des individus et faire le graphique. J'ai appliqué ici, voilà, j'ai découvert. Coordonnées factorielles, et vérifions maintenant dans `Scientist Tools`. Donc j'ai appliqué les calculs avec `Scientist Tools`, et ensuite je dois avoir les coordonnées factorielles. Voilà, voilà, voilà, voilà, alors je mets ça, puis ici, et on a bien les mêmes coordonnées factorielles, 0,55, 0,\"), ('290045', \" bien les mêmes coordonnées factorielles, 0,55, 0,55, voilà, 10,73, pour Boris Baker, 0,835. Alors on n'a pas les mêmes signes, mais encore une fois, on est en analyse factorielle, les signes ne sont pas importants, ce sont les positions relatives qui sont importantes. Parce que, oh, on n'a pas les mêmes signes, il y a une erreur. Non, non, non, non, non, non, non. C'est dû à l'algorithme de décomposition en valeur singulière, tout simplement. Ce qui est important, c'est les positions relatives \"), ('290046', \" qui est important, c'est les positions relatives dans le plan factoriel. S'il y a tel individu et proche de tel individu, avec un autre outil, on doit avoir les mêmes choses. Donc, tout le monde a vu, j'ai rajouté l'ACP dans mon pipeline, on est bien d'accord. On a bien les mêmes valeurs. Du coup, Une fois que j'ai appliqué le PCA, là, on est d'accord, préparation plus PCA, peut-être que je l'ai dit un peu vite. Alors, une fois que j'ai coordonné, du coup, je peux faire le graphique vectoriel, \"), ('290047', \"é, du coup, je peux faire le graphique vectoriel, donc j'ai fait ici avec mes individus, et on voit bien les différents joueurs, on voit bien les joueurs, d'ailleurs, de manière assez, là, on n'a que des revers à une main, là, par là, on est d'accord, là, des revers à deux mains, Vildender demain, Gassi demain, Borg, enfin, revers à deux mains, quoi. Alors, il y a Connors qui est un peu à part, mais Connors a toujours été un peu à part. Il n'a pas, ce n'est pas le Big Four comme on le dit ici, m\"), ('290048', \"s, ce n'est pas le Big Four comme on le dit ici, mais c'est un joueur particulièrement très différent, ça ne m'étonne pas. Déjà en termes de palmarès, pas sur les grands chelettes, mais en termes de palmarès global, il se démarque des autres. Donc, il est vraiment à part ici sur le premier facteur. Bon, je ne vais pas aller dans les commentaires. Moi, ce que je veux voir tout simplement, c'est que j'ai réussi, en utilisant un pipeline à reproduire les calculs de la FDM. Et pour bien montrer mon \"), ('290049', \"e les calculs de la FDM. Et pour bien montrer mon affaire, j'ai utilisé un logiciel qui fait référence, un package qui fait référence, c'est `Scientist Tools`. Et on voit bien qu'on a bien les mêmes résultats avec les mêmes proximités. Donc là, Connor c'est à droite, Connor c'est à gauche, mais on a bien les mêmes proximités. C'est ça qui est très important dans l'affaire. On est bien d'accord là-dessus. Alors, une fois que j'ai ça, qu'est-ce qui se passe ? Si je veux appliquer mon pipeline sur \"), ('290050', \" se passe ? Si je veux appliquer mon pipeline sur un individu supplémentaire, par exemple, je veux récupérer Federer. Federer fait partie des Big Four des années 2000-2010. Je vais le placer dans mon espace factoriel pour voir il est proche de qui. Voilà ces caractéristiques. Qu'est-ce qui se passe ? J'appelle juste le `transform` du pipeline. Vous avez vu ? Donc là, dans l'apprentissage, j'appelais juste le `FIT`, et c'est lui qui se charge d'appeler les `FIT`, `transform` les différentes étape\"), ('290051', \"peler les `FIT`, `transform` les différentes étapes qui sont là, à gauche et à droite, de réunir les bases, de faire la concaténation des bases, et d'appliquer l'ACP sur la base concaténée, la base rose qui est là. Et ensuite, quand j'ai un individu supplémentaire, j'appelle ici le `transform` tout simplement, et j'ai les coordonnées. Et c'est lui qui va charger de faire les différentes étapes préparation. Sinon, c'était à vous de le faire. Sinon, c'est à vous d'appliquer le `transform` de `Stan\"), ('290052', \", c'est à vous d'appliquer le `transform` de `StandardScaler`, de mettre de côté la base, la partie qualitative, d'appliquer le `transform`, de mettre de côté la quantitative à gauche, ensuite de mettre de côté la partie qualitative, de faire un `transform` de `OneHotEncoder`, de faire un `transform` de `MyScaling`, et ensuite le concaténation des bases et d'appeler, ensuite le `transform` de la CP, on ne s'en sort pas avec ça. C'est comme ça qu'on fait des erreurs. Et justement, le gros intérêt\"), ('290053', \"on fait des erreurs. Et justement, le gros intérêt du pipeline, c'est que c'est lui qui les fait successivement. Alors, on a une présentation bizarre ici en fait. En réalité, on a un vecteur de vecteurs. On a un vecteur de vecteurs là. Très bien. Donc, pour accéder aux valeurs, il faut accéder à 0 d'abord, pour accéder à cette partie-là, et ensuite 0, 1, 0, 0 et 0, 1 est là. Alors, une fois que j'ai les coordonnées, du coup, je place Federer dans l'espace factoriel et les gens particuliers, les \"), ('290054', \" l'espace factoriel et les gens particuliers, les personnes particulières, les tennis-mans particuliers sont proches les uns des autres. Federer, proche de Conor, ça ne m'étonne pas du tout. C'est deux êtres exceptionnels. Ils sont ensemble. Qui se ressemblent, s'assemblent, on va dire. Alors, bon, le gros intérêt ici, c'est que vous avez vu, j'ai fait appel à un `transform` du pipeline. Et c'est le pipeline qui s'occupe de scinder les bases, les quantités d'un côté et les qualités de l'autre, f\"), ('290055', \" quantités d'un côté et les qualités de l'autre, faire les différents `transform` à chaque étape, et ensuite de réunir les bases et de faire le `transform` de l'ACP pour obtenir les coordonnées factorielles. C'est ça qui est génial dans l'outil pipeline. En termes de déploiement, du coup, vous n'avez qu'à faire que `transform`. C'est ça l'idée. Vous n'avez pas besoin de faire des opérations complexes la seule opération que vous avez à faire quand vous déployez votre modèle c'est de faire `transf\"), ('290056', \" vous déployez votre modèle c'est de faire `transform`. En déploiement, c'est quelque chose qu'on aura à faire j'imagine je dis ça je dis rien alors une fois que c'est bon là justement mon pipeline je vais le sauvegarder le pipeline je vais le sauvegarder donc toute la partie en pointillé en tant que structure prédictive si vous voulez je vais la sauvegarder. Alors, j'aurais pu utiliser Pickle, mais en réalité, il y a un outil qui est plus souple, et qui marche mieux, finalement, c'est Deal, qui\"), ('290057', \", et qui marche mieux, finalement, c'est Deal, qui est là. Donc moi, qu'est-ce que je fais ? J'ai utilisé Deal, tout simplement. En fait, vous avez vu, Deal, c'est une extension de Pickle. Elle marche très bien. Et surtout, j'ai remarqué que quand je fais des transformations à la volée, et quand je ne passe pas par des classes de CKL pour faire des transformations, quand je fais des transformations à la volée, lui fonctionne. alors que Pickle ne marche pas. Le paquet de Pickle ne marche pas quan\"), ('290058', \"marche pas. Le paquet de Pickle ne marche pas quand on a des transformations à la volée. Donc, moi, ça me paraît plus pertinent d'utiliser Deal, pour le coup. C'est pour ça que, dans mon exemple ici, j'utilise Deal. Bon, je n'ai pas de fonction de transformation à la volée, c'est surcharger la classe pour avoir `MyScaling`. Mais bon, c'est des choses qu'il faut regarder. Alors, qu'est-ce qui se passe ? Voilà, donc j'importe le paquet qui est déjà installé par défaut. Ensuite, j'ouvre le fichier \"), ('290059', \" installé par défaut. Ensuite, j'ouvre le fichier en sortie, en écriture binaire, W c'est write, B c'est binary, et je vais sauvegarder dans ce fichier-là, `workflow.sav`. Qu'est-ce que je dump dans le fichier ? C'est mon pipeline, voilà, le pipeline entier qui est là, que je vais pouvoir utiliser pour le déploiement. Alors, une fois que j'ai dumpé, je ferme le fichier, bien sûr, sinon il y en aura des soucis. Alors, allons voir sur le disque, il est là, vous avez vu, 27.03, il est 10h03. 10h03,\"), ('290060', \" est là, vous avez vu, 27.03, il est 10h03. 10h03, 27 mars 2024. On est bien là. C'est bien vu. Et voilà. Et ce qu'il reste à voir alors, si on veut faire du déploiement, si on veut faire du déploiement et appliquer ce calcul pour un individu supplémentaire. Par exemple, je vais me charger de Djokovic. Alors déjà, il faut que je charge le modèle qui a été piquelisé avec Deal, qui a été sauvegardé en binaire. Je refais ici l'importation, mais je change le nom. Au lieu d'appeler `WKF` pour `workfl\"), ('290061', \"hange le nom. Au lieu d'appeler `WKF` pour `workflow`, je l'ai appelé `modèle`, pour bien voir que c'est un autre objet. Donc, c'est ce qu'on doit utiliser. Par exemple, si on écrit un programme à part pour faire le déploiement, on est en train de charger un nouvel objet, on est bien d'accord là-dessus. Ce que vous avez ici en amont, dans mon notebook, normalement, on n'a pas dans le programme de déploiement. On ne doit pas l'avoir dans le programme de déploiement. On ne doit pas l'avoir dans le\"), ('290062', \"mme de déploiement. On ne doit pas l'avoir dans le programme de déploiement. J'espère m'être bien entendu. J'espère qu'on m'a bien entendu, là, pour le coup. Très bien. Donc, moi, je félicite d'une thread. Pourquoi ? Parce que c'est plus facile à présenter. Mais, donc, je charge le workflow, le pipeline, dans un nouvel objet que j'appelle le modèle, qui n'est pas le même nom, du coup. Et j'affiche les différentes étapes. Voilà, `step`. Voilà, très bien, donc il m'a dit tu as fait un `ColumnTrans\"), ('290063', \"s bien, donc il m'a dit tu as fait un `ColumnTransformer` d'abord avec les quantitatives, tu as fait le `StandardScaler` et pour les qualitatives, tu as fait un pipeline avec donc un premier élément, une première étape c'est l'encodage avec le `OneHotEncoder` et une remise à l'échelle avec une classe `MyScaling` avec le `with mean` à la false que j'avais spécifié quand j'ai fait l'héritage très bien une fois que ça s'est fait Et voilà, donc la deuxième étape, donc la première étape, c'est prépar\"), ('290064', \"uxième étape, donc la première étape, c'est préparation avec ces deux aspects-là. L'étape suivante, c'est l'ACP, sur les données qui sont issues de la transformation. Alors, pour certaines personnes, j'imagine qu'ils auront ajouté un imputaire, peut-être, je ne sais pas, mais sûrement aussi, après l'ACP, ils vont remettre un algorithme de clustering, je ne sais pas, peut-être. Là, ici, après les coordonnées factorielles des individus, peut-être qu'ici, là, en dessous, là, en dessous, là, il va y\"), ('290065', \"u'ici, là, en dessous, là, en dessous, là, il va y avoir un clustering, genre `KMeans`, par exemple. Je ne sais pas, peut-être. Très bien. Bon, moi, en l'occurrence, j'ai chargé le modèle. Une fois que j'ai chargé le modèle, je vais mettre de côté, je vais isoler cette fois-ci Djokovic, voilà, et j'applique modèle que je viens de charger. Ça s'appelle le modèle, le modèle que je viens de charger. donc c'est un nouvel objet en mémoire. On est d'accord ? Ce nouvel modèle là, je la clique sur `Joko\"), ('290066', \"cord ? Ce nouvel modèle là, je la clique sur `Joko`. J'ai les coordonnées, et du coup, je peux faire les graphiques. Et on a bien cette fatigue. Revers à demain, Djokovic est proche de Agassi-Borg. Voilà, par là, ici. Encore une fois, je prends notre outil de référence, on voit bien. Connors, enfin Federer est proche de Connors. Voilà, ce sont les proximités qui comptent. Donc le fait que ce soit à gauche ici, à droite ici, ce n'est pas un problème du tout. Le signe arbitraire. En revanche, les \"), ('290067', \"me du tout. Le signe arbitraire. En revanche, les proximités sont très importantes. et on voit bien la proximité Federer-Connors, et on voit bien la proximité de Djokovic avec Agassi-Borg. Alors, pour conclure, nous devons maîtriser, là je parle à mes étudiants pour le coup, nous devons maîtriser le mécanisme des pipelines. En tout cas, c'est quelque chose de très important à connaître dans le processus machine learning. Alors souvent, les pipelines, c'est des séquences de traitement très simple\"), ('290068', \"nes, c'est des séquences de traitement très simples, qu'on met à la queue de l'oeuf, mais dans certains cas, on est amené à faire des actions différenciées selon les variables, selon les types de variables. Et on a un outil pour ça dans C-Kitler, c'est `ColumnTransformer`. Et pour bien montrer ce mécanisme-là, j'ai essayé de reproduire les calculs de la liste facturée des données mixtes, la FDM, en utilisant un pipeline, qui, d'une part, fait une action différenciée sur les variables quantitativ\"), ('290069', \" action différenciée sur les variables quantitatives avec un `StandardScaler` et pour les variables qualitatives, fait une double action, un `OneHotEncoder` et une remise à l'échelle spécifique. Une fois qu'on a fait ça, il reconquatène les deux bases transformées et on peut appliquer l'ACP sur cette base réunie. Et on a bien les résultats de l'analyse facturée des données mixtes. Pipeline, `ColumnTransformer`. Voilà, excellent travail à tous.\")]\n"
     ]
    }
   ],
   "source": [
    "chunks_with_ids = generate_chunk_ids(results)\n",
    "print(chunks_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarde CSV\n",
    "import csv\n",
    "\n",
    "def save_chunks_to_csv(chunks_with_ids: list[tuple[str, str]], filename: str):\n",
    "    \"\"\"Sauvegarde les chunks avec leurs IDs dans un fichier CSV\"\"\"\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Chunk ID\", \"Chunk Text\"])\n",
    "        # Écrire les données\n",
    "        for chunk_id, chunk_text in chunks_with_ids:\n",
    "            writer.writerow([chunk_id, chunk_text])\n",
    "\n",
    "save_chunks_to_csv(chunks_with_ids, 'chunks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings des chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "api_key = \"ilmf54nAP0qBHRhICxyqyC5UkrOWH8px\"\n",
    "\n",
    "# Initialiser le client Mistral\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def generate_embedding(text : str):\n",
    "    \"\"\"Génère un embedding\"\"\"\n",
    "    # Spécifier le modèle d'embedding\n",
    "    model = \"mistral-embed\"\n",
    "\n",
    "    # Appeler l'API pour générer les embeddings\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        inputs=[text]\n",
    "    )\n",
    "\n",
    "    # Extraire les embeddings de la réponse\n",
    "    prompt_embedding = response.data[0].embedding\n",
    "    return prompt_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.030181884765625, 0.037109375, 0.043853759765625, 0.022491455078125, 0.01277923583984375, 0.0179443359375, 0.00940704345703125, -0.009368896484375, -0.0079193115234375, -0.016815185546875, -0.0289459228515625, 0.042572021484375, -0.029571533203125, 0.0138092041015625, -0.059112548828125, 0.05853271484375, 0.0017900466918945312, 0.01495361328125, 0.0243377685546875, 0.0272979736328125, -0.021759033203125, -0.01209259033203125, -0.0662841796875, -0.0281829833984375, 0.0032634735107421875, -0.00801849365234375, -0.00447845458984375, -0.06390380859375, -0.027801513671875, -0.006488800048828125, -0.01087188720703125, -0.016387939453125, -0.00811767578125, -0.026885986328125, -0.0010395050048828125, 0.006534576416015625, -0.01107025146484375, -0.01629638671875, -0.004547119140625, 0.0341796875, 0.018524169921875, -0.01436614990234375, -0.0200042724609375, -0.0075225830078125, -0.0209808349609375, -0.055206298828125, 0.021697998046875, 0.007549285888671875, 0.031463623046875, -0.03424072265625, 0.038482666015625, 0.049102783203125, 0.0121917724609375, -0.044464111328125, -0.005741119384765625, 0.0301361083984375, 0.03277587890625, 0.0251312255859375, -0.0380859375, 0.03717041015625, -0.042449951171875, -0.002086639404296875, 0.01000213623046875, -0.0026988983154296875, 0.03961181640625, 0.027923583984375, -0.003322601318359375, -0.052581787109375, -0.005550384521484375, -4.07099723815918e-05, 0.0014562606811523438, 0.006988525390625, -0.0058135986328125, 0.0113677978515625, -0.036376953125, -0.03936767578125, -0.005229949951171875, 0.01491546630859375, 0.024627685546875, 0.0195159912109375, -0.06396484375, -0.0074005126953125, 0.05084228515625, -0.03131103515625, -0.0102691650390625, 0.0110931396484375, 0.018585205078125, 0.0125579833984375, -0.0052032470703125, 0.018341064453125, 0.04669189453125, -0.006549835205078125, 0.026947021484375, -0.0130767822265625, 0.0218353271484375, 0.06304931640625, -0.0194091796875, -0.0012292861938476562, -0.01456451416015625, 0.019744873046875, 0.01151275634765625, -0.0222930908203125, 0.0701904296875, 0.01369476318359375, 0.033447265625, -0.0260162353515625, 0.0670166015625, -0.002719879150390625, -0.007457733154296875, -0.038787841796875, -0.1094970703125, -0.0323486328125, -0.005878448486328125, -0.06805419921875, -0.00957489013671875, -0.0020465850830078125, -0.01180267333984375, 0.044586181640625, -0.0633544921875, -0.031951904296875, 0.0030612945556640625, 0.009735107421875, -0.0203857421875, 0.021270751953125, -0.04876708984375, -0.0200653076171875, 0.02587890625, 0.034820556640625, -0.0634765625, -0.023773193359375, -0.006011962890625, -0.0277099609375, -0.00107574462890625, -0.059844970703125, 0.020477294921875, -0.021759033203125, 0.0162811279296875, -0.017669677734375, -0.0016126632690429688, -0.0033245086669921875, 0.0192413330078125, -0.05645751953125, -0.04315185546875, -0.0066375732421875, -0.03668212890625, -0.00958251953125, -0.0280914306640625, 0.01042938232421875, -0.029266357421875, -0.0023708343505859375, 0.00823974609375, 0.0233306884765625, 0.04107666015625, 0.0184326171875, -0.0276031494140625, -0.0243377685546875, -0.025299072265625, -0.01934814453125, 0.0250244140625, -0.0097808837890625, -0.010345458984375, 0.0244598388671875, 0.02752685546875, 0.01265716552734375, -0.010101318359375, -0.048583984375, -0.0007600784301757812, 0.01016998291015625, -0.037994384765625, 0.005706787109375, 0.041473388671875, -0.040557861328125, 0.0311126708984375, 0.0543212890625, 0.01096343994140625, 0.021270751953125, 0.0203857421875, -0.01007080078125, 0.021820068359375, 0.0207061767578125, 0.007648468017578125, -0.035888671875, 0.0116119384765625, -0.0028743743896484375, 0.008544921875, -0.018585205078125, -0.027252197265625, 0.004062652587890625, -0.018524169921875, 0.020721435546875, -0.0298614501953125, -0.0261993408203125, -0.00461578369140625, 0.0268096923828125, -0.0066375732421875, 0.0284576416015625, 0.055206298828125, 0.01174163818359375, 0.0236968994140625, 0.0188751220703125, 0.007434844970703125, 0.027435302734375, -0.018585205078125, 0.037445068359375, 0.0267791748046875, 0.0251617431640625, -0.031280517578125, 0.048126220703125, 0.0196990966796875, -0.0321044921875, -0.00473785400390625, 0.028228759765625, -0.00859832763671875, -0.004184722900390625, -0.031341552734375, -0.0345458984375, 0.0282745361328125, -0.0223236083984375, -0.0300445556640625, 0.0013227462768554688, 0.00923919677734375, -0.05682373046875, 0.0010156631469726562, 0.0142059326171875, -0.00624847412109375, 0.042755126953125, 0.0009055137634277344, 0.0183258056640625, 0.01485443115234375, 0.002651214599609375, -0.0172576904296875, 0.04107666015625, 0.006244659423828125, -0.0012674331665039062, 0.0040283203125, -0.039794921875, 0.051849365234375, 0.0164031982421875, -0.018646240234375, -0.01824951171875, -0.015716552734375, 0.0178985595703125, -0.006591796875, 0.0235748291015625, 0.0260009765625, -0.032470703125, 0.0024318695068359375, -0.00244140625, 0.03851318359375, 0.04486083984375, -0.06787109375, -0.015899658203125, -0.0234527587890625, -0.016815185546875, -0.05364990234375, 0.0008368492126464844, 0.0035858154296875, -0.06097412109375, 0.034942626953125, -0.039581298828125, 0.0062255859375, 6.973743438720703e-06, -0.0280609130859375, -0.029296875, 0.0231170654296875, 0.01324462890625, -0.06396484375, 0.035400390625, 0.047576904296875, -0.0302276611328125, -0.0174560546875, -0.0289306640625, 0.037200927734375, 0.01251220703125, 0.0297088623046875, -0.03631591796875, 0.01500701904296875, 0.059814453125, -0.0016918182373046875, 0.0012845993041992188, 0.0300445556640625, 0.00730133056640625, 0.011962890625, 0.045928955078125, -0.01149749755859375, -0.01006317138671875, 0.02410888671875, -0.006134033203125, 0.02734375, -0.08331298828125, -0.00919342041015625, 0.056640625, 0.0211639404296875, 0.01262664794921875, 0.00493621826171875, -0.0372314453125, -0.0015897750854492188, 0.02667236328125, 0.017059326171875, 0.0242462158203125, 0.0217132568359375, -0.031402587890625, -0.035736083984375, 0.00730133056640625, 0.0170135498046875, -0.0335693359375, -0.0222320556640625, -0.0129241943359375, -0.003040313720703125, -0.0572509765625, -0.0174560546875, -0.0105743408203125, -0.024200439453125, 0.00012791156768798828, -0.01070404052734375, -0.0230560302734375, -0.035675048828125, -0.0045928955078125, 0.01354217529296875, -0.032989501953125, -0.04644775390625, -0.032562255859375, 0.0633544921875, -0.0262298583984375, -0.01438140869140625, -0.0265045166015625, -0.01168060302734375, -0.0284881591796875, 0.01448822021484375, 0.0028781890869140625, 0.0279083251953125, -0.0178375244140625, 0.059814453125, 0.0284576416015625, 0.019012451171875, 0.016357421875, 0.0821533203125, 0.05084228515625, -0.0204925537109375, 0.0194549560546875, -0.0194244384765625, -0.0236053466796875, 0.040069580078125, 0.007404327392578125, -0.0279693603515625, 0.0008778572082519531, -0.0308837890625, -0.03814697265625, 0.0599365234375, 0.007171630859375, 0.03436279296875, 0.00695037841796875, -0.0167236328125, 0.072265625, -0.0084228515625, 0.0946044921875, -0.0264129638671875, -0.021881103515625, 0.020294189453125, 0.0110931396484375, -0.004947662353515625, 0.004535675048828125, 0.035308837890625, 0.024383544921875, -0.0169525146484375, 0.05059814453125, -0.024200439453125, 0.043548583984375, -0.00868988037109375, -0.0289459228515625, 0.0039825439453125, 0.05511474609375, 0.0304412841796875, 0.0638427734375, 0.0306854248046875, 0.00173187255859375, 0.0028057098388671875, -0.0303497314453125, 0.003505706787109375, -0.013519287109375, -0.087158203125, 0.0283660888671875, -0.007656097412109375, 0.024444580078125, 0.0175018310546875, 0.0003094673156738281, -0.036712646484375, 0.0217132568359375, -0.034332275390625, 0.07464599609375, 0.060089111328125, 0.03570556640625, 0.01444244384765625, -0.00795745849609375, -0.00664520263671875, 0.0484619140625, 0.0213165283203125, -0.0019989013671875, 0.0220947265625, 0.00940704345703125, 0.03424072265625, -0.0294342041015625, 0.052459716796875, 0.0136871337890625, 0.0236663818359375, 0.01629638671875, 0.002185821533203125, 0.0258331298828125, 0.0305328369140625, -0.010406494140625, 0.03717041015625, 0.0009069442749023438, -0.036956787109375, 0.040252685546875, -0.01519775390625, 0.0189666748046875, -0.047119140625, 0.0179443359375, -0.01282501220703125, 0.015899658203125, 0.01371002197265625, 0.026153564453125, -0.032958984375, -0.0091094970703125, 0.036834716796875, -0.05914306640625, 0.00963592529296875, 0.01702880859375, 0.0087738037109375, 0.07952880859375, -0.06640625, -0.049530029296875, 0.03314208984375, -0.052642822265625, -0.037384033203125, 0.03851318359375, 0.0196685791015625, -0.045074462890625, -0.0279083251953125, -0.036376953125, 0.006725311279296875, -0.0245513916015625, -0.01316070556640625, 0.006549835205078125, -0.0308990478515625, 0.0201416015625, 0.025604248046875, 0.04437255859375, 0.0182037353515625, -0.01153564453125, 0.0208282470703125, -0.0031795501708984375, -0.004428863525390625, 0.0226898193359375, -0.0018310546875, 0.0264129638671875, -0.005035400390625, -0.028839111328125, -0.00415802001953125, -0.00392913818359375, 0.0264892578125, 0.021453857421875, -0.04583740234375, -0.03961181640625, -0.05645751953125, 0.047607421875, 0.00826263427734375, 0.025848388671875, -0.0036067962646484375, -0.04296875, 0.016845703125, -0.05621337890625, -0.031524658203125, -0.0165863037109375, 0.0034008026123046875, -0.028350830078125, 0.052825927734375, 0.041748046875, -0.01428985595703125, -0.01446533203125, 0.022613525390625, -0.00014674663543701172, 0.0310821533203125, -0.041595458984375, 0.05999755859375, 0.038543701171875, -0.08148193359375, 0.01238250732421875, -0.0110321044921875, -0.02154541015625, 0.053375244140625, -0.052734375, 0.00946807861328125, 0.001628875732421875, -0.053070068359375, 0.01194000244140625, -0.014251708984375, -0.0168609619140625, -0.02490234375, 0.0193634033203125, 0.02032470703125, -0.00756072998046875, -0.003795623779296875, 0.019287109375, 0.003475189208984375, 0.0096282958984375, -0.0180206298828125, -0.0274658203125, 0.050506591796875, -0.06463623046875, -0.06427001953125, -0.0301971435546875, 0.032318115234375, -0.0136871337890625, 0.00472259521484375, -0.0209503173828125, -0.046783447265625, 0.0267181396484375, -0.03253173828125, 0.016632080078125, 0.01107025146484375, -0.0322265625, -0.01363372802734375, -0.03594970703125, 0.00018024444580078125, 0.0167083740234375, 0.009490966796875, 0.0299530029296875, 0.005615234375, -0.06072998046875, 0.0227813720703125, 0.0290679931640625, -0.004154205322265625, -0.000843048095703125, -0.0066375732421875, -0.0091705322265625, 0.005779266357421875, -0.0017786026000976562, -0.060028076171875, -0.026336669921875, -0.0953369140625, 0.00995635986328125, -0.0012407302856445312, -0.0240936279296875, -0.019195556640625, -0.04248046875, -0.06854248046875, -0.00939178466796875, 0.035400390625, 0.01187896728515625, 0.022735595703125, -0.03839111328125, 0.0013370513916015625, -0.032012939453125, -0.0266265869140625, 0.03863525390625, -0.01351165771484375, 0.006999969482421875, -0.0191192626953125, 0.007534027099609375, 0.01007843017578125, -0.005767822265625, 0.04278564453125, -0.00875091552734375, 0.037841796875, -0.060272216796875, -0.01345062255859375, -0.0186309814453125, -0.022674560546875, -0.029083251953125, -0.041259765625, 0.052032470703125, -0.0263671875, -0.048919677734375, 0.0136260986328125, 0.007778167724609375, 0.0194244384765625, -0.013763427734375, 0.0003814697265625, 0.0212860107421875, 0.032623291015625, 0.0550537109375, -0.04193115234375, -0.072998046875, 0.047607421875, -0.00637054443359375, 0.027313232421875, 0.028411865234375, -0.002285003662109375, -0.0101470947265625, 0.0274200439453125, 0.040618896484375, -0.0240631103515625, -0.0501708984375, -0.03314208984375, 0.004360198974609375, 0.0248260498046875, 0.07073974609375, -0.00910186767578125, 0.0595703125, 0.015716552734375, -0.002391815185546875, 0.00641632080078125, 0.07135009765625, 0.03240966796875, -0.04864501953125, -0.031494140625, -0.02227783203125, 0.0171356201171875, -0.05462646484375, -0.033843994140625, 0.036163330078125, 0.020751953125, -0.05316162109375, -0.0114898681640625, 0.0151824951171875, 0.0160675048828125, 0.0263214111328125, 0.007434844970703125, -0.0012292861938476562, 0.05230712890625, 0.00164794921875, -0.04052734375, -0.01000213623046875, 0.0114898681640625, 0.027252197265625, 0.052459716796875, -0.0002034902572631836, -0.0230712890625, 0.01165008544921875, 0.0154266357421875, 0.038421630859375, -0.00925445556640625, 0.042510986328125, 0.042755126953125, 0.023223876953125, 0.0108489990234375, 0.0433349609375, -0.0208892822265625, -0.02752685546875, 0.0014934539794921875, 0.042022705078125, 0.0040283203125, 0.05096435546875, -0.0253143310546875, 0.0189208984375, -0.0041961669921875, -0.049468994140625, 0.0030460357666015625, 0.0243682861328125, -0.01346588134765625, -0.045562744140625, -0.0408935546875, 0.06732177734375, -0.00920867919921875, 0.022979736328125, 0.01325225830078125, -0.01554107666015625, 0.003307342529296875, 0.0556640625, -0.0206298828125, -0.06707763671875, 0.0009164810180664062, -0.0221405029296875, -0.04290771484375, -0.0080413818359375, -0.04400634765625, 0.033721923828125, -0.04071044921875, -0.05169677734375, -0.0290374755859375, 0.01540374755859375, -0.0036163330078125, -0.018310546875, -0.03924560546875, -0.01177978515625, -0.0245513916015625, -0.0131988525390625, -0.057281494140625, 0.0096893310546875, 0.0272979736328125, -0.00994873046875, -0.006847381591796875, -0.0028514862060546875, -0.0088043212890625, -0.004459381103515625, 0.0157623291015625, -0.042633056640625, 0.00948333740234375, -0.0081024169921875, 0.043212890625, -0.01154327392578125, 0.03607177734375, 0.041259765625, 0.0299530029296875, -0.0060577392578125, -0.015716552734375, 0.027252197265625, -0.049285888671875, 0.0158538818359375, 0.00563812255859375, -0.0004534721374511719, -0.0254364013671875, -0.0011453628540039062, 0.0611572265625, -0.06195068359375, 0.00925445556640625, -0.0201416015625, 0.029022216796875, -0.046539306640625, 0.03656005859375, 0.005138397216796875, 0.006916046142578125, 0.0621337890625, 0.005290985107421875, -0.01202392578125, -0.036224365234375, -0.045379638671875, 0.06500244140625, -0.01126861572265625, 0.014068603515625, -0.05364990234375, -0.0170135498046875, 0.010589599609375, -0.01451873779296875, 0.00386810302734375, -0.07635498046875, -0.0100555419921875, 0.023223876953125, 0.025848388671875, -0.0090484619140625, 0.01203155517578125, -0.007171630859375, -0.0075225830078125, -0.0413818359375, 0.011138916015625, 0.0220489501953125, 0.007259368896484375, -0.045257568359375, 0.00905609130859375, -0.033721923828125, -0.05572509765625, -0.01812744140625, -0.041778564453125, 0.0298614501953125, -0.005859375, -0.04925537109375, 0.0277862548828125, -0.01448822021484375, 0.0142974853515625, 0.0228729248046875, 0.0635986328125, -0.0021820068359375, -0.0123138427734375, -0.0026035308837890625, 0.0413818359375, 0.0153350830078125, 0.0158233642578125, 0.001903533935546875, 0.0019283294677734375, -0.00902557373046875, -0.01325225830078125, 0.047210693359375, 0.045440673828125, 0.005802154541015625, -0.028289794921875, -0.0010728836059570312, -0.06256103515625, 0.049530029296875, 0.0015459060668945312, 0.041259765625, -0.03179931640625, -0.023895263671875, -0.09393310546875, 0.04254150390625, -0.0144500732421875, 0.040283203125, -0.0219268798828125, -0.02313232421875, -0.0657958984375, -0.037567138671875, -0.0474853515625, 0.0215606689453125, -0.0023097991943359375, 0.04400634765625, -0.003673553466796875, -0.017547607421875, 0.0159759521484375, 0.01541900634765625, 0.005802154541015625, 0.04986572265625, 0.029205322265625, -0.04974365234375, 0.043243408203125, -0.0216522216796875, -0.0027313232421875, -0.00936126708984375, -0.002292633056640625, -0.01497650146484375, -0.005962371826171875, -0.057952880859375, -0.0333251953125, 0.03948974609375, 0.0025119781494140625, -0.01306915283203125, 0.0220489501953125, -0.0164642333984375, -0.0064697265625, -0.0240478515625, -0.001941680908203125, 0.01456451416015625, -0.006488800048828125, -0.0015058517456054688, -0.00013554096221923828, -0.04901123046875, -0.08233642578125, 0.016937255859375, -0.031494140625, -0.033966064453125, -0.0255126953125, -0.0216217041015625, 0.01404571533203125, -0.0302276611328125, 0.04351806640625, -0.0015401840209960938, -0.01209259033203125, -0.006511688232421875, -0.01335906982421875, 0.03466796875, 0.034332275390625, -0.024810791015625, -0.048126220703125, 0.0374755859375, 0.0482177734375, -0.0297088623046875, 0.028900146484375, 0.0224456787109375, -0.0229644775390625, 0.007049560546875, -0.0523681640625, -0.01412200927734375, 0.02862548828125, -0.00548553466796875, 0.0036220550537109375, -0.022247314453125, 0.0157928466796875, 0.00260162353515625, -0.04962158203125, 0.0048065185546875, -0.007511138916015625, -0.04864501953125, 0.0162811279296875, -0.01202392578125, 0.0083160400390625, -0.004940032958984375, 0.031890869140625, 0.04193115234375, 0.07574462890625, 0.019927978515625, -0.015777587890625, 0.0123443603515625, 0.0092620849609375, 0.03375244140625, 0.00991058349609375, 0.0292510986328125, 0.007701873779296875, -0.01629638671875, -0.00939178466796875, -0.037994384765625, 0.0189208984375, -0.04779052734375, -0.04742431640625, -0.03558349609375, 0.06585693359375, 0.040435791015625, 0.00487518310546875, 0.056976318359375, 0.053375244140625, 0.040496826171875, 0.0121612548828125, 0.0002536773681640625, -0.0611572265625, -0.01031494140625, 0.040618896484375, 0.0145111083984375, -0.0501708984375, 0.05609130859375, 0.0850830078125, -0.05682373046875, -0.030242919921875, 0.0006046295166015625, 0.0007791519165039062, 0.033050537109375, 0.050048828125, 0.05474853515625, -0.05096435546875, 0.0277862548828125, 0.02880859375, 0.003017425537109375, -0.01378631591796875, -0.0055389404296875, -0.0033740997314453125, 0.0123443603515625, -0.0303497314453125, 0.00632476806640625, -0.006816864013671875, 0.05621337890625, -0.0119781494140625, -0.0919189453125, 0.039154052734375, 0.03656005859375, -0.004016876220703125, -0.020782470703125, -0.01242828369140625, 0.007709503173828125, -0.049468994140625, 0.0298614501953125, 0.007289886474609375, -0.0151824951171875, 0.03375244140625, 0.0026645660400390625, -0.043975830078125, 0.0007119178771972656, 0.0078277587890625, -0.0126495361328125, -0.00885009765625, -0.033050537109375, 0.0015926361083984375, -0.02606201171875, 0.037353515625, 0.0443115234375, 0.0182952880859375, -0.0116119384765625, -0.0187225341796875, -0.0027523040771484375, 0.01387786865234375, 0.004772186279296875, 0.05706787109375, 0.041015625, -0.0005578994750976562, 0.002651214599609375, 0.0122222900390625, 0.007625579833984375, 0.006137847900390625, 0.006500244140625, -0.06060791015625, 0.02490234375, -0.06884765625, 0.002994537353515625, -0.0033397674560546875, -0.01727294921875, -0.0104217529296875, -0.0245513916015625, 0.0166778564453125, -0.015777587890625, -0.00812530517578125, -0.036834716796875, 0.03778076171875, -0.035675048828125, -0.00984954833984375, -0.0027790069580078125, -0.017974853515625, 0.020965576171875, -0.057891845703125, 0.057159423828125, 0.0092010498046875, 0.005001068115234375, -0.00286865234375, 0.0280914306640625, 0.0458984375, -0.0176849365234375, 0.02099609375, 0.004917144775390625, 0.040496826171875, 0.02459716796875, 0.0309295654296875, -0.00543975830078125, -0.023590087890625, 0.006580352783203125, -0.033782958984375, -0.0738525390625, 0.049346923828125, 0.00647735595703125, 0.0232086181640625, 0.0018568038940429688, -0.035064697265625, -0.016448974609375, -0.04644775390625, 0.06884765625, 0.038482666015625, -0.046142578125, 0.013519287109375, -0.036224365234375, 0.042999267578125, -0.0196685791015625, 0.050140380859375, -0.03485107421875, 0.032989501953125, 0.0240478515625, -0.00608062744140625, -0.01471710205078125, 0.0164031982421875, -0.018707275390625, 0.02880859375, -0.0186920166015625, 0.03662109375, 0.0100555419921875, -0.0065155029296875, -0.04022216796875, -0.0177764892578125, 0.0091705322265625, -0.0019063949584960938, 0.0224609375, -0.017791748046875, -0.0244598388671875, 0.0271759033203125, 0.02587890625, 0.035858154296875, -0.00739288330078125, 0.01151275634765625, -0.00191497802734375, -0.00319671630859375, -0.0122528076171875, 0.0016536712646484375], [-0.025665283203125, 0.0487060546875, 0.035430908203125, 0.031829833984375, 0.029083251953125, 0.02587890625, 0.038360595703125, -0.01096343994140625, 0.0087738037109375, 0.003513336181640625, -0.00392913818359375, 0.0550537109375, -0.0024166107177734375, 0.01126861572265625, -0.0640869140625, 0.051666259765625, 0.007511138916015625, -0.0022487640380859375, 0.0288543701171875, -0.0050811767578125, -0.0214080810546875, 0.00168609619140625, -0.03277587890625, 0.00885009765625, 0.00524139404296875, -0.00583648681640625, 0.01074981689453125, -0.06610107421875, -0.039337158203125, -0.0028839111328125, -0.003246307373046875, -0.0186920166015625, -0.01537322998046875, 0.007171630859375, -0.01035308837890625, -0.0009484291076660156, -0.0218048095703125, -0.036041259765625, -0.010467529296875, -0.005184173583984375, -0.01953125, -0.00611114501953125, 0.00971221923828125, -0.01898193359375, -0.01105499267578125, -0.0335693359375, 0.0237884521484375, -0.01197052001953125, 0.00762939453125, -0.034454345703125, 0.050323486328125, 0.027008056640625, 0.0052032470703125, -0.0285797119140625, -0.0240325927734375, 0.023040771484375, 0.049713134765625, -0.01430511474609375, -0.052886962890625, 0.03277587890625, -0.0357666015625, -0.0205078125, -0.0013704299926757812, 0.0141448974609375, 0.01056671142578125, 0.01372528076171875, -0.0226593017578125, -0.02569580078125, 0.00824737548828125, -0.0002002716064453125, 0.02459716796875, 0.022705078125, 0.028076171875, 0.0243682861328125, 0.0036144256591796875, -0.042938232421875, 0.00830841064453125, 0.033935546875, 0.0013790130615234375, -0.010009765625, -0.06451416015625, -0.020233154296875, 0.04302978515625, -0.0037937164306640625, -0.0271453857421875, 0.0223846435546875, 0.03057861328125, -0.0028553009033203125, -0.01513671875, 0.00525665283203125, 0.045806884765625, -0.003429412841796875, 0.025360107421875, 0.0075225830078125, 0.054595947265625, 0.06781005859375, -0.05255126953125, 0.01727294921875, -0.00047779083251953125, 0.03057861328125, 0.050933837890625, 0.0014896392822265625, 0.0950927734375, 0.006252288818359375, 0.0478515625, -0.042572021484375, 0.032196044921875, -0.0277862548828125, -0.02410888671875, -0.035858154296875, -0.10009765625, -0.049896240234375, -0.0200347900390625, -0.0531005859375, -0.0205078125, 0.00843048095703125, 0.0087127685546875, 0.078369140625, -0.0531005859375, -0.02496337890625, 0.0288543701171875, 0.0268402099609375, -0.0005903244018554688, 0.0188446044921875, -0.0307159423828125, -0.047760009765625, 0.0101470947265625, 0.027435302734375, -0.0509033203125, -0.0254364013671875, -0.0217742919921875, -0.037322998046875, 0.01088714599609375, -0.037261962890625, -0.02630615234375, -0.03173828125, 0.047943115234375, -0.0018472671508789062, 0.00701904296875, 0.00855255126953125, 0.0218658447265625, -0.03912353515625, -0.044586181640625, 0.01468658447265625, -0.02264404296875, -0.0207061767578125, -0.0306243896484375, 0.01849365234375, -0.043853759765625, -0.01340484619140625, -0.00569915771484375, 0.034393310546875, 0.047515869140625, -0.006084442138671875, -0.0269012451171875, -0.0263671875, -0.013885498046875, -0.01090240478515625, 0.0360107421875, -0.0210418701171875, -0.010986328125, 0.0032634735107421875, -0.01184844970703125, 0.012176513671875, -0.0251922607421875, -0.0236968994140625, -0.03643798828125, 0.00719451904296875, -0.0167999267578125, 0.01910400390625, 0.05133056640625, -0.03326416015625, 0.051910400390625, 0.05889892578125, 0.01108551025390625, 0.005298614501953125, 0.02276611328125, -0.0017108917236328125, 0.0293731689453125, 0.0189971923828125, 0.0018548965454101562, -0.01056671142578125, 0.03466796875, -0.0064849853515625, -0.00789642333984375, 0.0167083740234375, -0.0275726318359375, 0.0018053054809570312, -0.0263519287109375, 0.0303802490234375, -0.006134033203125, -0.020233154296875, -0.018707275390625, 0.009552001953125, 0.01355743408203125, 0.0430908203125, 0.02813720703125, -0.0133819580078125, -0.02130126953125, 0.01226043701171875, 0.0066375732421875, 0.044677734375, -0.00843048095703125, 0.05682373046875, 0.037628173828125, 0.006801605224609375, -0.05157470703125, 0.0361328125, 0.02679443359375, -0.038848876953125, 0.0215301513671875, 0.009307861328125, -0.020233154296875, -0.01500701904296875, -0.01430511474609375, -0.01290130615234375, 0.00231170654296875, -0.0186614990234375, -0.0157012939453125, -0.00037932395935058594, -0.01540374755859375, -0.0792236328125, 0.0231781005859375, 0.02325439453125, -0.01071929931640625, 0.03619384765625, 0.025238037109375, -0.01381683349609375, -0.0258331298828125, 0.0219573974609375, -0.0030384063720703125, 0.045196533203125, 0.0007796287536621094, -0.002529144287109375, -0.0010061264038085938, -0.016876220703125, 0.0511474609375, 0.023193359375, -0.0025424957275390625, -0.0015172958374023438, -0.025665283203125, 0.00875091552734375, -0.01125335693359375, 0.0272369384765625, 0.02178955078125, -0.0164337158203125, 0.00794219970703125, -0.0009832382202148438, 0.0223541259765625, 0.0266876220703125, -0.06536865234375, -0.025054931640625, -0.04669189453125, -0.007480621337890625, -0.0416259765625, 0.005107879638671875, -0.036376953125, -0.039215087890625, 0.06549072265625, -0.049957275390625, 0.0005831718444824219, -0.0014753341674804688, -0.0260009765625, 0.00794219970703125, 0.0266571044921875, 0.031829833984375, -0.0545654296875, -0.0027618408203125, 0.0738525390625, 0.0022029876708984375, -0.0211944580078125, -0.0257110595703125, 0.049346923828125, 0.0060577392578125, 0.035614013671875, -0.01342010498046875, 0.0026092529296875, 0.0440673828125, 0.0105438232421875, 0.0172882080078125, -0.0157012939453125, -0.005283355712890625, -0.01554107666015625, 0.0204925537109375, -0.0374755859375, -0.01255035400390625, 0.01910400390625, -0.01061248779296875, 0.02667236328125, -0.045074462890625, 0.00704193115234375, 0.042938232421875, 0.035186767578125, 0.00543212890625, -0.01143646240234375, -0.0355224609375, 0.016326904296875, 0.0186614990234375, 0.0034656524658203125, 0.059234619140625, 0.037261962890625, -0.0261077880859375, -0.0257568359375, 0.005832672119140625, 0.00481414794921875, -0.06475830078125, -0.051727294921875, -0.050811767578125, 0.01520538330078125, -0.048828125, -0.017730712890625, 0.01450347900390625, -0.0362548828125, 0.01354217529296875, -0.003314971923828125, 0.0015058517456054688, -0.0307464599609375, -0.02410888671875, 0.0016021728515625, -0.0428466796875, -0.040985107421875, -0.0156097412109375, 0.042724609375, -0.00983428955078125, -0.021270751953125, 0.005710601806640625, -0.00885009765625, -0.044891357421875, 0.018341064453125, -0.01432037353515625, 0.01235198974609375, -0.01372528076171875, 0.0304107666015625, 0.0077056884765625, 0.035797119140625, 0.005184173583984375, 0.066162109375, 0.030914306640625, -0.0177154541015625, 0.01285552978515625, 0.00380706787109375, 0.00492095947265625, 0.034454345703125, -0.0057830810546875, -0.0182952880859375, -0.0163116455078125, -0.0260162353515625, 0.00426483154296875, 0.0413818359375, 0.01751708984375, 0.0157623291015625, 0.004673004150390625, -0.00876617431640625, 0.0721435546875, 0.005535125732421875, 0.104736328125, -0.0283355712890625, -0.0208282470703125, 0.0055694580078125, 0.0240478515625, 0.0198822021484375, -0.0104217529296875, 0.031646728515625, 0.022125244140625, -0.0101318359375, 0.0295867919921875, -0.0335693359375, 0.002716064453125, 0.00948333740234375, -0.034912109375, 0.007965087890625, -0.021575927734375, 0.05316162109375, 0.053619384765625, 0.0182952880859375, 0.0181884765625, -0.006153106689453125, -0.0258026123046875, 0.0147247314453125, -0.033599853515625, -0.051422119140625, 0.01806640625, 0.007537841796875, 0.01366424560546875, 0.00853729248046875, -0.001964569091796875, -0.043060302734375, -0.0038013458251953125, -0.0158538818359375, 0.084716796875, 0.0655517578125, 0.0245361328125, 0.0151214599609375, -0.002452850341796875, 0.0090179443359375, 0.035797119140625, 0.01415252685546875, 0.040618896484375, 0.05609130859375, 0.0115814208984375, 0.037811279296875, -0.036376953125, 0.03558349609375, 0.02197265625, 0.0236968994140625, -0.0036678314208984375, 0.02227783203125, 0.02642822265625, 0.028533935546875, -0.044525146484375, 0.056854248046875, 0.0024738311767578125, -0.035919189453125, 0.03765869140625, -0.0059356689453125, 0.03411865234375, -0.049468994140625, -0.007537841796875, 0.005558013916015625, 0.007137298583984375, 0.005748748779296875, 0.0021610260009765625, -0.056671142578125, 0.0301513671875, 0.05029296875, -0.05706787109375, 0.017547607421875, 0.01450347900390625, -0.04052734375, 0.0447998046875, -0.0653076171875, -0.04833984375, 0.06103515625, -0.03167724609375, -0.039825439453125, 0.0207672119140625, 0.039581298828125, -0.0263824462890625, -0.035675048828125, -0.039764404296875, 0.0389404296875, 0.0145263671875, 1.7583370208740234e-05, 0.007221221923828125, -0.02801513671875, 0.0070037841796875, 0.01910400390625, 0.03363037109375, 0.0002529621124267578, -0.0227508544921875, 0.030517578125, -0.03411865234375, -0.0178070068359375, -0.01090240478515625, 0.020538330078125, 0.0192413330078125, -0.0283050537109375, -0.026031494140625, -0.0089569091796875, -0.00018262863159179688, -0.006084442138671875, 0.00437164306640625, -0.0628662109375, -0.0445556640625, -0.0299530029296875, 0.0290374755859375, 0.0286712646484375, 0.060150146484375, -0.0221710205078125, -0.0288848876953125, 0.003322601318359375, -0.06640625, -0.0249176025390625, -0.0064697265625, 0.0087432861328125, -0.04608154296875, 0.0679931640625, 0.01194000244140625, -0.011138916015625, -0.023345947265625, 0.0484619140625, 0.0159912109375, 0.027374267578125, -0.00505828857421875, 0.068359375, -0.00328826904296875, -0.06854248046875, -0.0156402587890625, -0.0010833740234375, 0.0172271728515625, 0.08416748046875, -0.03533935546875, -0.015625, -0.0033626556396484375, -0.061370849609375, -0.0182037353515625, 0.0006427764892578125, 0.007038116455078125, -0.04498291015625, 0.0251312255859375, 0.0195465087890625, -0.00046753883361816406, 0.015716552734375, 0.01006317138671875, 0.0117340087890625, 0.0017986297607421875, 0.0086822509765625, -0.0272979736328125, 0.042877197265625, -0.046112060546875, -0.04754638671875, -0.047760009765625, 0.030181884765625, -0.00927734375, 0.025146484375, -0.02008056640625, -0.03424072265625, -0.001285552978515625, -0.016357421875, 0.016754150390625, -0.0135498046875, -0.0283966064453125, -0.0203704833984375, -0.0214996337890625, 0.00797271728515625, 0.003810882568359375, -0.0131072998046875, 0.02325439453125, 0.053466796875, -0.042266845703125, 0.00934600830078125, 0.034393310546875, -0.0235137939453125, -0.0165557861328125, -0.017181396484375, -0.02178955078125, -0.002796173095703125, 0.006458282470703125, -0.02801513671875, -0.0367431640625, -0.08135986328125, -0.01345062255859375, -0.01230621337890625, -0.006465911865234375, -0.00992584228515625, -0.031707763671875, -0.051361083984375, -0.0214691162109375, 0.0218505859375, 0.025299072265625, 0.0141143798828125, -0.04888916015625, 0.007053375244140625, -0.00936126708984375, -0.05303955078125, 0.0164031982421875, -0.006084442138671875, 0.01436614990234375, 0.014892578125, -0.0155792236328125, 0.00894927978515625, -0.003021240234375, 0.04034423828125, 0.0236663818359375, 0.0211029052734375, -0.045684814453125, 0.0006971359252929688, 0.003955841064453125, -0.0263519287109375, -0.005718231201171875, -0.059814453125, 0.0316162109375, -0.0149383544921875, -0.0277862548828125, 0.0271148681640625, 0.0204620361328125, 0.0048980712890625, -0.0127410888671875, -0.0292510986328125, 0.0012645721435546875, 0.005939483642578125, 0.0721435546875, -0.022003173828125, -0.07562255859375, 0.023529052734375, 0.0009274482727050781, 0.0418701171875, 0.052215576171875, 0.022735595703125, -0.0290374755859375, 0.0172119140625, 0.0281829833984375, -0.020660400390625, -0.0777587890625, -0.0208587646484375, 0.040771484375, 0.017486572265625, 0.057769775390625, -0.01090240478515625, 0.032684326171875, 0.0193634033203125, -0.01373291015625, -0.0115814208984375, 0.080078125, 0.033447265625, -0.0231170654296875, -0.03497314453125, -0.023468017578125, 0.03057861328125, -0.07513427734375, -0.0394287109375, 0.023101806640625, 0.0250396728515625, -0.08502197265625, -0.027984619140625, 0.00909423828125, -0.01189422607421875, -0.0105743408203125, 0.01297760009765625, -0.0027332305908203125, 0.0338134765625, 0.01366424560546875, -0.05902099609375, -0.005474090576171875, 0.00505828857421875, -0.01000213623046875, 0.07568359375, 0.00910186767578125, -0.0220184326171875, 0.0113372802734375, 0.00916290283203125, 0.0164642333984375, -0.01837158203125, 0.0150909423828125, 0.0258026123046875, 0.013671875, 0.011322021484375, 0.05584716796875, -0.035400390625, -0.03143310546875, -0.007587432861328125, 0.06439208984375, 0.0201873779296875, 0.0491943359375, -0.0095977783203125, 0.0180816650390625, 0.0228424072265625, -0.038543701171875, -0.0005903244018554688, 0.00789642333984375, -0.0282440185546875, -0.0165863037109375, -0.031646728515625, 0.027191162109375, 0.0062713623046875, 0.00719451904296875, -0.00012290477752685547, -0.03564453125, -0.03302001953125, 0.053192138671875, -0.019805908203125, -0.08111572265625, 0.036407470703125, -2.0205974578857422e-05, -0.007785797119140625, -0.044189453125, -0.05499267578125, 0.0183868408203125, -0.0292205810546875, -0.060546875, -0.00800323486328125, 0.035858154296875, -0.0236053466796875, -0.024078369140625, -0.0177001953125, -0.01181793212890625, -0.010650634765625, -0.01514434814453125, -0.052581787109375, -0.0300445556640625, 0.01126861572265625, -0.04815673828125, -0.01318359375, 0.0237884521484375, 0.0214691162109375, -0.0125732421875, -0.00177001953125, -0.0208587646484375, 0.00612640380859375, 0.0049591064453125, 0.046142578125, -0.01442718505859375, -0.0194091796875, 0.03118896484375, 0.044830322265625, 0.0020580291748046875, -0.00516510009765625, 0.004283905029296875, -0.0601806640625, 0.0204315185546875, -0.003108978271484375, -0.00988006591796875, -0.01436614990234375, -0.029754638671875, 0.043304443359375, -0.066162109375, 0.0372314453125, -0.002529144287109375, 0.00490570068359375, -0.058563232421875, 0.008148193359375, -0.007537841796875, -0.00408172607421875, 0.04986572265625, -0.00351715087890625, -0.04302978515625, 0.01003265380859375, -0.04730224609375, 0.059814453125, 0.01195526123046875, 0.030242919921875, -0.04534912109375, 0.0013561248779296875, 0.005084991455078125, -0.0090179443359375, 0.00870513916015625, -0.09844970703125, -0.01419830322265625, 0.025726318359375, 0.007354736328125, -0.0090789794921875, 0.00841522216796875, 0.0152435302734375, 0.0016927719116210938, -0.04058837890625, 0.006099700927734375, -0.0016088485717773438, -0.0031681060791015625, -0.03497314453125, 0.00978851318359375, -0.037261962890625, -0.035675048828125, -0.01507568359375, -0.01161956787109375, 0.033935546875, 0.0161285400390625, -0.049713134765625, 0.025634765625, -0.01255035400390625, 0.02178955078125, 0.0335693359375, 0.06304931640625, -0.01091766357421875, -0.00807952880859375, 0.00899505615234375, 0.0362548828125, 0.004825592041015625, 0.01322174072265625, 0.0029888153076171875, -0.00899505615234375, 0.004497528076171875, -0.03753662109375, 0.036041259765625, 0.0269012451171875, -0.0272369384765625, -0.0260009765625, -0.007293701171875, -0.05438232421875, 0.040802001953125, 0.0019254684448242188, 0.03143310546875, -0.03985595703125, 0.0020694732666015625, -0.0968017578125, 0.06353759765625, -0.016448974609375, 0.02154541015625, 0.0267181396484375, -0.03619384765625, -0.06890869140625, -0.036834716796875, -0.03839111328125, 0.0152587890625, -0.025634765625, 0.05206298828125, -0.023529052734375, 0.0002282857894897461, 0.015411376953125, 0.025634765625, 0.005031585693359375, 0.042449951171875, 0.036376953125, -0.0162506103515625, 0.0673828125, -0.03228759765625, -0.01885986328125, -0.0193634033203125, 0.01247406005859375, 0.0087127685546875, -0.00901031494140625, -0.04229736328125, -0.0281524658203125, 0.020751953125, 0.00354766845703125, -0.0367431640625, 0.02020263671875, 0.01244354248046875, 0.01043701171875, -0.0217742919921875, 0.00909423828125, 0.005733489990234375, -0.00319671630859375, 0.01641845703125, 0.0030307769775390625, -0.04913330078125, -0.06719970703125, -0.016632080078125, -0.024871826171875, -0.0230865478515625, -0.01031494140625, 0.0087738037109375, 0.0224761962890625, -0.0166015625, 0.03826904296875, -0.0065765380859375, 0.0027675628662109375, -0.004306793212890625, -0.02685546875, -0.0265045166015625, 0.038116455078125, 0.01076507568359375, -0.0753173828125, 0.021331787109375, 0.005870819091796875, -0.03302001953125, -0.0055694580078125, 0.0259857177734375, -0.0189208984375, 0.03515625, -0.07574462890625, -0.0204925537109375, 0.012603759765625, 0.0261077880859375, 0.002964019775390625, -0.01477813720703125, 0.021728515625, -0.017791748046875, -0.05377197265625, 0.011199951171875, -0.01262664794921875, -0.0374755859375, 0.0201416015625, 0.0213165283203125, -0.01059722900390625, 0.0067596435546875, 0.034912109375, 0.030609130859375, 0.0418701171875, 0.0191802978515625, -0.034912109375, 0.0134429931640625, 0.0017566680908203125, 0.03704833984375, -0.00225067138671875, 0.049896240234375, -0.00783538818359375, -0.005435943603515625, 0.007137298583984375, -0.061981201171875, 0.003200531005859375, -0.032257080078125, -0.044952392578125, 0.01666259765625, 0.059356689453125, 0.026031494140625, -0.004734039306640625, 0.054229736328125, 0.05426025390625, 0.024932861328125, 0.01183319091796875, -0.05645751953125, -0.07672119140625, 0.00479888916015625, 0.0247039794921875, 0.0278472900390625, -0.07061767578125, 0.04718017578125, 0.07061767578125, -0.0423583984375, -0.0011415481567382812, 0.0133056640625, -0.00998687744140625, 0.03778076171875, 0.053131103515625, 0.01806640625, -0.053802490234375, 0.028045654296875, 0.038604736328125, -0.01003265380859375, -0.023834228515625, 0.0025577545166015625, -0.0226287841796875, 0.031768798828125, -0.044677734375, -0.0033168792724609375, -0.00910186767578125, 0.0684814453125, 0.01033782958984375, -0.045440673828125, -0.0017442703247070312, 0.03424072265625, -0.0036525726318359375, 0.00579071044921875, -0.0091094970703125, -0.007755279541015625, -0.0260162353515625, 0.0199737548828125, 0.03472900390625, 0.008270263671875, 0.055389404296875, 0.028839111328125, -0.003826141357421875, 0.006435394287109375, -0.01050567626953125, 0.0017213821411132812, -0.007671356201171875, -0.04937744140625, -0.0256500244140625, -0.050384521484375, 0.054168701171875, 0.008544921875, 0.05291748046875, -0.0008220672607421875, -0.03399658203125, -0.005016326904296875, -0.006317138671875, 0.0281524658203125, 0.03839111328125, 0.0133209228515625, 0.020172119140625, -0.00945281982421875, 0.0019254684448242188, -0.005870819091796875, -0.024688720703125, -0.007236480712890625, -0.0065460205078125, -0.0236358642578125, -0.07159423828125, 0.0311431884765625, 0.01861572265625, -0.007221221923828125, -0.00998687744140625, -0.00048661231994628906, 0.004032135009765625, -0.01091766357421875, -0.01690673828125, -0.0197906494140625, 0.0280303955078125, -0.06781005859375, 0.00689697265625, -0.004932403564453125, 0.00826263427734375, 0.0236968994140625, -0.08612060546875, 0.057952880859375, 0.014892578125, 0.041717529296875, 0.01064300537109375, 0.033843994140625, 0.01995849609375, -0.011932373046875, 0.029083251953125, 0.023651123046875, 0.00382232666015625, 0.045989990234375, 0.03143310546875, -0.00583648681640625, -0.038818359375, 0.017791748046875, -8.428096771240234e-05, -0.09173583984375, 0.042633056640625, -0.004299163818359375, 0.025146484375, 0.00579071044921875, -0.00186920166015625, -0.00830078125, -0.044342041015625, 0.06524658203125, 0.00982666015625, -0.047943115234375, 0.01544189453125, -0.041259765625, 0.0242767333984375, -0.012298583984375, 0.054931640625, -0.013092041015625, 0.03228759765625, 0.01009368896484375, 0.014404296875, 0.0031890869140625, -8.779764175415039e-05, -0.01120758056640625, 0.0242462158203125, -0.0299072265625, 0.049896240234375, 0.0303497314453125, -0.0080413818359375, -0.059112548828125, -0.01160430908203125, -0.0059814453125, -0.0338134765625, 0.006378173828125, -0.036163330078125, -0.036712646484375, 0.023529052734375, 0.042266845703125, 0.045257568359375, -0.015045166015625, 0.03179931640625, 0.010467529296875, -0.00010538101196289062, 0.006378173828125, -0.013031005859375]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Charger les embeddings depuis le fichier JSON\n",
    "with open('/Users/pierrebourbon/Desktop/archive sise camp/transcripts_embeddings.json', 'r') as f:\n",
    "    chunk_embed = json.load(f)\n",
    "\n",
    "# Afficher les premiers éléments pour vérification\n",
    "print(chunk_embed[:2])  # Affiche les 2 premiers embeddings pour vérifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création liste de chunks\n",
    "chunks = [chunk for _, chunk in chunks_with_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Générer les embeddings\n",
    "chunk_embed = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_embed.append(generate_embedding(chunk))\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarde des embeddings\n",
    "import json\n",
    "with open('transcripts_embeddings.json', 'w') as f:\n",
    "    json.dump(chunk_embed, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de l'index Faiss pour les chapitres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "#Générer la liste des IDs\n",
    "chunk_id = [id for id, _ in chunks_with_ids]\n",
    "\n",
    "#Définir la dimension des vecteurs \n",
    "dimension = 1024\n",
    "\n",
    "#Crée l'index Faiss\n",
    "index2 = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Ajouter les IDs des chapitres à l'index FAISS\n",
    "index_with_ids2 = faiss.IndexIDMap(index2)  # Permet d'ajouter des IDs personnalisés\n",
    "index_with_ids2.add_with_ids(np.array(chunk_embed, dtype=np.float32), np.array(chunk_id, dtype=np.int64))\n",
    "\n",
    "faiss.write_index(index_with_ids2, \"faiss_index_transcripts.bin\")  # Sauvegarde"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sisecamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
